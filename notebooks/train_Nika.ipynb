{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, T5Config, T5ForConditionalGeneration\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# 类\n",
    "\n",
    "class GPT2Dataset(Dataset):\n",
    "\n",
    "  def __init__(self, propmt_list, answer_list, tokenizer,\n",
    "               max_length_propmt=1024, max_length_answer=10):\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "    self.input_ids = []\n",
    "    self.attn_masks = []\n",
    "    self.answer_ids = []\n",
    "\n",
    "    # 设置填充参数为右填充\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    for txt in propmt_list:\n",
    "\n",
    "      encodings_dict = tokenizer('<s>'+ txt , truncation=True, max_length=max_length_propmt, padding=\"max_length\")\n",
    "\n",
    "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "    for ans in answer_list:\n",
    "\n",
    "      encodings_dict = tokenizer('<s>'+ str(ans), truncation=True, max_length=max_length_answer, padding=\"max_length\")\n",
    "\n",
    "      self.answer_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.attn_masks[idx], self.answer_ids[idx]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(config_path, model_path, tokenizer, train_dataloader,\n",
    "                   validation_dataloader,epochs,learning_rate,warmup_ratio,\n",
    "                   epsilon):\n",
    "    # I'm not really doing anything with the config buheret\n",
    "    configuration = T5Config.from_pretrained(config_path, output_hidden_states=False)\n",
    "\n",
    "    # instantiate the model\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path, config=configuration)\n",
    "\n",
    "    # this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "    # otherwise the tokenizer and model tensors won't match up\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        \n",
    "        #NIKA added this bit for tracking;\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        # Wrap the model with DataParallel to use both GPUs\n",
    "        model = DataParallel(model)\n",
    "        #NIKA added above bit for tracking;\n",
    "    \n",
    "    # Set the seed value all over the place to make this reproducible.\n",
    "    seed_val = 42\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    # some parameters I cooked up that work reasonably well\n",
    "    \n",
    "    \n",
    "\n",
    "    epochs = epochs\n",
    "    learning_rate = learning_rate\n",
    "    warmup_steps = int(len(train_dataloader) * epochs * warmup_ratio)\n",
    "    epsilon = epsilon\n",
    "\n",
    "    # this produces sample output every 100 steps\n",
    "    sample_every = 1000\n",
    "\n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                    lr = learning_rate,\n",
    "                    eps = epsilon\n",
    "                    )\n",
    "\n",
    "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    # This changes the learning rate as the training loop progresses\n",
    "    # scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "    #                                             num_warmup_steps = warmup_steps, \n",
    "    #                                             num_training_steps = total_steps)\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    \n",
    "    def format_time(elapsed):\n",
    "        return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    training_stats = []\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        total_train_loss = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            b_masks = batch[1].to(device)\n",
    "\n",
    "            model.zero_grad()        \n",
    "\n",
    "            outputs = model(  b_input_ids,\n",
    "                            labels=b_labels, \n",
    "                            attention_mask = b_masks,\n",
    "                            output_attentions=True\n",
    "                            )\n",
    "\n",
    "            loss = outputs[0]  \n",
    "            loss = loss.mean()  # 获取所有GPU上的平均损失\n",
    "\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            total_train_loss += batch_loss\n",
    "\n",
    "            # Get sample every x batches.\n",
    "            if step % sample_every == 0 and not step == 0:\n",
    "\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
    "                \n",
    "                model.train()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader) \n",
    "        writer.add_scalar(\"Loss/train\", avg_train_loss, epoch_i+1)      \n",
    "        \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "            \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_eval_loss = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            b_masks = batch[1].to(device)\n",
    "            \n",
    "            with torch.no_grad():        \n",
    "                outputs  = model(b_input_ids, \n",
    "    #                            token_type_ids=None, \n",
    "                                attention_mask = b_masks,\n",
    "                                labels=b_labels,\n",
    "                                output_attentions=True)\n",
    "            \n",
    "                loss = outputs[0]  \n",
    "                loss = loss.mean()  # 获取所有GPU上的平均损失\n",
    "            batch_loss = loss.item()\n",
    "            total_eval_loss += batch_loss        \n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        writer.add_scalar(\"Loss/test\", avg_val_loss, epoch_i+1)\n",
    "        \n",
    "        validation_time = format_time(time.time() - t0)    \n",
    "\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "    return model, tokenizer, training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hn/yy0f4gf550d1fb7rsdb9wszm0000gp/T/ipykernel_42838/2329879666.py:41: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(doc_lengths)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Density'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_path = 'seyonec/PubChem10M_SMILES_BPE_450k'\n",
    "path = 'GT4SD/multitask-text-and-chemistry-t5-base-standard'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(token_path, bos_token='<s>', eos_token='</s>', pad_token='[PAD]', padding_side='right') \n",
    "# print(\"The max model length is {} for this model.\".format(tokenizer.model_max_length))\n",
    "# print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
    "# print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
    "# print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))\n",
    "custom_tokens = [\n",
    "    'Predict the atomization energy of the following SMILES:',\n",
    "    'Predict the bandgap crystal of the following SMILES:',\n",
    "    'Predict the Tg of the following SMILES:',\n",
    "    'Predict the heat resistance class of the following SMILES:'\n",
    "]\n",
    "tokenizer.add_tokens(custom_tokens)\n",
    "\n",
    "# df_train = pd.read_csv('/work/PolyGPT/T5/0data&code/data/train/prompt-target.csv')\n",
    "df_train = pd.read_csv('../data/train/prompt-target.csv') #i have changed directory to mine\n",
    "df_train.dropna(inplace=True) #remove NA values\n",
    "prompt_list_train = df_train['prompt'].tolist() #just use the main bio text in this example\n",
    "answer_list_train = df_train['target'].tolist() #just use the main bio text in this example\n",
    "\n",
    "#df_test = pd.read_csv('/work/PolyGPT/T5/0data&code/data/test/prompt-target.csv')\n",
    "df_test = pd.read_csv('../data/test/prompt-target.csv') #i have changed directory to mine\n",
    "df_test.dropna(inplace=True) #remove NA values\n",
    "prompt_list_test = df_test['prompt'].tolist() #just use the main bio text in this example\n",
    "answer_list_test = df_test['target'].tolist() #just use the main bio text in this example\n",
    "\n",
    "doc_lengths = []\n",
    "\n",
    "for prompt in prompt_list_test:\n",
    "\n",
    "    # get rough token count distribution\n",
    "    # tokens = nltk.word_tokenize(bio)\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "\n",
    "    doc_lengths.append(len(tokens))\n",
    "\n",
    "doc_lengths = np.array(doc_lengths)\n",
    "\n",
    "sns.distplot(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20,650 training samples\n",
      "2,320 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/polyNC1/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 100 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1, Batch 1/173, Loss: 8.2443, Elapsed: 0:00:14\n",
      "  Epoch 1, Batch 11/173, Loss: 8.2160, Elapsed: 0:04:47\n",
      "  Epoch 1, Batch 21/173, Loss: 8.3131, Elapsed: 0:09:23\n",
      "  Epoch 1, Batch 31/173, Loss: 8.2435, Elapsed: 0:14:03\n",
      "  Epoch 1, Batch 41/173, Loss: 8.1990, Elapsed: 1:08:38\n",
      "  Epoch 1, Batch 51/173, Loss: 8.2485, Elapsed: 1:13:46\n",
      "  Epoch 1, Batch 61/173, Loss: 8.0013, Elapsed: 1:18:14\n",
      "  Epoch 1, Batch 71/173, Loss: 7.9682, Elapsed: 1:33:19\n",
      "  Epoch 1, Batch 81/173, Loss: 8.1037, Elapsed: 1:37:45\n",
      "  Epoch 1, Batch 91/173, Loss: 7.8917, Elapsed: 2:13:53\n",
      "  Epoch 1, Batch 101/173, Loss: 7.8809, Elapsed: 2:55:39\n",
      "  Epoch 1, Batch 111/173, Loss: 7.8485, Elapsed: 3:00:07\n",
      "  Epoch 1, Batch 121/173, Loss: 7.8496, Elapsed: 3:04:36\n",
      "  Epoch 1, Batch 131/173, Loss: 7.7306, Elapsed: 3:21:17\n",
      "  Epoch 1, Batch 141/173, Loss: 7.7323, Elapsed: 3:26:02\n",
      "  Epoch 1, Batch 151/173, Loss: 7.6524, Elapsed: 3:30:50\n",
      "  Epoch 1, Batch 161/173, Loss: 7.4436, Elapsed: 3:35:23\n",
      "  Epoch 1, Batch 171/173, Loss: 7.4411, Elapsed: 3:39:49\n",
      "\n",
      "  Average training loss: 7.92\n",
      "  Training epoch took: 3:40:32\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 6.89\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 2 / 100 ========\n",
      "Training...\n",
      "  Epoch 2, Batch 1/173, Loss: 7.4993, Elapsed: 0:15:26\n",
      "  Epoch 2, Batch 11/173, Loss: 7.2514, Elapsed: 1:08:02\n",
      "  Epoch 2, Batch 21/173, Loss: 7.2591, Elapsed: 1:45:59\n",
      "  Epoch 2, Batch 31/173, Loss: 7.2902, Elapsed: 1:50:37\n",
      "  Epoch 2, Batch 41/173, Loss: 7.2250, Elapsed: 1:55:09\n",
      "  Epoch 2, Batch 51/173, Loss: 7.5880, Elapsed: 1:59:45\n",
      "  Epoch 2, Batch 61/173, Loss: 7.1504, Elapsed: 2:04:17\n",
      "  Epoch 2, Batch 71/173, Loss: 7.0326, Elapsed: 2:08:55\n",
      "  Epoch 2, Batch 81/173, Loss: 7.0320, Elapsed: 2:44:45\n",
      "  Epoch 2, Batch 91/173, Loss: 6.8607, Elapsed: 3:21:47\n",
      "  Epoch 2, Batch 101/173, Loss: 6.9398, Elapsed: 4:42:59\n",
      "  Epoch 2, Batch 111/173, Loss: 6.9848, Elapsed: 5:03:41\n",
      "  Epoch 2, Batch 121/173, Loss: 6.6622, Elapsed: 5:43:26\n",
      "  Epoch 2, Batch 131/173, Loss: 6.8938, Elapsed: 6:29:17\n",
      "  Epoch 2, Batch 141/173, Loss: 6.4799, Elapsed: 7:06:42\n",
      "  Epoch 2, Batch 151/173, Loss: 6.5784, Elapsed: 7:41:50\n",
      "  Epoch 2, Batch 161/173, Loss: 6.4866, Elapsed: 8:01:29\n",
      "  Epoch 2, Batch 171/173, Loss: 6.4875, Elapsed: 8:21:43\n",
      "\n",
      "  Average training loss: 6.95\n",
      "  Training epoch took: 8:38:08\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 5.86\n",
      "  Validation took: 0:01:26\n",
      "\n",
      "======== Epoch 3 / 100 ========\n",
      "Training...\n",
      "  Epoch 3, Batch 1/173, Loss: 6.4950, Elapsed: 0:00:12\n",
      "  Epoch 3, Batch 11/173, Loss: 6.2341, Elapsed: 0:37:12\n",
      "  Epoch 3, Batch 21/173, Loss: 6.5029, Elapsed: 1:12:36\n",
      "  Epoch 3, Batch 31/173, Loss: 6.3647, Elapsed: 1:47:56\n",
      "  Epoch 3, Batch 41/173, Loss: 6.2668, Elapsed: 2:41:13\n",
      "  Epoch 3, Batch 51/173, Loss: 6.2512, Elapsed: 2:47:13\n",
      "  Epoch 3, Batch 61/173, Loss: 6.1839, Elapsed: 2:51:44\n",
      "  Epoch 3, Batch 71/173, Loss: 6.1843, Elapsed: 2:56:21\n",
      "  Epoch 3, Batch 81/173, Loss: 6.1679, Elapsed: 3:42:05\n",
      "  Epoch 3, Batch 91/173, Loss: 6.0112, Elapsed: 3:46:50\n",
      "  Epoch 3, Batch 101/173, Loss: 6.0554, Elapsed: 3:51:29\n",
      "  Epoch 3, Batch 111/173, Loss: 6.1436, Elapsed: 3:56:01\n",
      "  Epoch 3, Batch 121/173, Loss: 5.9512, Elapsed: 4:00:54\n",
      "  Epoch 3, Batch 131/173, Loss: 5.9953, Elapsed: 4:05:42\n",
      "  Epoch 3, Batch 141/173, Loss: 5.7244, Elapsed: 4:10:11\n",
      "  Epoch 3, Batch 151/173, Loss: 5.9429, Elapsed: 4:14:49\n",
      "  Epoch 3, Batch 161/173, Loss: 5.7425, Elapsed: 4:19:34\n",
      "  Epoch 3, Batch 171/173, Loss: 5.8740, Elapsed: 4:24:30\n",
      "\n",
      "  Average training loss: 6.17\n",
      "  Training epoch took: 4:25:18\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 5.27\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 4 / 100 ========\n",
      "Training...\n",
      "  Epoch 4, Batch 1/173, Loss: 5.8559, Elapsed: 0:00:12\n",
      "  Epoch 4, Batch 11/173, Loss: 5.8059, Elapsed: 0:04:57\n",
      "  Epoch 4, Batch 21/173, Loss: 5.7384, Elapsed: 0:09:34\n",
      "  Epoch 4, Batch 31/173, Loss: 5.6843, Elapsed: 0:14:03\n",
      "  Epoch 4, Batch 41/173, Loss: 5.7874, Elapsed: 0:19:03\n",
      "  Epoch 4, Batch 51/173, Loss: 5.7569, Elapsed: 0:24:08\n",
      "  Epoch 4, Batch 61/173, Loss: 5.6577, Elapsed: 0:28:39\n",
      "  Epoch 4, Batch 71/173, Loss: 5.7165, Elapsed: 0:33:08\n",
      "  Epoch 4, Batch 81/173, Loss: 5.6202, Elapsed: 1:09:17\n",
      "  Epoch 4, Batch 91/173, Loss: 5.6301, Elapsed: 1:16:31\n",
      "  Epoch 4, Batch 101/173, Loss: 5.4390, Elapsed: 1:21:31\n",
      "  Epoch 4, Batch 111/173, Loss: 5.7255, Elapsed: 1:26:45\n",
      "  Epoch 4, Batch 121/173, Loss: 5.6141, Elapsed: 1:32:00\n",
      "  Epoch 4, Batch 131/173, Loss: 5.2774, Elapsed: 1:37:28\n",
      "  Epoch 4, Batch 141/173, Loss: 5.4867, Elapsed: 1:42:33\n",
      "  Epoch 4, Batch 151/173, Loss: 5.4539, Elapsed: 1:47:37\n",
      "  Epoch 4, Batch 161/173, Loss: 5.3816, Elapsed: 1:52:59\n",
      "  Epoch 4, Batch 171/173, Loss: 5.3061, Elapsed: 1:58:15\n",
      "\n",
      "  Average training loss: 5.59\n",
      "  Training epoch took: 1:59:04\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 4.74\n",
      "  Validation took: 0:01:37\n",
      "\n",
      "======== Epoch 5 / 100 ========\n",
      "Training...\n",
      "  Epoch 5, Batch 1/173, Loss: 5.3387, Elapsed: 0:00:13\n",
      "  Epoch 5, Batch 11/173, Loss: 5.5265, Elapsed: 0:05:16\n",
      "  Epoch 5, Batch 21/173, Loss: 5.2358, Elapsed: 0:10:40\n",
      "  Epoch 5, Batch 31/173, Loss: 5.3225, Elapsed: 0:15:57\n",
      "  Epoch 5, Batch 41/173, Loss: 5.1234, Elapsed: 0:20:53\n",
      "  Epoch 5, Batch 51/173, Loss: 5.0575, Elapsed: 0:25:56\n",
      "  Epoch 5, Batch 61/173, Loss: 5.0633, Elapsed: 0:31:18\n",
      "  Epoch 5, Batch 71/173, Loss: 5.1702, Elapsed: 0:36:22\n",
      "  Epoch 5, Batch 81/173, Loss: 5.2138, Elapsed: 0:41:14\n",
      "  Epoch 5, Batch 91/173, Loss: 5.1958, Elapsed: 0:45:51\n",
      "  Epoch 5, Batch 101/173, Loss: 4.9694, Elapsed: 0:50:42\n",
      "  Epoch 5, Batch 111/173, Loss: 5.0081, Elapsed: 0:55:36\n",
      "  Epoch 5, Batch 121/173, Loss: 5.0308, Elapsed: 1:00:06\n",
      "  Epoch 5, Batch 131/173, Loss: 4.9135, Elapsed: 1:04:36\n",
      "  Epoch 5, Batch 141/173, Loss: 5.0553, Elapsed: 1:09:18\n",
      "  Epoch 5, Batch 151/173, Loss: 5.0029, Elapsed: 1:14:04\n",
      "  Epoch 5, Batch 161/173, Loss: 4.9059, Elapsed: 1:18:42\n",
      "  Epoch 5, Batch 171/173, Loss: 4.9850, Elapsed: 1:23:12\n",
      "\n",
      "  Average training loss: 5.11\n",
      "  Training epoch took: 1:23:55\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 4.32\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 6 / 100 ========\n",
      "Training...\n",
      "  Epoch 6, Batch 1/173, Loss: 4.7659, Elapsed: 0:00:12\n",
      "  Epoch 6, Batch 11/173, Loss: 5.0658, Elapsed: 0:04:40\n",
      "  Epoch 6, Batch 21/173, Loss: 4.8308, Elapsed: 0:09:23\n",
      "  Epoch 6, Batch 31/173, Loss: 4.7799, Elapsed: 0:14:07\n",
      "  Epoch 6, Batch 41/173, Loss: 4.7838, Elapsed: 0:18:46\n",
      "  Epoch 6, Batch 51/173, Loss: 4.8656, Elapsed: 0:23:20\n",
      "  Epoch 6, Batch 61/173, Loss: 4.6705, Elapsed: 0:27:49\n",
      "  Epoch 6, Batch 71/173, Loss: 4.6159, Elapsed: 0:32:26\n",
      "  Epoch 6, Batch 81/173, Loss: 4.6958, Elapsed: 0:37:07\n",
      "  Epoch 6, Batch 91/173, Loss: 4.8271, Elapsed: 0:41:45\n",
      "  Epoch 6, Batch 101/173, Loss: 4.5512, Elapsed: 0:46:13\n",
      "  Epoch 6, Batch 111/173, Loss: 4.6723, Elapsed: 0:50:40\n",
      "  Epoch 6, Batch 121/173, Loss: 4.5506, Elapsed: 0:55:15\n",
      "  Epoch 6, Batch 131/173, Loss: 4.5728, Elapsed: 1:00:05\n",
      "  Epoch 6, Batch 141/173, Loss: 4.4247, Elapsed: 1:04:49\n",
      "  Epoch 6, Batch 151/173, Loss: 4.5946, Elapsed: 1:09:19\n",
      "  Epoch 6, Batch 161/173, Loss: 4.6327, Elapsed: 1:13:47\n",
      "  Epoch 6, Batch 171/173, Loss: 4.5006, Elapsed: 1:18:28\n",
      "\n",
      "  Average training loss: 4.68\n",
      "  Training epoch took: 1:19:15\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 3.89\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 7 / 100 ========\n",
      "Training...\n",
      "  Epoch 7, Batch 1/173, Loss: 4.4354, Elapsed: 0:00:13\n",
      "  Epoch 7, Batch 11/173, Loss: 4.5442, Elapsed: 0:05:13\n",
      "  Epoch 7, Batch 21/173, Loss: 4.3085, Elapsed: 0:10:00\n",
      "  Epoch 7, Batch 31/173, Loss: 4.4639, Elapsed: 0:14:45\n",
      "  Epoch 7, Batch 41/173, Loss: 4.5194, Elapsed: 0:19:16\n",
      "  Epoch 7, Batch 51/173, Loss: 4.2999, Elapsed: 0:24:08\n",
      "  Epoch 7, Batch 61/173, Loss: 4.4210, Elapsed: 0:29:06\n",
      "  Epoch 7, Batch 71/173, Loss: 4.2443, Elapsed: 0:33:38\n",
      "  Epoch 7, Batch 81/173, Loss: 4.1786, Elapsed: 0:38:30\n",
      "  Epoch 7, Batch 91/173, Loss: 4.4982, Elapsed: 0:43:31\n",
      "  Epoch 7, Batch 101/173, Loss: 4.2790, Elapsed: 0:48:32\n",
      "  Epoch 7, Batch 111/173, Loss: 4.2516, Elapsed: 0:53:16\n",
      "  Epoch 7, Batch 121/173, Loss: 4.2431, Elapsed: 0:58:04\n",
      "  Epoch 7, Batch 131/173, Loss: 3.9849, Elapsed: 1:03:07\n",
      "  Epoch 7, Batch 141/173, Loss: 3.9298, Elapsed: 1:08:09\n",
      "  Epoch 7, Batch 151/173, Loss: 4.2101, Elapsed: 1:12:59\n",
      "  Epoch 7, Batch 161/173, Loss: 3.9496, Elapsed: 1:17:37\n",
      "  Epoch 7, Batch 171/173, Loss: 3.9644, Elapsed: 1:22:44\n",
      "\n",
      "  Average training loss: 4.26\n",
      "  Training epoch took: 1:23:30\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 3.44\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 8 / 100 ========\n",
      "Training...\n",
      "  Epoch 8, Batch 1/173, Loss: 4.1068, Elapsed: 0:00:13\n",
      "  Epoch 8, Batch 11/173, Loss: 4.0711, Elapsed: 0:05:18\n",
      "  Epoch 8, Batch 21/173, Loss: 3.9756, Elapsed: 0:10:15\n",
      "  Epoch 8, Batch 31/173, Loss: 4.0855, Elapsed: 0:14:59\n",
      "  Epoch 8, Batch 41/173, Loss: 3.9387, Elapsed: 0:19:48\n",
      "  Epoch 8, Batch 51/173, Loss: 3.9878, Elapsed: 0:24:29\n",
      "  Epoch 8, Batch 61/173, Loss: 3.9006, Elapsed: 0:28:58\n",
      "  Epoch 8, Batch 71/173, Loss: 3.8941, Elapsed: 0:33:29\n",
      "  Epoch 8, Batch 81/173, Loss: 3.8871, Elapsed: 0:38:09\n",
      "  Epoch 8, Batch 91/173, Loss: 3.8199, Elapsed: 0:42:57\n",
      "  Epoch 8, Batch 101/173, Loss: 3.7728, Elapsed: 0:47:42\n",
      "  Epoch 8, Batch 111/173, Loss: 3.7071, Elapsed: 0:52:11\n",
      "  Epoch 8, Batch 121/173, Loss: 3.6123, Elapsed: 0:56:42\n",
      "  Epoch 8, Batch 131/173, Loss: 3.7082, Elapsed: 1:01:18\n",
      "  Epoch 8, Batch 141/173, Loss: 3.8592, Elapsed: 1:06:04\n",
      "  Epoch 8, Batch 151/173, Loss: 3.7151, Elapsed: 1:10:52\n",
      "  Epoch 8, Batch 161/173, Loss: 3.3835, Elapsed: 1:15:25\n",
      "  Epoch 8, Batch 171/173, Loss: 3.6798, Elapsed: 1:19:54\n",
      "\n",
      "  Average training loss: 3.82\n",
      "  Training epoch took: 1:20:39\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 2.96\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 9 / 100 ========\n",
      "Training...\n",
      "  Epoch 9, Batch 1/173, Loss: 3.8336, Elapsed: 0:00:12\n",
      "  Epoch 9, Batch 11/173, Loss: 3.7577, Elapsed: 0:05:05\n",
      "  Epoch 9, Batch 21/173, Loss: 3.4230, Elapsed: 0:09:52\n",
      "  Epoch 9, Batch 31/173, Loss: 3.4840, Elapsed: 0:14:35\n",
      "  Epoch 9, Batch 41/173, Loss: 3.5366, Elapsed: 0:19:15\n",
      "  Epoch 9, Batch 51/173, Loss: 3.2958, Elapsed: 0:23:53\n",
      "  Epoch 9, Batch 61/173, Loss: 3.2730, Elapsed: 0:28:39\n",
      "  Epoch 9, Batch 71/173, Loss: 3.3500, Elapsed: 0:33:19\n",
      "  Epoch 9, Batch 81/173, Loss: 3.5855, Elapsed: 0:37:52\n",
      "  Epoch 9, Batch 91/173, Loss: 3.3400, Elapsed: 0:42:21\n",
      "  Epoch 9, Batch 101/173, Loss: 3.2652, Elapsed: 0:46:53\n",
      "  Epoch 9, Batch 111/173, Loss: 3.2826, Elapsed: 0:51:42\n",
      "  Epoch 9, Batch 121/173, Loss: 3.0276, Elapsed: 0:56:33\n",
      "  Epoch 9, Batch 131/173, Loss: 3.1137, Elapsed: 1:01:23\n",
      "  Epoch 9, Batch 141/173, Loss: 3.1589, Elapsed: 1:05:53\n",
      "  Epoch 9, Batch 151/173, Loss: 3.4777, Elapsed: 1:10:22\n",
      "  Epoch 9, Batch 161/173, Loss: 3.2053, Elapsed: 1:15:08\n",
      "  Epoch 9, Batch 171/173, Loss: 3.2089, Elapsed: 1:19:56\n",
      "\n",
      "  Average training loss: 3.36\n",
      "  Training epoch took: 1:20:42\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 2.49\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 10 / 100 ========\n",
      "Training...\n",
      "  Epoch 10, Batch 1/173, Loss: 3.0234, Elapsed: 0:00:12\n",
      "  Epoch 10, Batch 11/173, Loss: 2.9810, Elapsed: 0:04:43\n",
      "  Epoch 10, Batch 21/173, Loss: 2.8441, Elapsed: 0:09:14\n",
      "  Epoch 10, Batch 31/173, Loss: 3.3835, Elapsed: 0:13:44\n",
      "  Epoch 10, Batch 41/173, Loss: 2.9457, Elapsed: 0:18:30\n",
      "  Epoch 10, Batch 51/173, Loss: 3.0394, Elapsed: 0:23:13\n",
      "  Epoch 10, Batch 61/173, Loss: 2.7540, Elapsed: 0:27:50\n",
      "  Epoch 10, Batch 71/173, Loss: 2.7885, Elapsed: 0:32:19\n",
      "  Epoch 10, Batch 81/173, Loss: 3.1460, Elapsed: 0:36:48\n",
      "  Epoch 10, Batch 91/173, Loss: 3.0223, Elapsed: 0:41:28\n",
      "  Epoch 10, Batch 101/173, Loss: 2.9629, Elapsed: 0:46:13\n",
      "  Epoch 10, Batch 111/173, Loss: 3.0514, Elapsed: 0:50:57\n",
      "  Epoch 10, Batch 121/173, Loss: 3.1368, Elapsed: 0:55:36\n",
      "  Epoch 10, Batch 131/173, Loss: 2.7824, Elapsed: 1:00:07\n",
      "  Epoch 10, Batch 141/173, Loss: 2.7794, Elapsed: 1:04:41\n",
      "  Epoch 10, Batch 151/173, Loss: 2.4732, Elapsed: 1:09:28\n",
      "  Epoch 10, Batch 161/173, Loss: 2.9167, Elapsed: 1:14:10\n",
      "  Epoch 10, Batch 171/173, Loss: 2.8469, Elapsed: 1:18:42\n",
      "\n",
      "  Average training loss: 2.94\n",
      "  Training epoch took: 1:19:25\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 2.21\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 11 / 100 ========\n",
      "Training...\n",
      "  Epoch 11, Batch 1/173, Loss: 2.7209, Elapsed: 0:00:12\n",
      "  Epoch 11, Batch 11/173, Loss: 2.9101, Elapsed: 0:04:42\n",
      "  Epoch 11, Batch 21/173, Loss: 2.9230, Elapsed: 0:09:16\n",
      "  Epoch 11, Batch 31/173, Loss: 2.7580, Elapsed: 0:14:01\n",
      "  Epoch 11, Batch 41/173, Loss: 2.7097, Elapsed: 0:18:48\n",
      "  Epoch 11, Batch 51/173, Loss: 2.4861, Elapsed: 0:23:21\n",
      "  Epoch 11, Batch 61/173, Loss: 2.7854, Elapsed: 0:27:52\n",
      "  Epoch 11, Batch 71/173, Loss: 2.5653, Elapsed: 0:32:25\n",
      "  Epoch 11, Batch 81/173, Loss: 2.6469, Elapsed: 0:37:17\n",
      "  Epoch 11, Batch 91/173, Loss: 2.5374, Elapsed: 0:42:02\n",
      "  Epoch 11, Batch 101/173, Loss: 2.5169, Elapsed: 0:46:44\n",
      "  Epoch 11, Batch 111/173, Loss: 2.7201, Elapsed: 0:51:12\n",
      "  Epoch 11, Batch 121/173, Loss: 2.6143, Elapsed: 0:55:43\n",
      "  Epoch 11, Batch 131/173, Loss: 2.5034, Elapsed: 1:00:24\n",
      "  Epoch 11, Batch 141/173, Loss: 2.4541, Elapsed: 1:05:09\n",
      "  Epoch 11, Batch 151/173, Loss: 2.4011, Elapsed: 1:09:49\n",
      "  Epoch 11, Batch 161/173, Loss: 2.4094, Elapsed: 1:14:20\n",
      "  Epoch 11, Batch 171/173, Loss: 2.5299, Elapsed: 1:18:51\n",
      "\n",
      "  Average training loss: 2.65\n",
      "  Training epoch took: 1:19:35\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 2.00\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 12 / 100 ========\n",
      "Training...\n",
      "  Epoch 12, Batch 1/173, Loss: 2.4452, Elapsed: 0:00:12\n",
      "  Epoch 12, Batch 11/173, Loss: 2.4936, Elapsed: 0:04:51\n",
      "  Epoch 12, Batch 21/173, Loss: 2.5565, Elapsed: 0:09:33\n",
      "  Epoch 12, Batch 31/173, Loss: 2.4468, Elapsed: 0:14:20\n",
      "  Epoch 12, Batch 41/173, Loss: 2.3758, Elapsed: 0:18:54\n",
      "  Epoch 12, Batch 51/173, Loss: 2.2409, Elapsed: 0:23:25\n",
      "  Epoch 12, Batch 61/173, Loss: 2.6570, Elapsed: 0:28:04\n",
      "  Epoch 12, Batch 71/173, Loss: 2.2812, Elapsed: 0:32:49\n",
      "  Epoch 12, Batch 81/173, Loss: 2.3440, Elapsed: 0:37:30\n",
      "  Epoch 12, Batch 91/173, Loss: 2.3393, Elapsed: 0:41:59\n",
      "  Epoch 12, Batch 101/173, Loss: 2.3351, Elapsed: 0:46:31\n",
      "  Epoch 12, Batch 111/173, Loss: 2.4579, Elapsed: 0:51:10\n",
      "  Epoch 12, Batch 121/173, Loss: 2.2774, Elapsed: 0:55:53\n",
      "  Epoch 12, Batch 131/173, Loss: 2.4144, Elapsed: 1:00:44\n",
      "  Epoch 12, Batch 141/173, Loss: 2.3220, Elapsed: 1:05:17\n",
      "  Epoch 12, Batch 151/173, Loss: 2.4740, Elapsed: 1:09:46\n",
      "  Epoch 12, Batch 161/173, Loss: 2.3335, Elapsed: 1:14:20\n",
      "  Epoch 12, Batch 171/173, Loss: 2.4288, Elapsed: 1:19:01\n",
      "\n",
      "  Average training loss: 2.43\n",
      "  Training epoch took: 1:19:46\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.85\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 13 / 100 ========\n",
      "Training...\n",
      "  Epoch 13, Batch 1/173, Loss: 2.5560, Elapsed: 0:00:12\n",
      "  Epoch 13, Batch 11/173, Loss: 2.2648, Elapsed: 0:04:58\n",
      "  Epoch 13, Batch 21/173, Loss: 2.1989, Elapsed: 0:09:35\n",
      "  Epoch 13, Batch 31/173, Loss: 2.2714, Elapsed: 0:14:07\n",
      "  Epoch 13, Batch 41/173, Loss: 2.1676, Elapsed: 0:18:40\n",
      "  Epoch 13, Batch 51/173, Loss: 2.1754, Elapsed: 0:23:24\n",
      "  Epoch 13, Batch 61/173, Loss: 2.3804, Elapsed: 0:28:09\n",
      "  Epoch 13, Batch 71/173, Loss: 2.1838, Elapsed: 0:32:42\n",
      "  Epoch 13, Batch 81/173, Loss: 2.3777, Elapsed: 0:37:11\n",
      "  Epoch 13, Batch 91/173, Loss: 2.1435, Elapsed: 0:41:40\n",
      "  Epoch 13, Batch 101/173, Loss: 2.1107, Elapsed: 0:46:22\n",
      "  Epoch 13, Batch 111/173, Loss: 2.3076, Elapsed: 0:51:07\n",
      "  Epoch 13, Batch 121/173, Loss: 2.1097, Elapsed: 0:55:45\n",
      "  Epoch 13, Batch 131/173, Loss: 2.1344, Elapsed: 1:00:13\n",
      "  Epoch 13, Batch 141/173, Loss: 2.0462, Elapsed: 1:04:45\n",
      "  Epoch 13, Batch 151/173, Loss: 2.0023, Elapsed: 1:09:28\n",
      "  Epoch 13, Batch 161/173, Loss: 2.3708, Elapsed: 1:14:21\n",
      "  Epoch 13, Batch 171/173, Loss: 2.0931, Elapsed: 1:19:04\n",
      "\n",
      "  Average training loss: 2.25\n",
      "  Training epoch took: 1:19:50\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.72\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 14 / 100 ========\n",
      "Training...\n",
      "  Epoch 14, Batch 1/173, Loss: 2.2748, Elapsed: 0:00:12\n",
      "  Epoch 14, Batch 11/173, Loss: 2.3380, Elapsed: 0:04:45\n",
      "  Epoch 14, Batch 21/173, Loss: 2.0715, Elapsed: 0:09:13\n",
      "  Epoch 14, Batch 31/173, Loss: 2.1777, Elapsed: 0:13:46\n",
      "  Epoch 14, Batch 41/173, Loss: 1.8212, Elapsed: 0:18:30\n",
      "  Epoch 14, Batch 51/173, Loss: 1.8855, Elapsed: 0:23:13\n",
      "  Epoch 14, Batch 61/173, Loss: 2.2325, Elapsed: 0:27:42\n",
      "  Epoch 14, Batch 71/173, Loss: 2.2314, Elapsed: 0:32:12\n",
      "  Epoch 14, Batch 81/173, Loss: 1.9689, Elapsed: 0:36:42\n",
      "  Epoch 14, Batch 91/173, Loss: 2.1910, Elapsed: 0:41:18\n",
      "  Epoch 14, Batch 101/173, Loss: 2.1313, Elapsed: 0:45:59\n",
      "  Epoch 14, Batch 111/173, Loss: 2.1231, Elapsed: 0:50:47\n",
      "  Epoch 14, Batch 121/173, Loss: 2.0631, Elapsed: 0:55:23\n",
      "  Epoch 14, Batch 131/173, Loss: 2.0394, Elapsed: 0:59:59\n",
      "  Epoch 14, Batch 141/173, Loss: 1.9894, Elapsed: 1:04:35\n",
      "  Epoch 14, Batch 151/173, Loss: 1.8618, Elapsed: 1:09:20\n",
      "  Epoch 14, Batch 161/173, Loss: 1.9243, Elapsed: 1:14:05\n",
      "  Epoch 14, Batch 171/173, Loss: 1.9630, Elapsed: 1:18:33\n",
      "\n",
      "  Average training loss: 2.10\n",
      "  Training epoch took: 1:19:17\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.64\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 15 / 100 ========\n",
      "Training...\n",
      "  Epoch 15, Batch 1/173, Loss: 1.8903, Elapsed: 0:00:12\n",
      "  Epoch 15, Batch 11/173, Loss: 1.9527, Elapsed: 0:04:43\n",
      "  Epoch 15, Batch 21/173, Loss: 1.8727, Elapsed: 0:09:20\n",
      "  Epoch 15, Batch 31/173, Loss: 2.0191, Elapsed: 0:14:03\n",
      "  Epoch 15, Batch 41/173, Loss: 2.1884, Elapsed: 0:18:44\n",
      "  Epoch 15, Batch 51/173, Loss: 1.9515, Elapsed: 0:23:14\n",
      "  Epoch 15, Batch 61/173, Loss: 1.9115, Elapsed: 0:27:43\n",
      "  Epoch 15, Batch 71/173, Loss: 2.1478, Elapsed: 0:32:13\n",
      "  Epoch 15, Batch 81/173, Loss: 1.8369, Elapsed: 0:36:57\n",
      "  Epoch 15, Batch 91/173, Loss: 1.9730, Elapsed: 0:41:44\n",
      "  Epoch 15, Batch 101/173, Loss: 1.8420, Elapsed: 0:46:30\n",
      "  Epoch 15, Batch 111/173, Loss: 1.6712, Elapsed: 0:51:02\n",
      "  Epoch 15, Batch 121/173, Loss: 1.7951, Elapsed: 0:55:33\n",
      "  Epoch 15, Batch 131/173, Loss: 1.9492, Elapsed: 1:00:14\n",
      "  Epoch 15, Batch 141/173, Loss: 2.0327, Elapsed: 1:04:55\n",
      "  Epoch 15, Batch 151/173, Loss: 1.7719, Elapsed: 1:09:33\n",
      "  Epoch 15, Batch 161/173, Loss: 2.1247, Elapsed: 1:14:04\n",
      "  Epoch 15, Batch 171/173, Loss: 1.8138, Elapsed: 1:18:34\n",
      "\n",
      "  Average training loss: 1.98\n",
      "  Training epoch took: 1:19:17\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.58\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 16 / 100 ========\n",
      "Training...\n",
      "  Epoch 16, Batch 1/173, Loss: 1.8674, Elapsed: 0:00:12\n",
      "  Epoch 16, Batch 11/173, Loss: 2.0411, Elapsed: 0:04:50\n",
      "  Epoch 16, Batch 21/173, Loss: 1.8407, Elapsed: 0:09:38\n",
      "  Epoch 16, Batch 31/173, Loss: 1.8336, Elapsed: 0:14:23\n",
      "  Epoch 16, Batch 41/173, Loss: 2.2765, Elapsed: 0:18:54\n",
      "  Epoch 16, Batch 51/173, Loss: 1.8351, Elapsed: 0:23:26\n",
      "  Epoch 16, Batch 61/173, Loss: 1.9266, Elapsed: 0:28:02\n",
      "  Epoch 16, Batch 71/173, Loss: 1.9043, Elapsed: 0:32:45\n",
      "  Epoch 16, Batch 81/173, Loss: 1.9012, Elapsed: 0:37:28\n",
      "  Epoch 16, Batch 91/173, Loss: 1.8493, Elapsed: 0:41:59\n",
      "  Epoch 16, Batch 101/173, Loss: 1.9828, Elapsed: 0:46:29\n",
      "  Epoch 16, Batch 111/173, Loss: 1.6652, Elapsed: 0:50:59\n",
      "  Epoch 16, Batch 121/173, Loss: 1.8279, Elapsed: 0:55:45\n",
      "  Epoch 16, Batch 131/173, Loss: 1.8357, Elapsed: 1:00:26\n",
      "  Epoch 16, Batch 141/173, Loss: 2.0842, Elapsed: 1:05:10\n",
      "  Epoch 16, Batch 151/173, Loss: 1.7083, Elapsed: 1:09:43\n",
      "  Epoch 16, Batch 161/173, Loss: 1.8791, Elapsed: 1:14:14\n",
      "  Epoch 16, Batch 171/173, Loss: 1.7444, Elapsed: 1:18:57\n",
      "\n",
      "  Average training loss: 1.89\n",
      "  Training epoch took: 1:19:45\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.53\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 17 / 100 ========\n",
      "Training...\n",
      "  Epoch 17, Batch 1/173, Loss: 1.8424, Elapsed: 0:00:12\n",
      "  Epoch 17, Batch 11/173, Loss: 1.8681, Elapsed: 0:04:59\n",
      "  Epoch 17, Batch 21/173, Loss: 1.8553, Elapsed: 0:09:35\n",
      "  Epoch 17, Batch 31/173, Loss: 1.7486, Elapsed: 0:14:06\n",
      "  Epoch 17, Batch 41/173, Loss: 1.8462, Elapsed: 0:18:36\n",
      "  Epoch 17, Batch 51/173, Loss: 1.7925, Elapsed: 0:23:16\n",
      "  Epoch 17, Batch 61/173, Loss: 1.8443, Elapsed: 0:28:03\n",
      "  Epoch 17, Batch 71/173, Loss: 2.0305, Elapsed: 0:32:47\n",
      "  Epoch 17, Batch 81/173, Loss: 1.6176, Elapsed: 0:37:21\n",
      "  Epoch 17, Batch 91/173, Loss: 1.6426, Elapsed: 0:41:52\n",
      "  Epoch 17, Batch 101/173, Loss: 1.7932, Elapsed: 0:46:29\n",
      "  Epoch 17, Batch 111/173, Loss: 1.6375, Elapsed: 0:51:13\n",
      "  Epoch 17, Batch 121/173, Loss: 1.9516, Elapsed: 0:55:59\n",
      "  Epoch 17, Batch 131/173, Loss: 1.8158, Elapsed: 1:00:30\n",
      "  Epoch 17, Batch 141/173, Loss: 1.5078, Elapsed: 1:05:00\n",
      "  Epoch 17, Batch 151/173, Loss: 1.9367, Elapsed: 1:09:33\n",
      "  Epoch 17, Batch 161/173, Loss: 1.7694, Elapsed: 1:14:21\n",
      "  Epoch 17, Batch 171/173, Loss: 1.7591, Elapsed: 1:19:04\n",
      "\n",
      "  Average training loss: 1.82\n",
      "  Training epoch took: 1:19:49\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.49\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 18 / 100 ========\n",
      "Training...\n",
      "  Epoch 18, Batch 1/173, Loss: 1.7838, Elapsed: 0:00:12\n",
      "  Epoch 18, Batch 11/173, Loss: 1.6905, Elapsed: 0:04:51\n",
      "  Epoch 18, Batch 21/173, Loss: 1.7497, Elapsed: 0:09:20\n",
      "  Epoch 18, Batch 31/173, Loss: 1.8034, Elapsed: 0:13:51\n",
      "  Epoch 18, Batch 41/173, Loss: 1.9007, Elapsed: 0:18:37\n",
      "  Epoch 18, Batch 51/173, Loss: 1.7784, Elapsed: 0:23:22\n",
      "  Epoch 18, Batch 61/173, Loss: 1.8912, Elapsed: 0:27:55\n",
      "  Epoch 18, Batch 71/173, Loss: 1.8028, Elapsed: 0:32:26\n",
      "  Epoch 18, Batch 81/173, Loss: 2.0420, Elapsed: 0:36:55\n",
      "  Epoch 18, Batch 91/173, Loss: 1.7705, Elapsed: 0:41:34\n",
      "  Epoch 18, Batch 101/173, Loss: 1.8925, Elapsed: 0:46:17\n",
      "  Epoch 18, Batch 111/173, Loss: 1.9077, Elapsed: 0:50:58\n",
      "  Epoch 18, Batch 121/173, Loss: 1.8189, Elapsed: 0:55:29\n",
      "  Epoch 18, Batch 131/173, Loss: 1.8685, Elapsed: 0:59:58\n",
      "  Epoch 18, Batch 141/173, Loss: 1.6084, Elapsed: 1:04:32\n",
      "  Epoch 18, Batch 151/173, Loss: 1.6293, Elapsed: 1:09:15\n",
      "  Epoch 18, Batch 161/173, Loss: 1.7340, Elapsed: 1:13:59\n",
      "  Epoch 18, Batch 171/173, Loss: 1.8991, Elapsed: 1:18:42\n",
      "\n",
      "  Average training loss: 1.76\n",
      "  Training epoch took: 1:19:25\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.45\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 19 / 100 ========\n",
      "Training...\n",
      "  Epoch 19, Batch 1/173, Loss: 1.8554, Elapsed: 0:00:12\n",
      "  Epoch 19, Batch 11/173, Loss: 1.7511, Elapsed: 0:04:42\n",
      "  Epoch 19, Batch 21/173, Loss: 1.7719, Elapsed: 0:09:18\n",
      "  Epoch 19, Batch 31/173, Loss: 1.7193, Elapsed: 0:14:03\n",
      "  Epoch 19, Batch 41/173, Loss: 1.6159, Elapsed: 0:18:47\n",
      "  Epoch 19, Batch 51/173, Loss: 1.7440, Elapsed: 0:23:20\n",
      "  Epoch 19, Batch 61/173, Loss: 1.7301, Elapsed: 0:27:50\n",
      "  Epoch 19, Batch 71/173, Loss: 1.6227, Elapsed: 0:32:19\n",
      "  Epoch 19, Batch 81/173, Loss: 1.8765, Elapsed: 0:37:07\n",
      "  Epoch 19, Batch 91/173, Loss: 1.7500, Elapsed: 0:41:56\n",
      "  Epoch 19, Batch 101/173, Loss: 1.5692, Elapsed: 0:46:34\n",
      "  Epoch 19, Batch 111/173, Loss: 1.5834, Elapsed: 0:51:05\n",
      "  Epoch 19, Batch 121/173, Loss: 1.7386, Elapsed: 0:55:37\n",
      "  Epoch 19, Batch 131/173, Loss: 1.6266, Elapsed: 1:00:20\n",
      "  Epoch 19, Batch 141/173, Loss: 1.5259, Elapsed: 1:05:03\n",
      "  Epoch 19, Batch 151/173, Loss: 1.6538, Elapsed: 1:09:50\n",
      "  Epoch 19, Batch 161/173, Loss: 1.7698, Elapsed: 1:14:23\n",
      "  Epoch 19, Batch 171/173, Loss: 1.5522, Elapsed: 1:18:55\n",
      "\n",
      "  Average training loss: 1.70\n",
      "  Training epoch took: 1:19:39\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.42\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 20 / 100 ========\n",
      "Training...\n",
      "  Epoch 20, Batch 1/173, Loss: 1.7545, Elapsed: 0:00:12\n",
      "  Epoch 20, Batch 11/173, Loss: 1.4986, Elapsed: 0:40:30\n",
      "  Epoch 20, Batch 21/173, Loss: 1.5039, Elapsed: 0:45:20\n",
      "  Epoch 20, Batch 31/173, Loss: 1.6946, Elapsed: 0:50:03\n",
      "  Epoch 20, Batch 41/173, Loss: 1.7771, Elapsed: 0:54:41\n",
      "  Epoch 20, Batch 51/173, Loss: 1.7609, Elapsed: 0:59:47\n",
      "  Epoch 20, Batch 61/173, Loss: 1.5760, Elapsed: 1:04:52\n",
      "  Epoch 20, Batch 71/173, Loss: 1.5585, Elapsed: 1:09:59\n",
      "  Epoch 20, Batch 81/173, Loss: 1.6569, Elapsed: 1:14:52\n",
      "  Epoch 20, Batch 91/173, Loss: 1.5757, Elapsed: 1:20:13\n",
      "  Epoch 20, Batch 101/173, Loss: 1.7123, Elapsed: 1:25:12\n",
      "  Epoch 20, Batch 111/173, Loss: 1.7048, Elapsed: 1:29:58\n",
      "  Epoch 20, Batch 121/173, Loss: 1.4668, Elapsed: 1:34:56\n",
      "  Epoch 20, Batch 131/173, Loss: 1.5272, Elapsed: 1:43:52\n",
      "  Epoch 20, Batch 141/173, Loss: 1.6385, Elapsed: 1:48:32\n",
      "  Epoch 20, Batch 151/173, Loss: 1.5513, Elapsed: 1:53:10\n",
      "  Epoch 20, Batch 161/173, Loss: 1.5596, Elapsed: 1:58:04\n",
      "  Epoch 20, Batch 171/173, Loss: 1.6037, Elapsed: 2:03:12\n",
      "\n",
      "  Average training loss: 1.65\n",
      "  Training epoch took: 2:04:03\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.38\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 21 / 100 ========\n",
      "Training...\n",
      "  Epoch 21, Batch 1/173, Loss: 1.6307, Elapsed: 0:00:12\n",
      "  Epoch 21, Batch 11/173, Loss: 1.6606, Elapsed: 0:04:56\n",
      "  Epoch 21, Batch 21/173, Loss: 1.7731, Elapsed: 0:14:30\n",
      "  Epoch 21, Batch 31/173, Loss: 1.6388, Elapsed: 0:20:01\n",
      "  Epoch 21, Batch 41/173, Loss: 1.6983, Elapsed: 0:25:14\n",
      "  Epoch 21, Batch 51/173, Loss: 1.5902, Elapsed: 0:30:17\n",
      "  Epoch 21, Batch 61/173, Loss: 1.6385, Elapsed: 0:35:49\n",
      "  Epoch 21, Batch 71/173, Loss: 1.5834, Elapsed: 0:41:22\n",
      "  Epoch 21, Batch 81/173, Loss: 1.5047, Elapsed: 0:46:36\n",
      "  Epoch 21, Batch 91/173, Loss: 1.4978, Elapsed: 0:52:00\n",
      "  Epoch 21, Batch 101/173, Loss: 1.5510, Elapsed: 0:57:55\n",
      "  Epoch 21, Batch 111/173, Loss: 1.5463, Elapsed: 1:03:18\n",
      "  Epoch 21, Batch 121/173, Loss: 1.5334, Elapsed: 1:09:34\n",
      "  Epoch 21, Batch 131/173, Loss: 1.7048, Elapsed: 1:14:51\n",
      "  Epoch 21, Batch 141/173, Loss: 1.7646, Elapsed: 1:19:50\n",
      "  Epoch 21, Batch 151/173, Loss: 1.5174, Elapsed: 1:24:51\n",
      "  Epoch 21, Batch 161/173, Loss: 1.6109, Elapsed: 1:30:08\n",
      "  Epoch 21, Batch 171/173, Loss: 1.7091, Elapsed: 1:35:18\n",
      "\n",
      "  Average training loss: 1.61\n",
      "  Training epoch took: 1:36:06\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.34\n",
      "  Validation took: 0:01:35\n",
      "\n",
      "======== Epoch 22 / 100 ========\n",
      "Training...\n",
      "  Epoch 22, Batch 1/173, Loss: 1.7045, Elapsed: 0:00:14\n",
      "  Epoch 22, Batch 11/173, Loss: 1.7429, Elapsed: 0:05:04\n",
      "  Epoch 22, Batch 21/173, Loss: 1.5664, Elapsed: 0:10:35\n",
      "  Epoch 22, Batch 31/173, Loss: 1.6242, Elapsed: 0:16:10\n",
      "  Epoch 22, Batch 41/173, Loss: 1.5589, Elapsed: 0:21:01\n",
      "  Epoch 22, Batch 51/173, Loss: 1.6609, Elapsed: 0:25:59\n",
      "  Epoch 22, Batch 61/173, Loss: 1.6638, Elapsed: 0:31:04\n",
      "  Epoch 22, Batch 71/173, Loss: 1.6224, Elapsed: 0:36:36\n",
      "  Epoch 22, Batch 81/173, Loss: 1.6778, Elapsed: 0:41:32\n",
      "  Epoch 22, Batch 91/173, Loss: 1.6421, Elapsed: 0:48:45\n",
      "  Epoch 22, Batch 101/173, Loss: 1.4772, Elapsed: 1:05:11\n",
      "  Epoch 22, Batch 111/173, Loss: 1.6073, Elapsed: 1:10:06\n",
      "  Epoch 22, Batch 121/173, Loss: 1.5075, Elapsed: 1:14:35\n",
      "  Epoch 22, Batch 131/173, Loss: 1.5832, Elapsed: 1:19:09\n",
      "  Epoch 22, Batch 141/173, Loss: 1.7638, Elapsed: 1:23:55\n",
      "  Epoch 22, Batch 151/173, Loss: 1.4538, Elapsed: 1:28:46\n",
      "  Epoch 22, Batch 161/173, Loss: 1.5396, Elapsed: 1:33:18\n",
      "  Epoch 22, Batch 171/173, Loss: 1.5513, Elapsed: 1:37:50\n",
      "\n",
      "  Average training loss: 1.56\n",
      "  Training epoch took: 1:38:33\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.32\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 23 / 100 ========\n",
      "Training...\n",
      "  Epoch 23, Batch 1/173, Loss: 1.4143, Elapsed: 0:00:12\n",
      "  Epoch 23, Batch 11/173, Loss: 1.4280, Elapsed: 0:04:45\n",
      "  Epoch 23, Batch 21/173, Loss: 1.6114, Elapsed: 0:09:31\n",
      "  Epoch 23, Batch 31/173, Loss: 1.4573, Elapsed: 0:14:14\n",
      "  Epoch 23, Batch 41/173, Loss: 1.4508, Elapsed: 0:18:48\n",
      "  Epoch 23, Batch 51/173, Loss: 1.3912, Elapsed: 0:23:16\n",
      "  Epoch 23, Batch 61/173, Loss: 1.5837, Elapsed: 0:29:45\n",
      "  Epoch 23, Batch 71/173, Loss: 1.4238, Elapsed: 0:36:51\n",
      "  Epoch 23, Batch 81/173, Loss: 1.6141, Elapsed: 1:28:02\n",
      "  Epoch 23, Batch 91/173, Loss: 1.4903, Elapsed: 1:32:42\n",
      "  Epoch 23, Batch 101/173, Loss: 1.4010, Elapsed: 1:37:12\n",
      "  Epoch 23, Batch 111/173, Loss: 1.6360, Elapsed: 1:41:54\n",
      "  Epoch 23, Batch 121/173, Loss: 1.5158, Elapsed: 1:46:47\n",
      "  Epoch 23, Batch 131/173, Loss: 1.5169, Elapsed: 1:51:34\n",
      "  Epoch 23, Batch 141/173, Loss: 1.6716, Elapsed: 1:56:14\n",
      "  Epoch 23, Batch 151/173, Loss: 1.5127, Elapsed: 2:00:49\n",
      "  Epoch 23, Batch 161/173, Loss: 1.4038, Elapsed: 2:05:48\n",
      "  Epoch 23, Batch 171/173, Loss: 1.4266, Elapsed: 2:10:48\n",
      "\n",
      "  Average training loss: 1.53\n",
      "  Training epoch took: 2:11:37\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.30\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 24 / 100 ========\n",
      "Training...\n",
      "  Epoch 24, Batch 1/173, Loss: 1.3252, Elapsed: 0:00:12\n",
      "  Epoch 24, Batch 11/173, Loss: 1.4207, Elapsed: 0:05:02\n",
      "  Epoch 24, Batch 21/173, Loss: 1.2884, Elapsed: 0:09:49\n",
      "  Epoch 24, Batch 31/173, Loss: 1.4831, Elapsed: 0:14:48\n",
      "  Epoch 24, Batch 41/173, Loss: 1.4708, Elapsed: 0:19:51\n",
      "  Epoch 24, Batch 51/173, Loss: 1.5518, Elapsed: 0:24:25\n",
      "  Epoch 24, Batch 61/173, Loss: 1.5114, Elapsed: 0:28:59\n",
      "  Epoch 24, Batch 71/173, Loss: 1.6189, Elapsed: 0:33:48\n",
      "  Epoch 24, Batch 81/173, Loss: 1.5738, Elapsed: 0:38:37\n",
      "  Epoch 24, Batch 91/173, Loss: 1.6409, Elapsed: 0:43:21\n",
      "  Epoch 24, Batch 101/173, Loss: 1.5352, Elapsed: 0:47:53\n",
      "  Epoch 24, Batch 111/173, Loss: 1.5860, Elapsed: 0:52:27\n",
      "  Epoch 24, Batch 121/173, Loss: 1.5613, Elapsed: 2:10:01\n",
      "  Epoch 24, Batch 131/173, Loss: 1.5778, Elapsed: 2:15:54\n",
      "  Epoch 24, Batch 141/173, Loss: 1.5398, Elapsed: 2:20:46\n",
      "  Epoch 24, Batch 151/173, Loss: 1.5425, Elapsed: 2:25:22\n",
      "  Epoch 24, Batch 161/173, Loss: 1.4229, Elapsed: 2:30:01\n",
      "  Epoch 24, Batch 171/173, Loss: 1.4366, Elapsed: 2:34:56\n",
      "\n",
      "  Average training loss: 1.49\n",
      "  Training epoch took: 2:35:46\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.28\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 25 / 100 ========\n",
      "Training...\n",
      "  Epoch 25, Batch 1/173, Loss: 1.5780, Elapsed: 0:00:12\n",
      "  Epoch 25, Batch 11/173, Loss: 1.5255, Elapsed: 0:05:08\n",
      "  Epoch 25, Batch 21/173, Loss: 1.4850, Elapsed: 0:09:45\n",
      "  Epoch 25, Batch 31/173, Loss: 1.5218, Elapsed: 0:14:22\n",
      "  Epoch 25, Batch 41/173, Loss: 1.3874, Elapsed: 0:19:02\n",
      "  Epoch 25, Batch 51/173, Loss: 1.3904, Elapsed: 0:23:50\n",
      "  Epoch 25, Batch 61/173, Loss: 1.4335, Elapsed: 0:28:35\n",
      "  Epoch 25, Batch 71/173, Loss: 1.3261, Elapsed: 0:33:13\n",
      "  Epoch 25, Batch 81/173, Loss: 1.3124, Elapsed: 0:37:47\n",
      "  Epoch 25, Batch 91/173, Loss: 1.4558, Elapsed: 0:42:34\n",
      "  Epoch 25, Batch 101/173, Loss: 1.4802, Elapsed: 0:47:32\n",
      "  Epoch 25, Batch 111/173, Loss: 1.5257, Elapsed: 0:52:26\n",
      "  Epoch 25, Batch 121/173, Loss: 1.6383, Elapsed: 0:57:03\n",
      "  Epoch 25, Batch 131/173, Loss: 1.4626, Elapsed: 1:01:45\n",
      "  Epoch 25, Batch 141/173, Loss: 1.4968, Elapsed: 1:06:35\n",
      "  Epoch 25, Batch 151/173, Loss: 1.6084, Elapsed: 1:11:27\n",
      "  Epoch 25, Batch 161/173, Loss: 1.5055, Elapsed: 1:16:02\n",
      "  Epoch 25, Batch 171/173, Loss: 1.5633, Elapsed: 1:20:36\n",
      "\n",
      "  Average training loss: 1.46\n",
      "  Training epoch took: 1:21:20\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.26\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 26 / 100 ========\n",
      "Training...\n",
      "  Epoch 26, Batch 1/173, Loss: 1.3987, Elapsed: 0:00:12\n",
      "  Epoch 26, Batch 11/173, Loss: 1.2996, Elapsed: 0:05:03\n",
      "  Epoch 26, Batch 21/173, Loss: 1.5696, Elapsed: 0:09:59\n",
      "  Epoch 26, Batch 31/173, Loss: 1.3117, Elapsed: 0:14:39\n",
      "  Epoch 26, Batch 41/173, Loss: 1.4672, Elapsed: 0:19:16\n",
      "  Epoch 26, Batch 51/173, Loss: 1.4381, Elapsed: 0:23:55\n",
      "  Epoch 26, Batch 61/173, Loss: 1.4952, Elapsed: 0:28:49\n",
      "  Epoch 26, Batch 71/173, Loss: 1.4773, Elapsed: 0:33:39\n",
      "  Epoch 26, Batch 81/173, Loss: 1.5777, Elapsed: 0:38:17\n",
      "  Epoch 26, Batch 91/173, Loss: 1.3846, Elapsed: 0:42:51\n",
      "  Epoch 26, Batch 101/173, Loss: 1.4433, Elapsed: 0:47:40\n",
      "  Epoch 26, Batch 111/173, Loss: 1.4444, Elapsed: 0:52:42\n",
      "  Epoch 26, Batch 121/173, Loss: 1.2751, Elapsed: 0:57:40\n",
      "  Epoch 26, Batch 131/173, Loss: 1.3780, Elapsed: 1:02:18\n",
      "  Epoch 26, Batch 141/173, Loss: 1.5400, Elapsed: 1:06:58\n",
      "  Epoch 26, Batch 151/173, Loss: 1.4524, Elapsed: 1:11:54\n",
      "  Epoch 26, Batch 161/173, Loss: 1.3181, Elapsed: 1:16:41\n",
      "  Epoch 26, Batch 171/173, Loss: 1.2858, Elapsed: 1:21:16\n",
      "\n",
      "  Average training loss: 1.43\n",
      "  Training epoch took: 1:21:59\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.25\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 27 / 100 ========\n",
      "Training...\n",
      "  Epoch 27, Batch 1/173, Loss: 1.5302, Elapsed: 0:00:12\n",
      "  Epoch 27, Batch 11/173, Loss: 1.6100, Elapsed: 0:04:48\n",
      "  Epoch 27, Batch 21/173, Loss: 1.4124, Elapsed: 0:09:39\n",
      "  Epoch 27, Batch 31/173, Loss: 1.4854, Elapsed: 0:14:34\n",
      "  Epoch 27, Batch 41/173, Loss: 1.2950, Elapsed: 0:19:16\n",
      "  Epoch 27, Batch 51/173, Loss: 1.3573, Elapsed: 0:23:52\n",
      "  Epoch 27, Batch 61/173, Loss: 1.3364, Elapsed: 0:28:30\n",
      "  Epoch 27, Batch 71/173, Loss: 1.5038, Elapsed: 0:33:24\n",
      "  Epoch 27, Batch 81/173, Loss: 1.4073, Elapsed: 0:38:19\n",
      "  Epoch 27, Batch 91/173, Loss: 1.3885, Elapsed: 0:43:05\n",
      "  Epoch 27, Batch 101/173, Loss: 1.3221, Elapsed: 0:47:44\n",
      "  Epoch 27, Batch 111/173, Loss: 1.2981, Elapsed: 0:52:34\n",
      "  Epoch 27, Batch 121/173, Loss: 1.3212, Elapsed: 0:57:29\n",
      "  Epoch 27, Batch 131/173, Loss: 1.4679, Elapsed: 1:02:13\n",
      "  Epoch 27, Batch 141/173, Loss: 1.3203, Elapsed: 1:06:49\n",
      "  Epoch 27, Batch 151/173, Loss: 1.1800, Elapsed: 1:11:28\n",
      "  Epoch 27, Batch 161/173, Loss: 1.1823, Elapsed: 1:16:23\n",
      "  Epoch 27, Batch 171/173, Loss: 1.5110, Elapsed: 1:21:14\n",
      "\n",
      "  Average training loss: 1.41\n",
      "  Training epoch took: 1:21:57\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.24\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 28 / 100 ========\n",
      "Training...\n",
      "  Epoch 28, Batch 1/173, Loss: 1.3557, Elapsed: 0:00:12\n",
      "  Epoch 28, Batch 11/173, Loss: 1.4419, Elapsed: 0:04:49\n",
      "  Epoch 28, Batch 21/173, Loss: 1.3849, Elapsed: 0:09:24\n",
      "  Epoch 28, Batch 31/173, Loss: 1.4607, Elapsed: 0:14:16\n",
      "  Epoch 28, Batch 41/173, Loss: 1.3376, Elapsed: 0:19:14\n",
      "  Epoch 28, Batch 51/173, Loss: 1.4494, Elapsed: 0:23:56\n",
      "  Epoch 28, Batch 61/173, Loss: 1.2248, Elapsed: 0:28:29\n",
      "  Epoch 28, Batch 71/173, Loss: 1.5183, Elapsed: 0:33:13\n",
      "  Epoch 28, Batch 81/173, Loss: 1.4054, Elapsed: 0:38:07\n",
      "  Epoch 28, Batch 91/173, Loss: 1.3019, Elapsed: 0:43:04\n",
      "  Epoch 28, Batch 101/173, Loss: 1.5275, Elapsed: 0:47:45\n",
      "  Epoch 28, Batch 111/173, Loss: 1.4320, Elapsed: 0:52:22\n",
      "  Epoch 28, Batch 121/173, Loss: 1.5533, Elapsed: 0:57:13\n",
      "  Epoch 28, Batch 131/173, Loss: 1.3578, Elapsed: 1:02:11\n",
      "  Epoch 28, Batch 141/173, Loss: 1.4406, Elapsed: 1:06:53\n",
      "  Epoch 28, Batch 151/173, Loss: 1.4739, Elapsed: 1:11:32\n",
      "  Epoch 28, Batch 161/173, Loss: 1.3686, Elapsed: 1:16:17\n",
      "  Epoch 28, Batch 171/173, Loss: 1.2312, Elapsed: 1:21:22\n",
      "\n",
      "  Average training loss: 1.39\n",
      "  Training epoch took: 1:22:10\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.22\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 29 / 100 ========\n",
      "Training...\n",
      "  Epoch 29, Batch 1/173, Loss: 1.3723, Elapsed: 0:00:12\n",
      "  Epoch 29, Batch 11/173, Loss: 1.3569, Elapsed: 0:05:00\n",
      "  Epoch 29, Batch 21/173, Loss: 1.2804, Elapsed: 0:09:36\n",
      "  Epoch 29, Batch 31/173, Loss: 1.3852, Elapsed: 0:14:16\n",
      "  Epoch 29, Batch 41/173, Loss: 1.2531, Elapsed: 0:19:13\n",
      "  Epoch 29, Batch 51/173, Loss: 1.3786, Elapsed: 0:24:10\n",
      "  Epoch 29, Batch 61/173, Loss: 1.3079, Elapsed: 0:28:55\n",
      "  Epoch 29, Batch 71/173, Loss: 1.3519, Elapsed: 0:33:30\n",
      "  Epoch 29, Batch 81/173, Loss: 1.4118, Elapsed: 0:38:19\n",
      "  Epoch 29, Batch 91/173, Loss: 1.3231, Elapsed: 0:43:18\n",
      "  Epoch 29, Batch 101/173, Loss: 1.3320, Elapsed: 0:48:04\n",
      "  Epoch 29, Batch 111/173, Loss: 1.3288, Elapsed: 0:52:38\n",
      "  Epoch 29, Batch 121/173, Loss: 1.4281, Elapsed: 0:57:13\n",
      "  Epoch 29, Batch 131/173, Loss: 1.4257, Elapsed: 1:02:10\n",
      "  Epoch 29, Batch 141/173, Loss: 1.2988, Elapsed: 1:07:07\n",
      "  Epoch 29, Batch 151/173, Loss: 1.3428, Elapsed: 1:11:52\n",
      "  Epoch 29, Batch 161/173, Loss: 1.3659, Elapsed: 1:16:27\n",
      "  Epoch 29, Batch 171/173, Loss: 1.1806, Elapsed: 1:21:16\n",
      "\n",
      "  Average training loss: 1.36\n",
      "  Training epoch took: 1:22:02\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.21\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 30 / 100 ========\n",
      "Training...\n",
      "  Epoch 30, Batch 1/173, Loss: 1.4143, Elapsed: 0:00:14\n",
      "  Epoch 30, Batch 11/173, Loss: 1.3914, Elapsed: 0:05:11\n",
      "  Epoch 30, Batch 21/173, Loss: 1.3461, Elapsed: 0:09:55\n",
      "  Epoch 30, Batch 31/173, Loss: 1.2620, Elapsed: 0:14:31\n",
      "  Epoch 30, Batch 41/173, Loss: 1.3688, Elapsed: 0:19:16\n",
      "  Epoch 30, Batch 51/173, Loss: 1.4133, Elapsed: 0:24:13\n",
      "  Epoch 30, Batch 61/173, Loss: 1.3847, Elapsed: 0:29:06\n",
      "  Epoch 30, Batch 71/173, Loss: 1.3077, Elapsed: 0:33:43\n",
      "  Epoch 30, Batch 81/173, Loss: 1.3319, Elapsed: 0:38:21\n",
      "  Epoch 30, Batch 91/173, Loss: 1.3297, Elapsed: 0:43:09\n",
      "  Epoch 30, Batch 101/173, Loss: 1.2179, Elapsed: 0:48:08\n",
      "  Epoch 30, Batch 111/173, Loss: 1.3060, Elapsed: 0:52:46\n",
      "  Epoch 30, Batch 121/173, Loss: 1.4067, Elapsed: 0:57:22\n",
      "  Epoch 30, Batch 131/173, Loss: 1.3674, Elapsed: 1:02:03\n",
      "  Epoch 30, Batch 141/173, Loss: 1.3285, Elapsed: 1:06:55\n",
      "  Epoch 30, Batch 151/173, Loss: 1.4729, Elapsed: 1:11:50\n",
      "  Epoch 30, Batch 161/173, Loss: 1.3595, Elapsed: 1:16:36\n",
      "  Epoch 30, Batch 171/173, Loss: 1.1204, Elapsed: 1:21:08\n",
      "\n",
      "  Average training loss: 1.34\n",
      "  Training epoch took: 1:21:53\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.20\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 31 / 100 ========\n",
      "Training...\n",
      "  Epoch 31, Batch 1/173, Loss: 1.3227, Elapsed: 0:00:12\n",
      "  Epoch 31, Batch 11/173, Loss: 1.3851, Elapsed: 0:04:57\n",
      "  Epoch 31, Batch 21/173, Loss: 1.3267, Elapsed: 0:09:56\n",
      "  Epoch 31, Batch 31/173, Loss: 1.2284, Elapsed: 0:14:39\n",
      "  Epoch 31, Batch 41/173, Loss: 1.1672, Elapsed: 0:19:16\n",
      "  Epoch 31, Batch 51/173, Loss: 1.1271, Elapsed: 0:23:58\n",
      "  Epoch 31, Batch 61/173, Loss: 1.2745, Elapsed: 0:28:53\n",
      "  Epoch 31, Batch 71/173, Loss: 1.3148, Elapsed: 0:33:51\n",
      "  Epoch 31, Batch 81/173, Loss: 1.2724, Elapsed: 0:38:32\n",
      "  Epoch 31, Batch 91/173, Loss: 1.2866, Elapsed: 0:43:08\n",
      "  Epoch 31, Batch 101/173, Loss: 1.3069, Elapsed: 0:48:01\n",
      "  Epoch 31, Batch 111/173, Loss: 1.3033, Elapsed: 0:52:59\n",
      "  Epoch 31, Batch 121/173, Loss: 1.3748, Elapsed: 0:57:40\n",
      "  Epoch 31, Batch 131/173, Loss: 1.4528, Elapsed: 1:02:15\n",
      "  Epoch 31, Batch 141/173, Loss: 1.2844, Elapsed: 1:07:00\n",
      "  Epoch 31, Batch 151/173, Loss: 1.4387, Elapsed: 1:11:58\n",
      "  Epoch 31, Batch 161/173, Loss: 1.3711, Elapsed: 1:16:54\n",
      "  Epoch 31, Batch 171/173, Loss: 1.3104, Elapsed: 1:21:36\n",
      "\n",
      "  Average training loss: 1.33\n",
      "  Training epoch took: 1:22:20\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.19\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 32 / 100 ========\n",
      "Training...\n",
      "  Epoch 32, Batch 1/173, Loss: 1.3469, Elapsed: 0:00:12\n",
      "  Epoch 32, Batch 11/173, Loss: 1.2201, Elapsed: 0:04:48\n",
      "  Epoch 32, Batch 21/173, Loss: 1.2797, Elapsed: 0:09:48\n",
      "  Epoch 32, Batch 31/173, Loss: 1.3324, Elapsed: 0:14:41\n",
      "  Epoch 32, Batch 41/173, Loss: 1.2585, Elapsed: 0:19:14\n",
      "  Epoch 32, Batch 51/173, Loss: 1.2112, Elapsed: 0:23:50\n",
      "  Epoch 32, Batch 61/173, Loss: 1.2428, Elapsed: 0:28:39\n",
      "  Epoch 32, Batch 71/173, Loss: 1.3263, Elapsed: 0:33:36\n",
      "  Epoch 32, Batch 81/173, Loss: 1.2501, Elapsed: 0:38:22\n",
      "  Epoch 32, Batch 91/173, Loss: 1.3610, Elapsed: 0:42:59\n",
      "  Epoch 32, Batch 101/173, Loss: 1.3761, Elapsed: 0:47:39\n",
      "  Epoch 32, Batch 111/173, Loss: 1.3133, Elapsed: 0:52:34\n",
      "  Epoch 32, Batch 121/173, Loss: 1.2536, Elapsed: 0:57:29\n",
      "  Epoch 32, Batch 131/173, Loss: 1.2370, Elapsed: 1:02:16\n",
      "  Epoch 32, Batch 141/173, Loss: 1.3867, Elapsed: 1:06:47\n",
      "  Epoch 32, Batch 151/173, Loss: 1.3350, Elapsed: 1:11:27\n",
      "  Epoch 32, Batch 161/173, Loss: 1.2908, Elapsed: 1:16:17\n",
      "  Epoch 32, Batch 171/173, Loss: 1.1557, Elapsed: 1:21:00\n",
      "\n",
      "  Average training loss: 1.30\n",
      "  Training epoch took: 1:21:44\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.19\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 33 / 100 ========\n",
      "Training...\n",
      "  Epoch 33, Batch 1/173, Loss: 1.3773, Elapsed: 0:00:12\n",
      "  Epoch 33, Batch 11/173, Loss: 1.1722, Elapsed: 0:04:44\n",
      "  Epoch 33, Batch 21/173, Loss: 1.2381, Elapsed: 0:09:16\n",
      "  Epoch 33, Batch 31/173, Loss: 1.2491, Elapsed: 0:13:59\n",
      "  Epoch 33, Batch 41/173, Loss: 1.1301, Elapsed: 0:18:47\n",
      "  Epoch 33, Batch 51/173, Loss: 1.3709, Elapsed: 0:23:39\n",
      "  Epoch 33, Batch 61/173, Loss: 1.1430, Elapsed: 0:28:11\n",
      "  Epoch 33, Batch 71/173, Loss: 1.3620, Elapsed: 0:32:45\n",
      "  Epoch 33, Batch 81/173, Loss: 1.2171, Elapsed: 0:37:33\n",
      "  Epoch 33, Batch 91/173, Loss: 1.3085, Elapsed: 0:42:22\n",
      "  Epoch 33, Batch 101/173, Loss: 1.2374, Elapsed: 0:47:00\n",
      "  Epoch 33, Batch 111/173, Loss: 1.2508, Elapsed: 0:51:31\n",
      "  Epoch 33, Batch 121/173, Loss: 1.3268, Elapsed: 0:56:05\n",
      "  Epoch 33, Batch 131/173, Loss: 1.2487, Elapsed: 1:00:56\n",
      "  Epoch 33, Batch 141/173, Loss: 1.2596, Elapsed: 1:05:45\n",
      "  Epoch 33, Batch 151/173, Loss: 1.2910, Elapsed: 1:10:35\n",
      "  Epoch 33, Batch 161/173, Loss: 1.3417, Elapsed: 1:15:10\n",
      "  Epoch 33, Batch 171/173, Loss: 1.3267, Elapsed: 1:19:47\n",
      "\n",
      "  Average training loss: 1.28\n",
      "  Training epoch took: 1:20:34\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.18\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 34 / 100 ========\n",
      "Training...\n",
      "  Epoch 34, Batch 1/173, Loss: 1.3679, Elapsed: 0:00:12\n",
      "  Epoch 34, Batch 11/173, Loss: 1.2753, Elapsed: 0:05:04\n",
      "  Epoch 34, Batch 21/173, Loss: 1.1574, Elapsed: 0:09:51\n",
      "  Epoch 34, Batch 31/173, Loss: 1.2786, Elapsed: 0:14:23\n",
      "  Epoch 34, Batch 41/173, Loss: 1.2276, Elapsed: 0:18:59\n",
      "  Epoch 34, Batch 51/173, Loss: 1.3038, Elapsed: 0:23:52\n",
      "  Epoch 34, Batch 61/173, Loss: 1.1734, Elapsed: 0:28:50\n",
      "  Epoch 34, Batch 71/173, Loss: 1.1843, Elapsed: 0:33:31\n",
      "  Epoch 34, Batch 81/173, Loss: 1.2462, Elapsed: 0:38:08\n",
      "  Epoch 34, Batch 91/173, Loss: 1.3337, Elapsed: 0:42:46\n",
      "  Epoch 34, Batch 101/173, Loss: 1.3298, Elapsed: 0:47:45\n",
      "  Epoch 34, Batch 111/173, Loss: 1.2808, Elapsed: 0:52:42\n",
      "  Epoch 34, Batch 121/173, Loss: 1.2305, Elapsed: 0:57:25\n",
      "  Epoch 34, Batch 131/173, Loss: 1.3669, Elapsed: 1:02:05\n",
      "  Epoch 34, Batch 141/173, Loss: 1.3859, Elapsed: 1:06:58\n",
      "  Epoch 34, Batch 151/173, Loss: 1.2163, Elapsed: 1:11:55\n",
      "  Epoch 34, Batch 161/173, Loss: 1.2522, Elapsed: 1:16:34\n",
      "  Epoch 34, Batch 171/173, Loss: 1.2364, Elapsed: 1:21:12\n",
      "\n",
      "  Average training loss: 1.27\n",
      "  Training epoch took: 1:21:57\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.18\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 35 / 100 ========\n",
      "Training...\n",
      "  Epoch 35, Batch 1/173, Loss: 1.3352, Elapsed: 0:00:13\n",
      "  Epoch 35, Batch 11/173, Loss: 1.2953, Elapsed: 0:04:59\n",
      "  Epoch 35, Batch 21/173, Loss: 1.2075, Elapsed: 0:09:57\n",
      "  Epoch 35, Batch 31/173, Loss: 1.1792, Elapsed: 0:15:00\n",
      "  Epoch 35, Batch 41/173, Loss: 1.2942, Elapsed: 0:19:35\n",
      "  Epoch 35, Batch 51/173, Loss: 1.1980, Elapsed: 0:24:18\n",
      "  Epoch 35, Batch 61/173, Loss: 1.2856, Elapsed: 0:29:18\n",
      "  Epoch 35, Batch 71/173, Loss: 1.1591, Elapsed: 0:34:13\n",
      "  Epoch 35, Batch 81/173, Loss: 1.1423, Elapsed: 0:38:50\n",
      "  Epoch 35, Batch 91/173, Loss: 1.4601, Elapsed: 0:43:28\n",
      "  Epoch 35, Batch 101/173, Loss: 1.2444, Elapsed: 0:48:28\n",
      "  Epoch 35, Batch 111/173, Loss: 1.2593, Elapsed: 0:53:27\n",
      "  Epoch 35, Batch 121/173, Loss: 1.2615, Elapsed: 0:58:05\n",
      "  Epoch 35, Batch 131/173, Loss: 1.2696, Elapsed: 1:02:41\n",
      "  Epoch 35, Batch 141/173, Loss: 1.4139, Elapsed: 1:07:25\n",
      "  Epoch 35, Batch 151/173, Loss: 1.2618, Elapsed: 1:12:24\n",
      "  Epoch 35, Batch 161/173, Loss: 1.2576, Elapsed: 1:17:21\n",
      "  Epoch 35, Batch 171/173, Loss: 1.3694, Elapsed: 1:22:01\n",
      "\n",
      "  Average training loss: 1.26\n",
      "  Training epoch took: 1:22:45\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.17\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 36 / 100 ========\n",
      "Training...\n",
      "  Epoch 36, Batch 1/173, Loss: 1.2595, Elapsed: 0:00:12\n",
      "  Epoch 36, Batch 11/173, Loss: 1.1919, Elapsed: 0:04:52\n",
      "  Epoch 36, Batch 21/173, Loss: 1.1848, Elapsed: 0:09:49\n",
      "  Epoch 36, Batch 31/173, Loss: 1.2777, Elapsed: 0:14:46\n",
      "  Epoch 36, Batch 41/173, Loss: 1.2297, Elapsed: 0:19:26\n",
      "  Epoch 36, Batch 51/173, Loss: 1.1208, Elapsed: 0:24:15\n",
      "  Epoch 36, Batch 61/173, Loss: 1.2345, Elapsed: 0:29:21\n",
      "  Epoch 36, Batch 71/173, Loss: 1.2940, Elapsed: 0:34:26\n",
      "  Epoch 36, Batch 81/173, Loss: 1.2804, Elapsed: 0:39:09\n",
      "  Epoch 36, Batch 91/173, Loss: 1.2093, Elapsed: 0:43:45\n",
      "  Epoch 36, Batch 101/173, Loss: 1.2153, Elapsed: 0:48:34\n",
      "  Epoch 36, Batch 111/173, Loss: 1.3198, Elapsed: 0:53:26\n",
      "  Epoch 36, Batch 121/173, Loss: 1.2539, Elapsed: 0:58:15\n",
      "  Epoch 36, Batch 131/173, Loss: 1.2131, Elapsed: 1:02:50\n",
      "  Epoch 36, Batch 141/173, Loss: 1.2016, Elapsed: 1:07:22\n",
      "  Epoch 36, Batch 151/173, Loss: 1.2310, Elapsed: 1:12:13\n",
      "  Epoch 36, Batch 161/173, Loss: 1.2460, Elapsed: 1:17:08\n",
      "  Epoch 36, Batch 171/173, Loss: 1.3111, Elapsed: 1:21:45\n",
      "\n",
      "  Average training loss: 1.24\n",
      "  Training epoch took: 1:22:29\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.16\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 37 / 100 ========\n",
      "Training...\n",
      "  Epoch 37, Batch 1/173, Loss: 1.1879, Elapsed: 0:00:13\n",
      "  Epoch 37, Batch 11/173, Loss: 1.2096, Elapsed: 0:04:48\n",
      "  Epoch 37, Batch 21/173, Loss: 1.1358, Elapsed: 0:09:38\n",
      "  Epoch 37, Batch 31/173, Loss: 1.2518, Elapsed: 0:14:35\n",
      "  Epoch 37, Batch 41/173, Loss: 1.1887, Elapsed: 0:19:24\n",
      "  Epoch 37, Batch 51/173, Loss: 1.2354, Elapsed: 0:24:02\n",
      "  Epoch 37, Batch 61/173, Loss: 1.0457, Elapsed: 0:28:47\n",
      "  Epoch 37, Batch 71/173, Loss: 1.2486, Elapsed: 0:33:45\n",
      "  Epoch 37, Batch 81/173, Loss: 1.3285, Elapsed: 0:38:42\n",
      "  Epoch 37, Batch 91/173, Loss: 1.2364, Elapsed: 0:43:23\n",
      "  Epoch 37, Batch 101/173, Loss: 1.3057, Elapsed: 0:48:03\n",
      "  Epoch 37, Batch 111/173, Loss: 1.1387, Elapsed: 0:53:07\n",
      "  Epoch 37, Batch 121/173, Loss: 1.1552, Elapsed: 0:58:06\n",
      "  Epoch 37, Batch 131/173, Loss: 1.2575, Elapsed: 1:02:43\n",
      "  Epoch 37, Batch 141/173, Loss: 1.0938, Elapsed: 1:07:21\n",
      "  Epoch 37, Batch 151/173, Loss: 1.3991, Elapsed: 1:12:26\n",
      "  Epoch 37, Batch 161/173, Loss: 1.2933, Elapsed: 1:17:25\n",
      "  Epoch 37, Batch 171/173, Loss: 1.1992, Elapsed: 1:22:14\n",
      "\n",
      "  Average training loss: 1.23\n",
      "  Training epoch took: 1:22:58\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.16\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 38 / 100 ========\n",
      "Training...\n",
      "  Epoch 38, Batch 1/173, Loss: 1.2523, Elapsed: 0:00:12\n",
      "  Epoch 38, Batch 11/173, Loss: 1.2805, Elapsed: 0:04:43\n",
      "  Epoch 38, Batch 21/173, Loss: 1.2527, Elapsed: 0:09:21\n",
      "  Epoch 38, Batch 31/173, Loss: 1.2206, Elapsed: 0:14:10\n",
      "  Epoch 38, Batch 41/173, Loss: 1.1038, Elapsed: 0:18:58\n",
      "  Epoch 38, Batch 51/173, Loss: 1.2281, Elapsed: 0:23:29\n",
      "  Epoch 38, Batch 61/173, Loss: 1.3104, Elapsed: 0:28:00\n",
      "  Epoch 38, Batch 71/173, Loss: 1.1826, Elapsed: 0:32:41\n",
      "  Epoch 38, Batch 81/173, Loss: 1.2653, Elapsed: 0:37:27\n",
      "  Epoch 38, Batch 91/173, Loss: 1.3536, Elapsed: 0:42:16\n",
      "  Epoch 38, Batch 101/173, Loss: 1.4239, Elapsed: 0:46:49\n",
      "  Epoch 38, Batch 111/173, Loss: 1.2305, Elapsed: 0:51:20\n",
      "  Epoch 38, Batch 121/173, Loss: 1.1645, Elapsed: 0:56:00\n",
      "  Epoch 38, Batch 131/173, Loss: 1.3232, Elapsed: 1:00:49\n",
      "  Epoch 38, Batch 141/173, Loss: 1.2431, Elapsed: 1:05:38\n",
      "  Epoch 38, Batch 151/173, Loss: 1.1617, Elapsed: 1:10:19\n",
      "  Epoch 38, Batch 161/173, Loss: 1.2658, Elapsed: 1:14:50\n",
      "  Epoch 38, Batch 171/173, Loss: 1.1262, Elapsed: 1:19:26\n",
      "\n",
      "  Average training loss: 1.22\n",
      "  Training epoch took: 1:20:14\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.16\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 39 / 100 ========\n",
      "Training...\n",
      "  Epoch 39, Batch 1/173, Loss: 1.1748, Elapsed: 0:00:12\n",
      "  Epoch 39, Batch 11/173, Loss: 1.1901, Elapsed: 0:04:59\n",
      "  Epoch 39, Batch 21/173, Loss: 1.1585, Elapsed: 0:09:48\n",
      "  Epoch 39, Batch 31/173, Loss: 1.1566, Elapsed: 0:14:18\n",
      "  Epoch 39, Batch 41/173, Loss: 1.3238, Elapsed: 0:18:50\n",
      "  Epoch 39, Batch 51/173, Loss: 1.1636, Elapsed: 0:23:28\n",
      "  Epoch 39, Batch 61/173, Loss: 1.3520, Elapsed: 0:28:26\n",
      "  Epoch 39, Batch 71/173, Loss: 1.1612, Elapsed: 0:33:14\n",
      "  Epoch 39, Batch 81/173, Loss: 1.3066, Elapsed: 0:37:47\n",
      "  Epoch 39, Batch 91/173, Loss: 1.2982, Elapsed: 0:42:22\n",
      "  Epoch 39, Batch 101/173, Loss: 1.1465, Elapsed: 0:47:03\n",
      "  Epoch 39, Batch 111/173, Loss: 1.1529, Elapsed: 0:51:52\n",
      "  Epoch 39, Batch 121/173, Loss: 1.1657, Elapsed: 0:56:38\n",
      "  Epoch 39, Batch 131/173, Loss: 1.2003, Elapsed: 1:01:26\n",
      "  Epoch 39, Batch 141/173, Loss: 1.1678, Elapsed: 1:06:09\n",
      "  Epoch 39, Batch 151/173, Loss: 1.2376, Elapsed: 1:11:05\n",
      "  Epoch 39, Batch 161/173, Loss: 1.2117, Elapsed: 1:15:53\n",
      "  Epoch 39, Batch 171/173, Loss: 1.1530, Elapsed: 1:20:33\n",
      "\n",
      "  Average training loss: 1.21\n",
      "  Training epoch took: 1:21:17\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.16\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 40 / 100 ========\n",
      "Training...\n",
      "  Epoch 40, Batch 1/173, Loss: 1.2280, Elapsed: 0:00:12\n",
      "  Epoch 40, Batch 11/173, Loss: 1.1247, Elapsed: 0:04:43\n",
      "  Epoch 40, Batch 21/173, Loss: 1.1485, Elapsed: 0:09:19\n",
      "  Epoch 40, Batch 31/173, Loss: 1.1520, Elapsed: 0:14:10\n",
      "  Epoch 40, Batch 41/173, Loss: 1.1441, Elapsed: 0:19:00\n",
      "  Epoch 40, Batch 51/173, Loss: 1.2256, Elapsed: 0:23:40\n",
      "  Epoch 40, Batch 61/173, Loss: 1.1128, Elapsed: 0:28:13\n",
      "  Epoch 40, Batch 71/173, Loss: 1.2102, Elapsed: 0:32:48\n",
      "  Epoch 40, Batch 81/173, Loss: 1.2034, Elapsed: 0:37:33\n",
      "  Epoch 40, Batch 91/173, Loss: 1.2376, Elapsed: 0:42:27\n",
      "  Epoch 40, Batch 101/173, Loss: 1.1566, Elapsed: 0:47:01\n",
      "  Epoch 40, Batch 111/173, Loss: 1.1491, Elapsed: 0:51:34\n",
      "  Epoch 40, Batch 121/173, Loss: 1.1830, Elapsed: 0:56:11\n",
      "  Epoch 40, Batch 131/173, Loss: 1.1945, Elapsed: 1:01:00\n",
      "  Epoch 40, Batch 141/173, Loss: 1.1880, Elapsed: 1:05:51\n",
      "  Epoch 40, Batch 151/173, Loss: 1.1638, Elapsed: 1:10:36\n",
      "  Epoch 40, Batch 161/173, Loss: 1.2560, Elapsed: 1:15:10\n",
      "  Epoch 40, Batch 171/173, Loss: 1.1535, Elapsed: 1:19:44\n",
      "\n",
      "  Average training loss: 1.20\n",
      "  Training epoch took: 1:20:31\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.16\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 41 / 100 ========\n",
      "Training...\n",
      "  Epoch 41, Batch 1/173, Loss: 1.1838, Elapsed: 0:00:12\n",
      "  Epoch 41, Batch 11/173, Loss: 1.1318, Elapsed: 0:05:02\n",
      "  Epoch 41, Batch 21/173, Loss: 1.2945, Elapsed: 0:09:50\n",
      "  Epoch 41, Batch 31/173, Loss: 1.2179, Elapsed: 0:14:23\n",
      "  Epoch 41, Batch 41/173, Loss: 1.2464, Elapsed: 0:18:54\n",
      "  Epoch 41, Batch 51/173, Loss: 1.1269, Elapsed: 0:23:31\n",
      "  Epoch 41, Batch 61/173, Loss: 1.2435, Elapsed: 0:28:21\n",
      "  Epoch 41, Batch 71/173, Loss: 1.2921, Elapsed: 0:33:11\n",
      "  Epoch 41, Batch 81/173, Loss: 1.1694, Elapsed: 0:37:42\n",
      "  Epoch 41, Batch 91/173, Loss: 1.2130, Elapsed: 0:42:14\n",
      "  Epoch 41, Batch 101/173, Loss: 1.0253, Elapsed: 0:46:51\n",
      "  Epoch 41, Batch 111/173, Loss: 1.1726, Elapsed: 0:51:36\n",
      "  Epoch 41, Batch 121/173, Loss: 1.2007, Elapsed: 0:56:21\n",
      "  Epoch 41, Batch 131/173, Loss: 1.2108, Elapsed: 1:00:56\n",
      "  Epoch 41, Batch 141/173, Loss: 1.1024, Elapsed: 1:05:27\n",
      "  Epoch 41, Batch 151/173, Loss: 1.2198, Elapsed: 1:09:59\n",
      "  Epoch 41, Batch 161/173, Loss: 1.2369, Elapsed: 1:14:50\n",
      "  Epoch 41, Batch 171/173, Loss: 1.2287, Elapsed: 1:19:41\n",
      "\n",
      "  Average training loss: 1.19\n",
      "  Training epoch took: 1:20:27\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.15\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 42 / 100 ========\n",
      "Training...\n",
      "  Epoch 42, Batch 1/173, Loss: 1.1170, Elapsed: 0:00:12\n",
      "  Epoch 42, Batch 11/173, Loss: 1.2280, Elapsed: 0:04:52\n",
      "  Epoch 42, Batch 21/173, Loss: 1.2362, Elapsed: 0:09:27\n",
      "  Epoch 42, Batch 31/173, Loss: 1.0861, Elapsed: 0:14:10\n",
      "  Epoch 42, Batch 41/173, Loss: 1.1166, Elapsed: 0:18:59\n",
      "  Epoch 42, Batch 51/173, Loss: 0.9859, Elapsed: 0:23:45\n",
      "  Epoch 42, Batch 61/173, Loss: 1.1394, Elapsed: 0:28:17\n",
      "  Epoch 42, Batch 71/173, Loss: 1.2337, Elapsed: 0:32:48\n",
      "  Epoch 42, Batch 81/173, Loss: 1.2427, Elapsed: 0:37:26\n",
      "  Epoch 42, Batch 91/173, Loss: 1.1799, Elapsed: 0:42:17\n",
      "  Epoch 42, Batch 101/173, Loss: 1.1459, Elapsed: 0:47:11\n",
      "  Epoch 42, Batch 111/173, Loss: 1.2135, Elapsed: 0:51:53\n",
      "  Epoch 42, Batch 121/173, Loss: 1.1066, Elapsed: 0:56:26\n",
      "  Epoch 42, Batch 131/173, Loss: 1.1995, Elapsed: 1:01:07\n",
      "  Epoch 42, Batch 141/173, Loss: 1.2588, Elapsed: 1:05:54\n",
      "  Epoch 42, Batch 151/173, Loss: 1.2379, Elapsed: 1:10:44\n",
      "  Epoch 42, Batch 161/173, Loss: 1.1945, Elapsed: 1:15:15\n",
      "  Epoch 42, Batch 171/173, Loss: 1.2499, Elapsed: 1:19:46\n",
      "\n",
      "  Average training loss: 1.18\n",
      "  Training epoch took: 1:20:30\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.15\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 43 / 100 ========\n",
      "Training...\n",
      "  Epoch 43, Batch 1/173, Loss: 1.2052, Elapsed: 0:00:12\n",
      "  Epoch 43, Batch 11/173, Loss: 1.2011, Elapsed: 0:04:53\n",
      "  Epoch 43, Batch 21/173, Loss: 1.1975, Elapsed: 0:09:45\n",
      "  Epoch 43, Batch 31/173, Loss: 1.2492, Elapsed: 0:14:28\n",
      "  Epoch 43, Batch 41/173, Loss: 1.3129, Elapsed: 0:19:02\n",
      "  Epoch 43, Batch 51/173, Loss: 1.0244, Elapsed: 0:23:32\n",
      "  Epoch 43, Batch 61/173, Loss: 1.2803, Elapsed: 0:28:05\n",
      "  Epoch 43, Batch 71/173, Loss: 1.2627, Elapsed: 0:32:53\n",
      "  Epoch 43, Batch 81/173, Loss: 1.3219, Elapsed: 0:37:47\n",
      "  Epoch 43, Batch 91/173, Loss: 1.3104, Elapsed: 0:42:32\n",
      "  Epoch 43, Batch 101/173, Loss: 1.0693, Elapsed: 0:47:05\n",
      "  Epoch 43, Batch 111/173, Loss: 1.0803, Elapsed: 0:51:41\n",
      "  Epoch 43, Batch 121/173, Loss: 1.2885, Elapsed: 0:56:32\n",
      "  Epoch 43, Batch 131/173, Loss: 1.1286, Elapsed: 1:01:20\n",
      "  Epoch 43, Batch 141/173, Loss: 1.1058, Elapsed: 1:05:55\n",
      "  Epoch 43, Batch 151/173, Loss: 1.2344, Elapsed: 1:10:28\n",
      "  Epoch 43, Batch 161/173, Loss: 1.1968, Elapsed: 1:15:06\n",
      "  Epoch 43, Batch 171/173, Loss: 1.0853, Elapsed: 1:19:54\n",
      "\n",
      "  Average training loss: 1.17\n",
      "  Training epoch took: 1:20:42\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 44 / 100 ========\n",
      "Training...\n",
      "  Epoch 44, Batch 1/173, Loss: 1.1376, Elapsed: 0:00:12\n",
      "  Epoch 44, Batch 11/173, Loss: 1.1269, Elapsed: 0:04:56\n",
      "  Epoch 44, Batch 21/173, Loss: 1.2852, Elapsed: 0:09:30\n",
      "  Epoch 44, Batch 31/173, Loss: 1.2123, Elapsed: 0:14:03\n",
      "  Epoch 44, Batch 41/173, Loss: 1.1320, Elapsed: 0:18:44\n",
      "  Epoch 44, Batch 51/173, Loss: 1.0917, Elapsed: 0:23:30\n",
      "  Epoch 44, Batch 61/173, Loss: 1.2026, Elapsed: 0:28:17\n",
      "  Epoch 44, Batch 71/173, Loss: 1.1658, Elapsed: 0:32:53\n",
      "  Epoch 44, Batch 81/173, Loss: 1.2255, Elapsed: 0:37:25\n",
      "  Epoch 44, Batch 91/173, Loss: 1.0797, Elapsed: 0:42:07\n",
      "  Epoch 44, Batch 101/173, Loss: 1.1779, Elapsed: 0:47:03\n",
      "  Epoch 44, Batch 111/173, Loss: 1.1466, Elapsed: 0:51:47\n",
      "  Epoch 44, Batch 121/173, Loss: 1.1065, Elapsed: 0:56:19\n",
      "  Epoch 44, Batch 131/173, Loss: 1.1441, Elapsed: 1:00:50\n",
      "  Epoch 44, Batch 141/173, Loss: 1.0233, Elapsed: 1:05:35\n",
      "  Epoch 44, Batch 151/173, Loss: 1.1134, Elapsed: 1:10:26\n",
      "  Epoch 44, Batch 161/173, Loss: 1.0587, Elapsed: 1:15:23\n",
      "  Epoch 44, Batch 171/173, Loss: 1.1351, Elapsed: 1:20:03\n",
      "\n",
      "  Average training loss: 1.16\n",
      "  Training epoch took: 1:20:46\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.15\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 45 / 100 ========\n",
      "Training...\n",
      "  Epoch 45, Batch 1/173, Loss: 1.2786, Elapsed: 0:00:12\n",
      "  Epoch 45, Batch 11/173, Loss: 1.0766, Elapsed: 0:04:47\n",
      "  Epoch 45, Batch 21/173, Loss: 1.2402, Elapsed: 0:09:39\n",
      "  Epoch 45, Batch 31/173, Loss: 1.1349, Elapsed: 0:14:27\n",
      "  Epoch 45, Batch 41/173, Loss: 1.1505, Elapsed: 0:19:03\n",
      "  Epoch 45, Batch 51/173, Loss: 1.0685, Elapsed: 0:23:34\n",
      "  Epoch 45, Batch 61/173, Loss: 1.1465, Elapsed: 0:28:08\n",
      "  Epoch 45, Batch 71/173, Loss: 1.1670, Elapsed: 0:33:00\n",
      "  Epoch 45, Batch 81/173, Loss: 1.1120, Elapsed: 0:37:51\n",
      "  Epoch 45, Batch 91/173, Loss: 1.1745, Elapsed: 0:42:41\n",
      "  Epoch 45, Batch 101/173, Loss: 1.2295, Elapsed: 0:47:12\n",
      "  Epoch 45, Batch 111/173, Loss: 1.1963, Elapsed: 0:51:44\n",
      "  Epoch 45, Batch 121/173, Loss: 1.1616, Elapsed: 0:56:33\n",
      "  Epoch 45, Batch 131/173, Loss: 1.0959, Elapsed: 1:01:25\n",
      "  Epoch 45, Batch 141/173, Loss: 1.1542, Elapsed: 1:06:01\n",
      "  Epoch 45, Batch 151/173, Loss: 1.1238, Elapsed: 1:10:36\n",
      "  Epoch 45, Batch 161/173, Loss: 1.1883, Elapsed: 1:15:08\n",
      "  Epoch 45, Batch 171/173, Loss: 1.0256, Elapsed: 1:19:51\n",
      "\n",
      "  Average training loss: 1.15\n",
      "  Training epoch took: 1:20:38\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.15\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 46 / 100 ========\n",
      "Training...\n",
      "  Epoch 46, Batch 1/173, Loss: 1.3077, Elapsed: 0:00:12\n",
      "  Epoch 46, Batch 11/173, Loss: 1.0742, Elapsed: 0:05:01\n",
      "  Epoch 46, Batch 21/173, Loss: 1.1790, Elapsed: 0:09:40\n",
      "  Epoch 46, Batch 31/173, Loss: 1.1996, Elapsed: 0:14:14\n",
      "  Epoch 46, Batch 41/173, Loss: 1.0629, Elapsed: 0:18:53\n",
      "  Epoch 46, Batch 51/173, Loss: 1.1680, Elapsed: 0:23:50\n",
      "  Epoch 46, Batch 61/173, Loss: 1.1018, Elapsed: 0:28:44\n",
      "  Epoch 46, Batch 71/173, Loss: 1.0251, Elapsed: 0:33:31\n",
      "  Epoch 46, Batch 81/173, Loss: 1.2257, Elapsed: 0:38:03\n",
      "  Epoch 46, Batch 91/173, Loss: 1.0986, Elapsed: 0:42:42\n",
      "  Epoch 46, Batch 101/173, Loss: 1.1840, Elapsed: 0:47:36\n",
      "  Epoch 46, Batch 111/173, Loss: 1.1358, Elapsed: 0:52:26\n",
      "  Epoch 46, Batch 121/173, Loss: 1.1722, Elapsed: 0:56:58\n",
      "  Epoch 46, Batch 131/173, Loss: 1.1951, Elapsed: 1:01:28\n",
      "  Epoch 46, Batch 141/173, Loss: 1.1728, Elapsed: 1:06:02\n",
      "  Epoch 46, Batch 151/173, Loss: 1.1776, Elapsed: 1:10:49\n",
      "  Epoch 46, Batch 161/173, Loss: 1.1282, Elapsed: 1:15:40\n",
      "  Epoch 46, Batch 171/173, Loss: 1.1844, Elapsed: 1:20:13\n",
      "\n",
      "  Average training loss: 1.15\n",
      "  Training epoch took: 1:20:56\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 47 / 100 ========\n",
      "Training...\n",
      "  Epoch 47, Batch 1/173, Loss: 1.2781, Elapsed: 0:00:12\n",
      "  Epoch 47, Batch 11/173, Loss: 1.1104, Elapsed: 0:04:42\n",
      "  Epoch 47, Batch 21/173, Loss: 1.0714, Elapsed: 0:09:18\n",
      "  Epoch 47, Batch 31/173, Loss: 1.2090, Elapsed: 0:14:11\n",
      "  Epoch 47, Batch 41/173, Loss: 1.1040, Elapsed: 0:19:02\n",
      "  Epoch 47, Batch 51/173, Loss: 1.2116, Elapsed: 0:23:46\n",
      "  Epoch 47, Batch 61/173, Loss: 1.1246, Elapsed: 0:28:21\n",
      "  Epoch 47, Batch 71/173, Loss: 1.1733, Elapsed: 0:33:00\n",
      "  Epoch 47, Batch 81/173, Loss: 1.0788, Elapsed: 0:37:51\n",
      "  Epoch 47, Batch 91/173, Loss: 1.0788, Elapsed: 0:42:36\n",
      "  Epoch 47, Batch 101/173, Loss: 1.1107, Elapsed: 0:47:10\n",
      "  Epoch 47, Batch 111/173, Loss: 1.0854, Elapsed: 0:51:44\n",
      "  Epoch 47, Batch 121/173, Loss: 1.1058, Elapsed: 0:56:22\n",
      "  Epoch 47, Batch 131/173, Loss: 1.2324, Elapsed: 1:01:09\n",
      "  Epoch 47, Batch 141/173, Loss: 1.0749, Elapsed: 1:05:59\n",
      "  Epoch 47, Batch 151/173, Loss: 1.1799, Elapsed: 1:10:30\n",
      "  Epoch 47, Batch 161/173, Loss: 1.0352, Elapsed: 1:15:01\n",
      "  Epoch 47, Batch 171/173, Loss: 1.2188, Elapsed: 1:19:39\n",
      "\n",
      "  Average training loss: 1.14\n",
      "  Training epoch took: 1:20:25\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 48 / 100 ========\n",
      "Training...\n",
      "  Epoch 48, Batch 1/173, Loss: 1.2360, Elapsed: 0:00:12\n",
      "  Epoch 48, Batch 11/173, Loss: 1.2228, Elapsed: 0:04:58\n",
      "  Epoch 48, Batch 21/173, Loss: 1.0925, Elapsed: 0:09:50\n",
      "  Epoch 48, Batch 31/173, Loss: 1.1883, Elapsed: 0:14:31\n",
      "  Epoch 48, Batch 41/173, Loss: 1.1180, Elapsed: 0:19:01\n",
      "  Epoch 48, Batch 51/173, Loss: 1.1734, Elapsed: 0:23:41\n",
      "  Epoch 48, Batch 61/173, Loss: 1.1026, Elapsed: 0:28:26\n",
      "  Epoch 48, Batch 71/173, Loss: 1.1807, Elapsed: 0:33:09\n",
      "  Epoch 48, Batch 81/173, Loss: 1.1791, Elapsed: 0:37:45\n",
      "  Epoch 48, Batch 91/173, Loss: 1.1215, Elapsed: 0:42:18\n",
      "  Epoch 48, Batch 101/173, Loss: 1.0793, Elapsed: 0:46:57\n",
      "  Epoch 48, Batch 111/173, Loss: 1.1122, Elapsed: 0:51:56\n",
      "  Epoch 48, Batch 121/173, Loss: 1.1530, Elapsed: 0:56:44\n",
      "  Epoch 48, Batch 131/173, Loss: 1.1554, Elapsed: 1:01:17\n",
      "  Epoch 48, Batch 141/173, Loss: 1.0988, Elapsed: 1:05:48\n",
      "  Epoch 48, Batch 151/173, Loss: 1.1353, Elapsed: 1:10:27\n",
      "  Epoch 48, Batch 161/173, Loss: 1.1776, Elapsed: 1:15:24\n",
      "  Epoch 48, Batch 171/173, Loss: 1.1693, Elapsed: 1:20:17\n",
      "\n",
      "  Average training loss: 1.13\n",
      "  Training epoch took: 1:21:04\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 49 / 100 ========\n",
      "Training...\n",
      "  Epoch 49, Batch 1/173, Loss: 1.0545, Elapsed: 0:00:12\n",
      "  Epoch 49, Batch 11/173, Loss: 1.1810, Elapsed: 0:04:47\n",
      "  Epoch 49, Batch 21/173, Loss: 1.1007, Elapsed: 0:09:19\n",
      "  Epoch 49, Batch 31/173, Loss: 1.0657, Elapsed: 0:14:00\n",
      "  Epoch 49, Batch 41/173, Loss: 1.1667, Elapsed: 0:18:43\n",
      "  Epoch 49, Batch 51/173, Loss: 1.0823, Elapsed: 0:23:29\n",
      "  Epoch 49, Batch 61/173, Loss: 1.2361, Elapsed: 0:28:01\n",
      "  Epoch 49, Batch 71/173, Loss: 1.1981, Elapsed: 0:32:33\n",
      "  Epoch 49, Batch 81/173, Loss: 1.1436, Elapsed: 0:37:08\n",
      "  Epoch 49, Batch 91/173, Loss: 1.0414, Elapsed: 0:41:53\n",
      "  Epoch 49, Batch 101/173, Loss: 1.1740, Elapsed: 0:46:41\n",
      "  Epoch 49, Batch 111/173, Loss: 1.1221, Elapsed: 0:51:13\n",
      "  Epoch 49, Batch 121/173, Loss: 1.1002, Elapsed: 0:55:44\n",
      "  Epoch 49, Batch 131/173, Loss: 1.0459, Elapsed: 1:00:21\n",
      "  Epoch 49, Batch 141/173, Loss: 1.3509, Elapsed: 1:05:07\n",
      "  Epoch 49, Batch 151/173, Loss: 1.0536, Elapsed: 1:09:54\n",
      "  Epoch 49, Batch 161/173, Loss: 1.0290, Elapsed: 1:14:25\n",
      "  Epoch 49, Batch 171/173, Loss: 1.1856, Elapsed: 1:18:58\n",
      "\n",
      "  Average training loss: 1.13\n",
      "  Training epoch took: 1:19:43\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:28\n",
      "\n",
      "======== Epoch 50 / 100 ========\n",
      "Training...\n",
      "  Epoch 50, Batch 1/173, Loss: 1.1093, Elapsed: 0:00:12\n",
      "  Epoch 50, Batch 11/173, Loss: 1.1467, Elapsed: 0:04:53\n",
      "  Epoch 50, Batch 21/173, Loss: 1.1202, Elapsed: 0:09:40\n",
      "  Epoch 50, Batch 31/173, Loss: 1.0469, Elapsed: 0:14:30\n",
      "  Epoch 50, Batch 41/173, Loss: 1.0672, Elapsed: 0:19:14\n",
      "  Epoch 50, Batch 51/173, Loss: 1.0977, Elapsed: 0:23:46\n",
      "  Epoch 50, Batch 61/173, Loss: 1.1106, Elapsed: 0:28:28\n",
      "  Epoch 50, Batch 71/173, Loss: 1.0317, Elapsed: 0:33:17\n",
      "  Epoch 50, Batch 81/173, Loss: 1.1185, Elapsed: 0:38:00\n",
      "  Epoch 50, Batch 91/173, Loss: 0.9690, Elapsed: 0:42:31\n",
      "  Epoch 50, Batch 101/173, Loss: 1.1151, Elapsed: 0:47:01\n",
      "  Epoch 50, Batch 111/173, Loss: 1.1055, Elapsed: 0:51:36\n",
      "  Epoch 50, Batch 121/173, Loss: 1.0862, Elapsed: 0:56:23\n",
      "  Epoch 50, Batch 131/173, Loss: 1.1519, Elapsed: 1:01:11\n",
      "  Epoch 50, Batch 141/173, Loss: 1.1204, Elapsed: 1:05:57\n",
      "  Epoch 50, Batch 151/173, Loss: 1.1175, Elapsed: 1:10:30\n",
      "  Epoch 50, Batch 161/173, Loss: 1.0018, Elapsed: 1:15:04\n",
      "  Epoch 50, Batch 171/173, Loss: 1.0026, Elapsed: 1:19:52\n",
      "\n",
      "  Average training loss: 1.12\n",
      "  Training epoch took: 1:20:39\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 51 / 100 ========\n",
      "Training...\n",
      "  Epoch 51, Batch 1/173, Loss: 1.2945, Elapsed: 0:00:12\n",
      "  Epoch 51, Batch 11/173, Loss: 1.0620, Elapsed: 0:05:00\n",
      "  Epoch 51, Batch 21/173, Loss: 1.2927, Elapsed: 0:09:31\n",
      "  Epoch 51, Batch 31/173, Loss: 1.0327, Elapsed: 0:14:03\n",
      "  Epoch 51, Batch 41/173, Loss: 1.1042, Elapsed: 0:18:37\n",
      "  Epoch 51, Batch 51/173, Loss: 1.2303, Elapsed: 0:23:24\n",
      "  Epoch 51, Batch 61/173, Loss: 1.2187, Elapsed: 0:28:07\n",
      "  Epoch 51, Batch 71/173, Loss: 1.1937, Elapsed: 0:32:43\n",
      "  Epoch 51, Batch 81/173, Loss: 1.0779, Elapsed: 0:37:17\n",
      "  Epoch 51, Batch 91/173, Loss: 1.0571, Elapsed: 0:41:49\n",
      "  Epoch 51, Batch 101/173, Loss: 1.1319, Elapsed: 0:46:40\n",
      "  Epoch 51, Batch 111/173, Loss: 1.0780, Elapsed: 0:51:31\n",
      "  Epoch 51, Batch 121/173, Loss: 1.1030, Elapsed: 0:56:20\n",
      "  Epoch 51, Batch 131/173, Loss: 0.9874, Elapsed: 1:00:53\n",
      "  Epoch 51, Batch 141/173, Loss: 1.1720, Elapsed: 1:05:25\n",
      "  Epoch 51, Batch 151/173, Loss: 1.0666, Elapsed: 1:10:14\n",
      "  Epoch 51, Batch 161/173, Loss: 1.2112, Elapsed: 1:15:08\n",
      "  Epoch 51, Batch 171/173, Loss: 1.0692, Elapsed: 1:19:45\n",
      "\n",
      "  Average training loss: 1.11\n",
      "  Training epoch took: 1:20:29\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 52 / 100 ========\n",
      "Training...\n",
      "  Epoch 52, Batch 1/173, Loss: 1.1621, Elapsed: 0:00:12\n",
      "  Epoch 52, Batch 11/173, Loss: 1.1387, Elapsed: 0:04:44\n",
      "  Epoch 52, Batch 21/173, Loss: 1.1490, Elapsed: 0:09:21\n",
      "  Epoch 52, Batch 31/173, Loss: 1.1811, Elapsed: 0:14:10\n",
      "  Epoch 52, Batch 41/173, Loss: 1.1307, Elapsed: 0:19:03\n",
      "  Epoch 52, Batch 51/173, Loss: 1.1770, Elapsed: 0:23:36\n",
      "  Epoch 52, Batch 61/173, Loss: 1.1843, Elapsed: 0:28:07\n",
      "  Epoch 52, Batch 71/173, Loss: 1.1299, Elapsed: 0:32:43\n",
      "  Epoch 52, Batch 81/173, Loss: 1.1574, Elapsed: 0:37:28\n",
      "  Epoch 52, Batch 91/173, Loss: 1.1640, Elapsed: 0:42:13\n",
      "  Epoch 52, Batch 101/173, Loss: 1.0480, Elapsed: 0:46:47\n",
      "  Epoch 52, Batch 111/173, Loss: 1.1151, Elapsed: 0:51:17\n",
      "  Epoch 52, Batch 121/173, Loss: 1.1827, Elapsed: 0:55:51\n",
      "  Epoch 52, Batch 131/173, Loss: 1.1342, Elapsed: 1:00:38\n",
      "  Epoch 52, Batch 141/173, Loss: 0.9866, Elapsed: 1:05:24\n",
      "  Epoch 52, Batch 151/173, Loss: 1.1093, Elapsed: 1:10:10\n",
      "  Epoch 52, Batch 161/173, Loss: 1.1913, Elapsed: 1:14:42\n",
      "  Epoch 52, Batch 171/173, Loss: 1.1775, Elapsed: 1:19:18\n",
      "\n",
      "  Average training loss: 1.11\n",
      "  Training epoch took: 1:20:05\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 53 / 100 ========\n",
      "Training...\n",
      "  Epoch 53, Batch 1/173, Loss: 1.2241, Elapsed: 0:00:12\n",
      "  Epoch 53, Batch 11/173, Loss: 1.2093, Elapsed: 0:05:04\n",
      "  Epoch 53, Batch 21/173, Loss: 1.1340, Elapsed: 0:09:59\n",
      "  Epoch 53, Batch 31/173, Loss: 1.1661, Elapsed: 0:14:30\n",
      "  Epoch 53, Batch 41/173, Loss: 1.1283, Elapsed: 0:19:03\n",
      "  Epoch 53, Batch 51/173, Loss: 1.1703, Elapsed: 0:23:41\n",
      "  Epoch 53, Batch 61/173, Loss: 1.1646, Elapsed: 0:28:32\n",
      "  Epoch 53, Batch 71/173, Loss: 0.9603, Elapsed: 0:33:22\n",
      "  Epoch 53, Batch 81/173, Loss: 1.0069, Elapsed: 0:38:06\n",
      "  Epoch 53, Batch 91/173, Loss: 1.2086, Elapsed: 0:42:38\n",
      "  Epoch 53, Batch 101/173, Loss: 1.0819, Elapsed: 0:47:15\n",
      "  Epoch 53, Batch 111/173, Loss: 1.0589, Elapsed: 0:52:02\n",
      "  Epoch 53, Batch 121/173, Loss: 1.1640, Elapsed: 0:56:58\n",
      "  Epoch 53, Batch 131/173, Loss: 0.9343, Elapsed: 1:01:38\n",
      "  Epoch 53, Batch 141/173, Loss: 1.1794, Elapsed: 1:06:21\n",
      "  Epoch 53, Batch 151/173, Loss: 1.0639, Elapsed: 1:11:13\n",
      "  Epoch 53, Batch 161/173, Loss: 1.1023, Elapsed: 1:16:12\n",
      "  Epoch 53, Batch 171/173, Loss: 1.0807, Elapsed: 1:21:10\n",
      "\n",
      "  Average training loss: 1.11\n",
      "  Training epoch took: 1:21:56\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 54 / 100 ========\n",
      "Training...\n",
      "  Epoch 54, Batch 1/173, Loss: 1.1119, Elapsed: 0:00:13\n",
      "  Epoch 54, Batch 11/173, Loss: 1.1326, Elapsed: 0:04:47\n",
      "  Epoch 54, Batch 21/173, Loss: 1.1456, Elapsed: 0:09:25\n",
      "  Epoch 54, Batch 31/173, Loss: 1.0916, Elapsed: 0:14:20\n",
      "  Epoch 54, Batch 41/173, Loss: 1.0686, Elapsed: 0:19:07\n",
      "  Epoch 54, Batch 51/173, Loss: 1.0396, Elapsed: 0:23:37\n",
      "  Epoch 54, Batch 61/173, Loss: 1.1058, Elapsed: 0:28:08\n",
      "  Epoch 54, Batch 71/173, Loss: 1.1752, Elapsed: 0:32:41\n",
      "  Epoch 54, Batch 81/173, Loss: 1.1046, Elapsed: 0:37:29\n",
      "  Epoch 54, Batch 91/173, Loss: 1.0152, Elapsed: 0:42:17\n",
      "  Epoch 54, Batch 101/173, Loss: 0.9715, Elapsed: 0:46:53\n",
      "  Epoch 54, Batch 111/173, Loss: 1.0135, Elapsed: 0:51:22\n",
      "  Epoch 54, Batch 121/173, Loss: 0.9925, Elapsed: 0:55:53\n",
      "  Epoch 54, Batch 131/173, Loss: 1.0673, Elapsed: 1:00:39\n",
      "  Epoch 54, Batch 141/173, Loss: 1.1400, Elapsed: 1:05:33\n",
      "  Epoch 54, Batch 151/173, Loss: 1.1640, Elapsed: 1:10:10\n",
      "  Epoch 54, Batch 161/173, Loss: 1.1714, Elapsed: 1:14:42\n",
      "  Epoch 54, Batch 171/173, Loss: 1.0351, Elapsed: 1:19:14\n",
      "\n",
      "  Average training loss: 1.10\n",
      "  Training epoch took: 1:20:00\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 55 / 100 ========\n",
      "Training...\n",
      "  Epoch 55, Batch 1/173, Loss: 1.0677, Elapsed: 0:00:12\n",
      "  Epoch 55, Batch 11/173, Loss: 1.0905, Elapsed: 0:05:06\n",
      "  Epoch 55, Batch 21/173, Loss: 1.2157, Elapsed: 0:10:01\n",
      "  Epoch 55, Batch 31/173, Loss: 1.1691, Elapsed: 0:14:46\n",
      "  Epoch 55, Batch 41/173, Loss: 0.9461, Elapsed: 0:19:18\n",
      "  Epoch 55, Batch 51/173, Loss: 1.0829, Elapsed: 0:23:55\n",
      "  Epoch 55, Batch 61/173, Loss: 1.1445, Elapsed: 0:28:40\n",
      "  Epoch 55, Batch 71/173, Loss: 1.2164, Elapsed: 0:33:28\n",
      "  Epoch 55, Batch 81/173, Loss: 1.0406, Elapsed: 0:38:00\n",
      "  Epoch 55, Batch 91/173, Loss: 1.0145, Elapsed: 0:42:32\n",
      "  Epoch 55, Batch 101/173, Loss: 1.0639, Elapsed: 0:47:05\n",
      "  Epoch 55, Batch 111/173, Loss: 1.1631, Elapsed: 0:51:46\n",
      "  Epoch 55, Batch 121/173, Loss: 1.1433, Elapsed: 0:56:30\n",
      "  Epoch 55, Batch 131/173, Loss: 0.9832, Elapsed: 1:01:16\n",
      "  Epoch 55, Batch 141/173, Loss: 1.0437, Elapsed: 1:05:46\n",
      "  Epoch 55, Batch 151/173, Loss: 1.0781, Elapsed: 1:10:18\n",
      "  Epoch 55, Batch 161/173, Loss: 1.1021, Elapsed: 1:15:04\n",
      "  Epoch 55, Batch 171/173, Loss: 1.1805, Elapsed: 1:19:53\n",
      "\n",
      "  Average training loss: 1.09\n",
      "  Training epoch took: 1:20:39\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 56 / 100 ========\n",
      "Training...\n",
      "  Epoch 56, Batch 1/173, Loss: 1.0542, Elapsed: 0:00:12\n",
      "  Epoch 56, Batch 11/173, Loss: 1.0756, Elapsed: 0:04:47\n",
      "  Epoch 56, Batch 21/173, Loss: 1.0026, Elapsed: 0:09:18\n",
      "  Epoch 56, Batch 31/173, Loss: 1.0058, Elapsed: 0:13:49\n",
      "  Epoch 56, Batch 41/173, Loss: 1.1192, Elapsed: 0:18:52\n",
      "  Epoch 56, Batch 51/173, Loss: 1.1053, Elapsed: 0:24:10\n",
      "  Epoch 56, Batch 61/173, Loss: 1.1197, Elapsed: 0:28:55\n",
      "  Epoch 56, Batch 71/173, Loss: 1.2192, Elapsed: 0:33:34\n",
      "  Epoch 56, Batch 81/173, Loss: 1.0114, Elapsed: 0:38:34\n",
      "  Epoch 56, Batch 91/173, Loss: 1.1192, Elapsed: 0:43:32\n",
      "  Epoch 56, Batch 101/173, Loss: 1.1095, Elapsed: 0:48:13\n",
      "  Epoch 56, Batch 111/173, Loss: 1.1636, Elapsed: 0:52:53\n",
      "  Epoch 56, Batch 121/173, Loss: 1.0574, Elapsed: 0:57:43\n",
      "  Epoch 56, Batch 131/173, Loss: 1.0528, Elapsed: 1:02:49\n",
      "  Epoch 56, Batch 141/173, Loss: 1.1548, Elapsed: 1:07:35\n",
      "  Epoch 56, Batch 151/173, Loss: 1.0844, Elapsed: 1:12:13\n",
      "  Epoch 56, Batch 161/173, Loss: 1.0347, Elapsed: 1:17:04\n",
      "  Epoch 56, Batch 171/173, Loss: 1.1624, Elapsed: 1:22:10\n",
      "\n",
      "  Average training loss: 1.09\n",
      "  Training epoch took: 1:22:59\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:31\n",
      "\n",
      "======== Epoch 57 / 100 ========\n",
      "Training...\n",
      "  Epoch 57, Batch 1/173, Loss: 1.0010, Elapsed: 0:00:12\n",
      "  Epoch 57, Batch 11/173, Loss: 1.0495, Elapsed: 0:05:06\n",
      "  Epoch 57, Batch 21/173, Loss: 1.0547, Elapsed: 0:09:44\n",
      "  Epoch 57, Batch 31/173, Loss: 1.0211, Elapsed: 0:14:32\n",
      "  Epoch 57, Batch 41/173, Loss: 1.0963, Elapsed: 0:19:35\n",
      "  Epoch 57, Batch 51/173, Loss: 1.1834, Elapsed: 0:24:27\n",
      "  Epoch 57, Batch 61/173, Loss: 1.0384, Elapsed: 0:29:06\n",
      "  Epoch 57, Batch 71/173, Loss: 1.1024, Elapsed: 0:33:47\n",
      "  Epoch 57, Batch 81/173, Loss: 1.1904, Elapsed: 0:38:44\n",
      "  Epoch 57, Batch 91/173, Loss: 1.0645, Elapsed: 0:43:37\n",
      "  Epoch 57, Batch 101/173, Loss: 1.1221, Elapsed: 0:48:22\n",
      "  Epoch 57, Batch 111/173, Loss: 1.0396, Elapsed: 0:52:53\n",
      "  Epoch 57, Batch 121/173, Loss: 1.1246, Elapsed: 0:57:32\n",
      "  Epoch 57, Batch 131/173, Loss: 0.9324, Elapsed: 1:02:20\n",
      "  Epoch 57, Batch 141/173, Loss: 1.0702, Elapsed: 1:07:09\n",
      "  Epoch 57, Batch 151/173, Loss: 1.1606, Elapsed: 1:11:42\n",
      "  Epoch 57, Batch 161/173, Loss: 1.0144, Elapsed: 1:16:16\n",
      "  Epoch 57, Batch 171/173, Loss: 1.0063, Elapsed: 1:20:57\n",
      "\n",
      "  Average training loss: 1.09\n",
      "  Training epoch took: 1:21:44\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 58 / 100 ========\n",
      "Training...\n",
      "  Epoch 58, Batch 1/173, Loss: 1.0759, Elapsed: 0:00:12\n",
      "  Epoch 58, Batch 11/173, Loss: 1.0577, Elapsed: 0:05:01\n",
      "  Epoch 58, Batch 21/173, Loss: 1.1704, Elapsed: 0:09:53\n",
      "  Epoch 58, Batch 31/173, Loss: 1.0814, Elapsed: 0:14:30\n",
      "  Epoch 58, Batch 41/173, Loss: 1.0220, Elapsed: 0:19:03\n",
      "  Epoch 58, Batch 51/173, Loss: 1.0325, Elapsed: 0:23:55\n",
      "  Epoch 58, Batch 61/173, Loss: 1.0162, Elapsed: 0:28:43\n",
      "  Epoch 58, Batch 71/173, Loss: 1.1325, Elapsed: 0:33:23\n",
      "  Epoch 58, Batch 81/173, Loss: 1.0456, Elapsed: 0:38:00\n",
      "  Epoch 58, Batch 91/173, Loss: 1.0175, Elapsed: 0:42:40\n",
      "  Epoch 58, Batch 101/173, Loss: 1.0767, Elapsed: 0:47:30\n",
      "  Epoch 58, Batch 111/173, Loss: 0.9946, Elapsed: 0:52:19\n",
      "  Epoch 58, Batch 121/173, Loss: 0.9326, Elapsed: 0:56:56\n",
      "  Epoch 58, Batch 131/173, Loss: 1.1496, Elapsed: 1:01:30\n",
      "  Epoch 58, Batch 141/173, Loss: 1.1059, Elapsed: 1:06:09\n",
      "  Epoch 58, Batch 151/173, Loss: 1.1313, Elapsed: 1:10:57\n",
      "  Epoch 58, Batch 161/173, Loss: 1.1275, Elapsed: 1:15:46\n",
      "  Epoch 58, Batch 171/173, Loss: 1.1080, Elapsed: 1:20:18\n",
      "\n",
      "  Average training loss: 1.08\n",
      "  Training epoch took: 1:21:02\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 59 / 100 ========\n",
      "Training...\n",
      "  Epoch 59, Batch 1/173, Loss: 1.1916, Elapsed: 0:00:12\n",
      "  Epoch 59, Batch 11/173, Loss: 1.0594, Elapsed: 0:04:45\n",
      "  Epoch 59, Batch 21/173, Loss: 1.0756, Elapsed: 0:09:30\n",
      "  Epoch 59, Batch 31/173, Loss: 1.0687, Elapsed: 0:14:22\n",
      "  Epoch 59, Batch 41/173, Loss: 1.0363, Elapsed: 0:19:13\n",
      "  Epoch 59, Batch 51/173, Loss: 1.0671, Elapsed: 0:23:49\n",
      "  Epoch 59, Batch 61/173, Loss: 1.0867, Elapsed: 0:28:20\n",
      "  Epoch 59, Batch 71/173, Loss: 1.0032, Elapsed: 0:33:01\n",
      "  Epoch 59, Batch 81/173, Loss: 1.0064, Elapsed: 0:37:50\n",
      "  Epoch 59, Batch 91/173, Loss: 1.2068, Elapsed: 0:42:34\n",
      "  Epoch 59, Batch 101/173, Loss: 1.1011, Elapsed: 0:47:10\n",
      "  Epoch 59, Batch 111/173, Loss: 1.0665, Elapsed: 0:51:48\n",
      "  Epoch 59, Batch 121/173, Loss: 1.1315, Elapsed: 0:56:44\n",
      "  Epoch 59, Batch 131/173, Loss: 1.1187, Elapsed: 1:01:40\n",
      "  Epoch 59, Batch 141/173, Loss: 1.0673, Elapsed: 1:06:28\n",
      "  Epoch 59, Batch 151/173, Loss: 1.1369, Elapsed: 1:11:01\n",
      "  Epoch 59, Batch 161/173, Loss: 1.1325, Elapsed: 1:15:43\n",
      "  Epoch 59, Batch 171/173, Loss: 1.1446, Elapsed: 1:20:37\n",
      "\n",
      "  Average training loss: 1.08\n",
      "  Training epoch took: 1:21:24\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 60 / 100 ========\n",
      "Training...\n",
      "  Epoch 60, Batch 1/173, Loss: 1.0475, Elapsed: 0:00:12\n",
      "  Epoch 60, Batch 11/173, Loss: 1.0580, Elapsed: 0:04:52\n",
      "  Epoch 60, Batch 21/173, Loss: 1.1844, Elapsed: 0:09:25\n",
      "  Epoch 60, Batch 31/173, Loss: 1.1344, Elapsed: 0:13:58\n",
      "  Epoch 60, Batch 41/173, Loss: 1.1664, Elapsed: 0:18:48\n",
      "  Epoch 60, Batch 51/173, Loss: 1.1008, Elapsed: 0:23:46\n",
      "  Epoch 60, Batch 61/173, Loss: 1.1360, Elapsed: 0:28:37\n",
      "  Epoch 60, Batch 71/173, Loss: 1.1892, Elapsed: 0:33:12\n",
      "  Epoch 60, Batch 81/173, Loss: 1.0407, Elapsed: 0:37:46\n",
      "  Epoch 60, Batch 91/173, Loss: 1.0350, Elapsed: 0:42:38\n",
      "  Epoch 60, Batch 101/173, Loss: 1.0037, Elapsed: 0:47:32\n",
      "  Epoch 60, Batch 111/173, Loss: 1.1378, Elapsed: 0:52:07\n",
      "  Epoch 60, Batch 121/173, Loss: 0.9900, Elapsed: 0:56:38\n",
      "  Epoch 60, Batch 131/173, Loss: 0.9921, Elapsed: 1:01:15\n",
      "  Epoch 60, Batch 141/173, Loss: 1.0836, Elapsed: 1:06:05\n",
      "  Epoch 60, Batch 151/173, Loss: 1.1147, Elapsed: 1:10:53\n",
      "  Epoch 60, Batch 161/173, Loss: 1.1744, Elapsed: 1:15:28\n",
      "  Epoch 60, Batch 171/173, Loss: 1.1493, Elapsed: 1:20:00\n",
      "\n",
      "  Average training loss: 1.07\n",
      "  Training epoch took: 1:20:44\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 61 / 100 ========\n",
      "Training...\n",
      "  Epoch 61, Batch 1/173, Loss: 0.9810, Elapsed: 0:00:13\n",
      "  Epoch 61, Batch 11/173, Loss: 1.0188, Elapsed: 0:04:51\n",
      "  Epoch 61, Batch 21/173, Loss: 0.9681, Elapsed: 0:09:43\n",
      "  Epoch 61, Batch 31/173, Loss: 1.1638, Elapsed: 0:14:36\n",
      "  Epoch 61, Batch 41/173, Loss: 1.0788, Elapsed: 0:19:15\n",
      "  Epoch 61, Batch 51/173, Loss: 1.2154, Elapsed: 0:23:49\n",
      "  Epoch 61, Batch 61/173, Loss: 1.0326, Elapsed: 0:28:32\n",
      "  Epoch 61, Batch 71/173, Loss: 1.0870, Elapsed: 0:33:20\n",
      "  Epoch 61, Batch 81/173, Loss: 1.1372, Elapsed: 0:38:04\n",
      "  Epoch 61, Batch 91/173, Loss: 1.0169, Elapsed: 0:42:38\n",
      "  Epoch 61, Batch 101/173, Loss: 1.0033, Elapsed: 0:47:12\n",
      "  Epoch 61, Batch 111/173, Loss: 1.2043, Elapsed: 0:52:08\n",
      "  Epoch 61, Batch 121/173, Loss: 0.9763, Elapsed: 0:57:06\n",
      "  Epoch 61, Batch 131/173, Loss: 1.1642, Elapsed: 1:01:44\n",
      "  Epoch 61, Batch 141/173, Loss: 1.1717, Elapsed: 1:06:31\n",
      "  Epoch 61, Batch 151/173, Loss: 1.1191, Elapsed: 1:11:27\n",
      "  Epoch 61, Batch 161/173, Loss: 1.1167, Elapsed: 1:16:33\n",
      "  Epoch 61, Batch 171/173, Loss: 1.0800, Elapsed: 1:21:27\n",
      "\n",
      "  Average training loss: 1.07\n",
      "  Training epoch took: 1:22:11\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 62 / 100 ========\n",
      "Training...\n",
      "  Epoch 62, Batch 1/173, Loss: 1.0218, Elapsed: 0:00:12\n",
      "  Epoch 62, Batch 11/173, Loss: 1.0065, Elapsed: 0:04:44\n",
      "  Epoch 62, Batch 21/173, Loss: 1.1351, Elapsed: 0:09:19\n",
      "  Epoch 62, Batch 31/173, Loss: 1.0953, Elapsed: 0:14:05\n",
      "  Epoch 62, Batch 41/173, Loss: 1.0817, Elapsed: 0:18:54\n",
      "  Epoch 62, Batch 51/173, Loss: 1.1806, Elapsed: 0:23:25\n",
      "  Epoch 62, Batch 61/173, Loss: 1.1095, Elapsed: 0:27:55\n",
      "  Epoch 62, Batch 71/173, Loss: 0.9854, Elapsed: 0:32:24\n",
      "  Epoch 62, Batch 81/173, Loss: 1.1202, Elapsed: 0:37:12\n",
      "  Epoch 62, Batch 91/173, Loss: 1.0084, Elapsed: 0:42:01\n",
      "  Epoch 62, Batch 101/173, Loss: 1.1280, Elapsed: 0:46:48\n",
      "  Epoch 62, Batch 111/173, Loss: 0.9869, Elapsed: 0:51:25\n",
      "  Epoch 62, Batch 121/173, Loss: 1.2444, Elapsed: 0:55:56\n",
      "  Epoch 62, Batch 131/173, Loss: 1.0262, Elapsed: 1:00:36\n",
      "  Epoch 62, Batch 141/173, Loss: 1.0940, Elapsed: 1:05:25\n",
      "  Epoch 62, Batch 151/173, Loss: 1.0512, Elapsed: 1:10:08\n",
      "  Epoch 62, Batch 161/173, Loss: 1.0848, Elapsed: 1:14:37\n",
      "  Epoch 62, Batch 171/173, Loss: 1.1197, Elapsed: 1:19:09\n",
      "\n",
      "  Average training loss: 1.07\n",
      "  Training epoch took: 1:19:52\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 63 / 100 ========\n",
      "Training...\n",
      "  Epoch 63, Batch 1/173, Loss: 1.0553, Elapsed: 0:00:12\n",
      "  Epoch 63, Batch 11/173, Loss: 1.0110, Elapsed: 0:04:54\n",
      "  Epoch 63, Batch 21/173, Loss: 0.9626, Elapsed: 0:09:46\n",
      "  Epoch 63, Batch 31/173, Loss: 1.0903, Elapsed: 0:14:30\n",
      "  Epoch 63, Batch 41/173, Loss: 1.1207, Elapsed: 0:19:01\n",
      "  Epoch 63, Batch 51/173, Loss: 1.0528, Elapsed: 0:23:34\n",
      "  Epoch 63, Batch 61/173, Loss: 1.1112, Elapsed: 0:28:12\n",
      "  Epoch 63, Batch 71/173, Loss: 1.0249, Elapsed: 0:32:58\n",
      "  Epoch 63, Batch 81/173, Loss: 1.0388, Elapsed: 0:37:46\n",
      "  Epoch 63, Batch 91/173, Loss: 1.0399, Elapsed: 0:42:19\n",
      "  Epoch 63, Batch 101/173, Loss: 1.0353, Elapsed: 0:46:51\n",
      "  Epoch 63, Batch 111/173, Loss: 1.0006, Elapsed: 0:51:28\n",
      "  Epoch 63, Batch 121/173, Loss: 1.0809, Elapsed: 0:56:19\n",
      "  Epoch 63, Batch 131/173, Loss: 1.0534, Elapsed: 1:01:08\n",
      "  Epoch 63, Batch 141/173, Loss: 1.0106, Elapsed: 1:06:03\n",
      "  Epoch 63, Batch 151/173, Loss: 1.1015, Elapsed: 1:10:35\n",
      "  Epoch 63, Batch 161/173, Loss: 0.9961, Elapsed: 1:15:08\n",
      "  Epoch 63, Batch 171/173, Loss: 1.0589, Elapsed: 1:19:57\n",
      "\n",
      "  Average training loss: 1.06\n",
      "  Training epoch took: 1:20:44\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 64 / 100 ========\n",
      "Training...\n",
      "  Epoch 64, Batch 1/173, Loss: 1.0071, Elapsed: 0:00:12\n",
      "  Epoch 64, Batch 11/173, Loss: 0.9790, Elapsed: 0:04:55\n",
      "  Epoch 64, Batch 21/173, Loss: 1.0190, Elapsed: 0:09:31\n",
      "  Epoch 64, Batch 31/173, Loss: 1.0367, Elapsed: 0:14:01\n",
      "  Epoch 64, Batch 41/173, Loss: 1.1242, Elapsed: 0:18:32\n",
      "  Epoch 64, Batch 51/173, Loss: 1.0745, Elapsed: 0:23:16\n",
      "  Epoch 64, Batch 61/173, Loss: 1.0991, Elapsed: 0:27:59\n",
      "  Epoch 64, Batch 71/173, Loss: 1.0841, Elapsed: 0:32:43\n",
      "  Epoch 64, Batch 81/173, Loss: 1.1748, Elapsed: 0:37:25\n",
      "  Epoch 64, Batch 91/173, Loss: 1.1323, Elapsed: 0:41:56\n",
      "  Epoch 64, Batch 101/173, Loss: 1.0650, Elapsed: 0:46:35\n",
      "  Epoch 64, Batch 111/173, Loss: 1.0387, Elapsed: 0:51:23\n",
      "  Epoch 64, Batch 121/173, Loss: 0.9314, Elapsed: 0:56:04\n",
      "  Epoch 64, Batch 131/173, Loss: 1.1643, Elapsed: 1:00:34\n",
      "  Epoch 64, Batch 141/173, Loss: 0.9914, Elapsed: 1:05:04\n",
      "  Epoch 64, Batch 151/173, Loss: 1.0620, Elapsed: 1:09:40\n",
      "  Epoch 64, Batch 161/173, Loss: 1.0535, Elapsed: 1:14:27\n",
      "  Epoch 64, Batch 171/173, Loss: 1.0899, Elapsed: 1:19:15\n",
      "\n",
      "  Average training loss: 1.06\n",
      "  Training epoch took: 1:20:01\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 65 / 100 ========\n",
      "Training...\n",
      "  Epoch 65, Batch 1/173, Loss: 1.1226, Elapsed: 0:00:12\n",
      "  Epoch 65, Batch 11/173, Loss: 1.0928, Elapsed: 0:04:45\n",
      "  Epoch 65, Batch 21/173, Loss: 1.1951, Elapsed: 0:09:18\n",
      "  Epoch 65, Batch 31/173, Loss: 1.0476, Elapsed: 0:13:47\n",
      "  Epoch 65, Batch 41/173, Loss: 0.9942, Elapsed: 0:18:30\n",
      "  Epoch 65, Batch 51/173, Loss: 1.0288, Elapsed: 0:23:15\n",
      "  Epoch 65, Batch 61/173, Loss: 1.0119, Elapsed: 0:28:03\n",
      "  Epoch 65, Batch 71/173, Loss: 1.0695, Elapsed: 0:32:34\n",
      "  Epoch 65, Batch 81/173, Loss: 1.0941, Elapsed: 0:37:06\n",
      "  Epoch 65, Batch 91/173, Loss: 1.0921, Elapsed: 0:41:46\n",
      "  Epoch 65, Batch 101/173, Loss: 0.9552, Elapsed: 0:46:35\n",
      "  Epoch 65, Batch 111/173, Loss: 1.0662, Elapsed: 0:51:20\n",
      "  Epoch 65, Batch 121/173, Loss: 1.0330, Elapsed: 0:55:51\n",
      "  Epoch 65, Batch 131/173, Loss: 1.1038, Elapsed: 1:00:20\n",
      "  Epoch 65, Batch 141/173, Loss: 1.0240, Elapsed: 1:04:55\n",
      "  Epoch 65, Batch 151/173, Loss: 1.0702, Elapsed: 1:09:39\n",
      "  Epoch 65, Batch 161/173, Loss: 1.0454, Elapsed: 1:14:23\n",
      "  Epoch 65, Batch 171/173, Loss: 0.9951, Elapsed: 1:18:59\n",
      "\n",
      "  Average training loss: 1.06\n",
      "  Training epoch took: 1:19:42\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 66 / 100 ========\n",
      "Training...\n",
      "  Epoch 66, Batch 1/173, Loss: 0.9614, Elapsed: 0:00:12\n",
      "  Epoch 66, Batch 11/173, Loss: 0.9399, Elapsed: 0:04:44\n",
      "  Epoch 66, Batch 21/173, Loss: 1.0017, Elapsed: 0:09:19\n",
      "  Epoch 66, Batch 31/173, Loss: 1.0266, Elapsed: 0:14:03\n",
      "  Epoch 66, Batch 41/173, Loss: 1.0826, Elapsed: 0:18:49\n",
      "  Epoch 66, Batch 51/173, Loss: 1.1787, Elapsed: 0:23:31\n",
      "  Epoch 66, Batch 61/173, Loss: 1.0525, Elapsed: 0:28:03\n",
      "  Epoch 66, Batch 71/173, Loss: 1.1839, Elapsed: 0:32:33\n",
      "  Epoch 66, Batch 81/173, Loss: 1.0965, Elapsed: 0:37:11\n",
      "  Epoch 66, Batch 91/173, Loss: 1.2350, Elapsed: 0:41:51\n",
      "  Epoch 66, Batch 101/173, Loss: 1.0111, Elapsed: 0:46:33\n",
      "  Epoch 66, Batch 111/173, Loss: 1.0685, Elapsed: 0:51:04\n",
      "  Epoch 66, Batch 121/173, Loss: 1.0217, Elapsed: 0:55:32\n",
      "  Epoch 66, Batch 131/173, Loss: 1.0038, Elapsed: 1:00:03\n",
      "  Epoch 66, Batch 141/173, Loss: 1.1403, Elapsed: 1:04:47\n",
      "  Epoch 66, Batch 151/173, Loss: 0.9788, Elapsed: 1:09:31\n",
      "  Epoch 66, Batch 161/173, Loss: 1.0920, Elapsed: 1:14:07\n",
      "  Epoch 66, Batch 171/173, Loss: 1.0775, Elapsed: 1:18:37\n",
      "\n",
      "  Average training loss: 1.05\n",
      "  Training epoch took: 1:19:20\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 67 / 100 ========\n",
      "Training...\n",
      "  Epoch 67, Batch 1/173, Loss: 1.0902, Elapsed: 0:00:12\n",
      "  Epoch 67, Batch 11/173, Loss: 0.9640, Elapsed: 0:04:42\n",
      "  Epoch 67, Batch 21/173, Loss: 1.0235, Elapsed: 0:09:28\n",
      "  Epoch 67, Batch 31/173, Loss: 1.0201, Elapsed: 0:14:12\n",
      "  Epoch 67, Batch 41/173, Loss: 1.0508, Elapsed: 0:18:55\n",
      "  Epoch 67, Batch 51/173, Loss: 1.0835, Elapsed: 0:23:31\n",
      "  Epoch 67, Batch 61/173, Loss: 1.0839, Elapsed: 0:28:02\n",
      "  Epoch 67, Batch 71/173, Loss: 1.0935, Elapsed: 0:32:36\n",
      "  Epoch 67, Batch 81/173, Loss: 1.1075, Elapsed: 0:37:19\n",
      "  Epoch 67, Batch 91/173, Loss: 0.9960, Elapsed: 0:42:02\n",
      "  Epoch 67, Batch 101/173, Loss: 0.9990, Elapsed: 0:46:35\n",
      "  Epoch 67, Batch 111/173, Loss: 0.9975, Elapsed: 0:51:07\n",
      "  Epoch 67, Batch 121/173, Loss: 1.0544, Elapsed: 0:55:39\n",
      "  Epoch 67, Batch 131/173, Loss: 0.9754, Elapsed: 1:00:20\n",
      "  Epoch 67, Batch 141/173, Loss: 1.0434, Elapsed: 1:04:59\n",
      "  Epoch 67, Batch 151/173, Loss: 1.0502, Elapsed: 1:09:40\n",
      "  Epoch 67, Batch 161/173, Loss: 1.0641, Elapsed: 1:14:17\n",
      "  Epoch 67, Batch 171/173, Loss: 1.0827, Elapsed: 1:18:47\n",
      "\n",
      "  Average training loss: 1.05\n",
      "  Training epoch took: 1:19:30\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 68 / 100 ========\n",
      "Training...\n",
      "  Epoch 68, Batch 1/173, Loss: 0.9584, Elapsed: 0:00:12\n",
      "  Epoch 68, Batch 11/173, Loss: 0.9905, Elapsed: 0:04:53\n",
      "  Epoch 68, Batch 21/173, Loss: 1.1074, Elapsed: 0:09:41\n",
      "  Epoch 68, Batch 31/173, Loss: 0.8811, Elapsed: 0:14:25\n",
      "  Epoch 68, Batch 41/173, Loss: 1.0043, Elapsed: 0:18:58\n",
      "  Epoch 68, Batch 51/173, Loss: 1.0806, Elapsed: 0:23:27\n",
      "  Epoch 68, Batch 61/173, Loss: 0.9907, Elapsed: 0:28:02\n",
      "  Epoch 68, Batch 71/173, Loss: 1.1835, Elapsed: 0:32:44\n",
      "  Epoch 68, Batch 81/173, Loss: 1.0252, Elapsed: 0:37:27\n",
      "  Epoch 68, Batch 91/173, Loss: 1.0145, Elapsed: 0:41:58\n",
      "  Epoch 68, Batch 101/173, Loss: 1.1002, Elapsed: 0:46:30\n",
      "  Epoch 68, Batch 111/173, Loss: 1.0016, Elapsed: 0:51:00\n",
      "  Epoch 68, Batch 121/173, Loss: 1.0202, Elapsed: 0:55:40\n",
      "  Epoch 68, Batch 131/173, Loss: 1.1583, Elapsed: 1:00:24\n",
      "  Epoch 68, Batch 141/173, Loss: 0.9996, Elapsed: 1:05:08\n",
      "  Epoch 68, Batch 151/173, Loss: 1.0625, Elapsed: 1:09:38\n",
      "  Epoch 68, Batch 161/173, Loss: 1.0939, Elapsed: 1:14:09\n",
      "  Epoch 68, Batch 171/173, Loss: 1.0705, Elapsed: 1:18:46\n",
      "\n",
      "  Average training loss: 1.05\n",
      "  Training epoch took: 1:19:32\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 69 / 100 ========\n",
      "Training...\n",
      "  Epoch 69, Batch 1/173, Loss: 1.0678, Elapsed: 0:00:12\n",
      "  Epoch 69, Batch 11/173, Loss: 0.9991, Elapsed: 0:04:55\n",
      "  Epoch 69, Batch 21/173, Loss: 1.0523, Elapsed: 0:09:39\n",
      "  Epoch 69, Batch 31/173, Loss: 0.9037, Elapsed: 0:14:19\n",
      "  Epoch 69, Batch 41/173, Loss: 1.0123, Elapsed: 0:18:49\n",
      "  Epoch 69, Batch 51/173, Loss: 1.1600, Elapsed: 0:23:21\n",
      "  Epoch 69, Batch 61/173, Loss: 1.1697, Elapsed: 0:28:07\n",
      "  Epoch 69, Batch 71/173, Loss: 1.0368, Elapsed: 0:32:52\n",
      "  Epoch 69, Batch 81/173, Loss: 1.0304, Elapsed: 0:37:30\n",
      "  Epoch 69, Batch 91/173, Loss: 0.9898, Elapsed: 0:42:00\n",
      "  Epoch 69, Batch 101/173, Loss: 1.0162, Elapsed: 0:46:32\n",
      "  Epoch 69, Batch 111/173, Loss: 1.1037, Elapsed: 0:51:16\n",
      "  Epoch 69, Batch 121/173, Loss: 0.9620, Elapsed: 0:56:01\n",
      "  Epoch 69, Batch 131/173, Loss: 1.1355, Elapsed: 1:00:45\n",
      "  Epoch 69, Batch 141/173, Loss: 1.1610, Elapsed: 1:05:16\n",
      "  Epoch 69, Batch 151/173, Loss: 1.1293, Elapsed: 1:09:49\n",
      "  Epoch 69, Batch 161/173, Loss: 1.1146, Elapsed: 1:14:30\n",
      "  Epoch 69, Batch 171/173, Loss: 1.1175, Elapsed: 1:53:17\n",
      "\n",
      "  Average training loss: 1.05\n",
      "  Training epoch took: 1:54:33\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:04:08\n",
      "\n",
      "======== Epoch 70 / 100 ========\n",
      "Training...\n",
      "  Epoch 70, Batch 1/173, Loss: 1.0878, Elapsed: 0:00:13\n",
      "  Epoch 70, Batch 11/173, Loss: 1.0234, Elapsed: 0:05:12\n",
      "  Epoch 70, Batch 21/173, Loss: 0.9926, Elapsed: 0:09:51\n",
      "  Epoch 70, Batch 31/173, Loss: 1.1174, Elapsed: 0:14:33\n",
      "  Epoch 70, Batch 41/173, Loss: 1.0239, Elapsed: 0:19:29\n",
      "  Epoch 70, Batch 51/173, Loss: 0.9503, Elapsed: 0:24:21\n",
      "  Epoch 70, Batch 61/173, Loss: 1.0453, Elapsed: 0:28:55\n",
      "  Epoch 70, Batch 71/173, Loss: 1.0542, Elapsed: 0:33:33\n",
      "  Epoch 70, Batch 81/173, Loss: 1.0597, Elapsed: 0:55:01\n",
      "  Epoch 70, Batch 91/173, Loss: 1.0266, Elapsed: 0:59:49\n",
      "  Epoch 70, Batch 101/173, Loss: 1.3088, Elapsed: 1:04:40\n",
      "  Epoch 70, Batch 111/173, Loss: 1.0139, Elapsed: 1:09:29\n",
      "  Epoch 70, Batch 121/173, Loss: 1.0170, Elapsed: 1:14:18\n",
      "  Epoch 70, Batch 131/173, Loss: 0.9906, Elapsed: 1:19:12\n",
      "  Epoch 70, Batch 141/173, Loss: 1.0977, Elapsed: 1:23:55\n",
      "  Epoch 70, Batch 151/173, Loss: 1.0162, Elapsed: 1:28:30\n",
      "  Epoch 70, Batch 161/173, Loss: 1.0827, Elapsed: 1:33:02\n",
      "  Epoch 70, Batch 171/173, Loss: 0.9888, Elapsed: 1:37:41\n",
      "\n",
      "  Average training loss: 1.04\n",
      "  Training epoch took: 1:38:28\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 71 / 100 ========\n",
      "Training...\n",
      "  Epoch 71, Batch 1/173, Loss: 1.0836, Elapsed: 0:00:13\n",
      "  Epoch 71, Batch 11/173, Loss: 1.0475, Elapsed: 0:05:01\n",
      "  Epoch 71, Batch 21/173, Loss: 1.0340, Elapsed: 0:09:49\n",
      "  Epoch 71, Batch 31/173, Loss: 1.0203, Elapsed: 0:17:44\n",
      "  Epoch 71, Batch 41/173, Loss: 1.0846, Elapsed: 0:41:00\n",
      "  Epoch 71, Batch 51/173, Loss: 1.0980, Elapsed: 0:46:15\n",
      "  Epoch 71, Batch 61/173, Loss: 1.1992, Elapsed: 0:51:12\n",
      "  Epoch 71, Batch 71/173, Loss: 1.1424, Elapsed: 0:55:55\n",
      "  Epoch 71, Batch 81/173, Loss: 1.0932, Elapsed: 1:00:37\n",
      "  Epoch 71, Batch 91/173, Loss: 1.0436, Elapsed: 1:05:34\n",
      "  Epoch 71, Batch 101/173, Loss: 0.9497, Elapsed: 1:10:35\n",
      "  Epoch 71, Batch 111/173, Loss: 1.0184, Elapsed: 1:15:16\n",
      "  Epoch 71, Batch 121/173, Loss: 1.1435, Elapsed: 1:19:51\n",
      "  Epoch 71, Batch 131/173, Loss: 1.0411, Elapsed: 1:24:44\n",
      "  Epoch 71, Batch 141/173, Loss: 1.1310, Elapsed: 1:29:46\n",
      "  Epoch 71, Batch 151/173, Loss: 0.9922, Elapsed: 1:34:44\n",
      "  Epoch 71, Batch 161/173, Loss: 0.9564, Elapsed: 1:39:21\n",
      "  Epoch 71, Batch 171/173, Loss: 1.0043, Elapsed: 1:44:05\n",
      "\n",
      "  Average training loss: 1.04\n",
      "  Training epoch took: 1:44:53\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 72 / 100 ========\n",
      "Training...\n",
      "  Epoch 72, Batch 1/173, Loss: 1.0197, Elapsed: 0:00:13\n",
      "  Epoch 72, Batch 11/173, Loss: 1.0734, Elapsed: 0:05:13\n",
      "  Epoch 72, Batch 21/173, Loss: 1.1530, Elapsed: 0:10:00\n",
      "  Epoch 72, Batch 31/173, Loss: 1.0273, Elapsed: 0:34:02\n",
      "  Epoch 72, Batch 41/173, Loss: 0.9457, Elapsed: 1:36:52\n",
      "  Epoch 72, Batch 51/173, Loss: 1.0029, Elapsed: 1:44:55\n",
      "  Epoch 72, Batch 61/173, Loss: 1.1418, Elapsed: 1:49:38\n",
      "  Epoch 72, Batch 71/173, Loss: 1.0427, Elapsed: 1:54:14\n",
      "  Epoch 72, Batch 81/173, Loss: 1.0144, Elapsed: 1:58:55\n",
      "  Epoch 72, Batch 91/173, Loss: 0.9898, Elapsed: 2:03:55\n",
      "  Epoch 72, Batch 101/173, Loss: 1.0669, Elapsed: 2:08:56\n",
      "  Epoch 72, Batch 111/173, Loss: 1.0693, Elapsed: 2:13:42\n",
      "  Epoch 72, Batch 121/173, Loss: 1.0176, Elapsed: 2:18:17\n",
      "  Epoch 72, Batch 131/173, Loss: 1.1228, Elapsed: 2:23:16\n",
      "  Epoch 72, Batch 141/173, Loss: 1.0082, Elapsed: 2:28:20\n",
      "  Epoch 72, Batch 151/173, Loss: 1.1209, Elapsed: 2:33:01\n",
      "  Epoch 72, Batch 161/173, Loss: 1.1511, Elapsed: 2:37:48\n",
      "  Epoch 72, Batch 171/173, Loss: 0.9421, Elapsed: 2:48:18\n",
      "\n",
      "  Average training loss: 1.04\n",
      "  Training epoch took: 2:49:11\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:32\n",
      "\n",
      "======== Epoch 73 / 100 ========\n",
      "Training...\n",
      "  Epoch 73, Batch 1/173, Loss: 0.9864, Elapsed: 0:00:13\n",
      "  Epoch 73, Batch 11/173, Loss: 1.1696, Elapsed: 0:05:40\n",
      "  Epoch 73, Batch 21/173, Loss: 1.0109, Elapsed: 0:10:59\n",
      "  Epoch 73, Batch 31/173, Loss: 1.0783, Elapsed: 0:16:42\n",
      "  Epoch 73, Batch 41/173, Loss: 0.8732, Elapsed: 0:22:16\n",
      "  Epoch 73, Batch 51/173, Loss: 0.9679, Elapsed: 0:27:11\n",
      "  Epoch 73, Batch 61/173, Loss: 1.1615, Elapsed: 0:32:35\n",
      "  Epoch 73, Batch 71/173, Loss: 0.9706, Elapsed: 0:37:54\n",
      "  Epoch 73, Batch 81/173, Loss: 1.1063, Elapsed: 0:42:42\n",
      "  Epoch 73, Batch 91/173, Loss: 1.1019, Elapsed: 0:47:48\n",
      "  Epoch 73, Batch 101/173, Loss: 1.0153, Elapsed: 0:53:15\n",
      "  Epoch 73, Batch 111/173, Loss: 0.9745, Elapsed: 0:58:08\n",
      "  Epoch 73, Batch 121/173, Loss: 1.0393, Elapsed: 1:02:52\n",
      "  Epoch 73, Batch 131/173, Loss: 1.1950, Elapsed: 1:07:46\n",
      "  Epoch 73, Batch 141/173, Loss: 1.0637, Elapsed: 1:12:46\n",
      "  Epoch 73, Batch 151/173, Loss: 1.0136, Elapsed: 1:17:44\n",
      "  Epoch 73, Batch 161/173, Loss: 0.9903, Elapsed: 1:22:40\n",
      "  Epoch 73, Batch 171/173, Loss: 1.0407, Elapsed: 1:27:49\n",
      "\n",
      "  Average training loss: 1.03\n",
      "  Training epoch took: 1:28:38\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 74 / 100 ========\n",
      "Training...\n",
      "  Epoch 74, Batch 1/173, Loss: 1.0645, Elapsed: 0:00:12\n",
      "  Epoch 74, Batch 11/173, Loss: 1.0404, Elapsed: 0:05:13\n",
      "  Epoch 74, Batch 21/173, Loss: 0.9713, Elapsed: 0:10:02\n",
      "  Epoch 74, Batch 31/173, Loss: 0.9747, Elapsed: 0:14:59\n",
      "  Epoch 74, Batch 41/173, Loss: 1.0759, Elapsed: 0:20:16\n",
      "  Epoch 74, Batch 51/173, Loss: 0.9486, Elapsed: 0:25:25\n",
      "  Epoch 74, Batch 61/173, Loss: 1.1232, Elapsed: 0:30:23\n",
      "  Epoch 74, Batch 71/173, Loss: 1.1044, Elapsed: 0:35:25\n",
      "  Epoch 74, Batch 81/173, Loss: 0.9810, Elapsed: 0:40:54\n",
      "  Epoch 74, Batch 91/173, Loss: 0.9560, Elapsed: 0:45:57\n",
      "  Epoch 74, Batch 101/173, Loss: 1.0605, Elapsed: 0:50:40\n",
      "  Epoch 74, Batch 111/173, Loss: 0.9799, Elapsed: 0:55:41\n",
      "  Epoch 74, Batch 121/173, Loss: 0.9913, Elapsed: 1:39:28\n",
      "  Epoch 74, Batch 131/173, Loss: 0.9519, Elapsed: 1:46:43\n",
      "  Epoch 74, Batch 141/173, Loss: 0.9332, Elapsed: 1:51:25\n",
      "  Epoch 74, Batch 151/173, Loss: 1.0416, Elapsed: 1:56:16\n",
      "  Epoch 74, Batch 161/173, Loss: 0.9708, Elapsed: 2:01:13\n",
      "  Epoch 74, Batch 171/173, Loss: 0.9766, Elapsed: 2:06:12\n",
      "\n",
      "  Average training loss: 1.04\n",
      "  Training epoch took: 2:06:59\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 75 / 100 ========\n",
      "Training...\n",
      "  Epoch 75, Batch 1/173, Loss: 0.9817, Elapsed: 0:00:12\n",
      "  Epoch 75, Batch 11/173, Loss: 1.0160, Elapsed: 0:04:52\n",
      "  Epoch 75, Batch 21/173, Loss: 1.0797, Elapsed: 0:09:35\n",
      "  Epoch 75, Batch 31/173, Loss: 0.9812, Elapsed: 0:14:28\n",
      "  Epoch 75, Batch 41/173, Loss: 1.0470, Elapsed: 0:19:20\n",
      "  Epoch 75, Batch 51/173, Loss: 0.9280, Elapsed: 0:23:55\n",
      "  Epoch 75, Batch 61/173, Loss: 1.0021, Elapsed: 0:28:31\n",
      "  Epoch 75, Batch 71/173, Loss: 0.9321, Elapsed: 0:33:22\n",
      "  Epoch 75, Batch 81/173, Loss: 1.0851, Elapsed: 0:38:22\n",
      "  Epoch 75, Batch 91/173, Loss: 1.1351, Elapsed: 0:43:13\n",
      "  Epoch 75, Batch 101/173, Loss: 0.9127, Elapsed: 0:47:54\n",
      "  Epoch 75, Batch 111/173, Loss: 1.0722, Elapsed: 0:52:41\n",
      "  Epoch 75, Batch 121/173, Loss: 0.9799, Elapsed: 0:57:40\n",
      "  Epoch 75, Batch 131/173, Loss: 0.9864, Elapsed: 1:02:50\n",
      "  Epoch 75, Batch 141/173, Loss: 1.0007, Elapsed: 1:07:36\n",
      "  Epoch 75, Batch 151/173, Loss: 1.0456, Elapsed: 1:12:14\n",
      "  Epoch 75, Batch 161/173, Loss: 1.0736, Elapsed: 1:17:23\n",
      "  Epoch 75, Batch 171/173, Loss: 0.9847, Elapsed: 1:22:28\n",
      "\n",
      "  Average training loss: 1.04\n",
      "  Training epoch took: 1:23:15\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 76 / 100 ========\n",
      "Training...\n",
      "  Epoch 76, Batch 1/173, Loss: 1.2093, Elapsed: 0:00:13\n",
      "  Epoch 76, Batch 11/173, Loss: 1.1138, Elapsed: 0:04:59\n",
      "  Epoch 76, Batch 21/173, Loss: 1.0852, Elapsed: 0:09:59\n",
      "  Epoch 76, Batch 31/173, Loss: 1.0639, Elapsed: 0:15:11\n",
      "  Epoch 76, Batch 41/173, Loss: 1.1564, Elapsed: 0:20:14\n",
      "  Epoch 76, Batch 51/173, Loss: 1.0328, Elapsed: 0:24:53\n",
      "  Epoch 76, Batch 61/173, Loss: 0.9864, Elapsed: 0:29:43\n",
      "  Epoch 76, Batch 71/173, Loss: 1.0268, Elapsed: 0:34:40\n",
      "  Epoch 76, Batch 81/173, Loss: 0.9741, Elapsed: 0:39:28\n",
      "  Epoch 76, Batch 91/173, Loss: 1.0640, Elapsed: 0:44:11\n",
      "  Epoch 76, Batch 101/173, Loss: 1.0273, Elapsed: 0:49:06\n",
      "  Epoch 76, Batch 111/173, Loss: 0.9932, Elapsed: 0:54:17\n",
      "  Epoch 76, Batch 121/173, Loss: 0.9944, Elapsed: 0:59:06\n",
      "  Epoch 76, Batch 131/173, Loss: 0.8365, Elapsed: 1:03:50\n",
      "  Epoch 76, Batch 141/173, Loss: 1.0657, Elapsed: 1:08:30\n",
      "  Epoch 76, Batch 151/173, Loss: 1.0622, Elapsed: 1:13:31\n",
      "  Epoch 76, Batch 161/173, Loss: 0.9363, Elapsed: 1:18:27\n",
      "  Epoch 76, Batch 171/173, Loss: 0.8797, Elapsed: 1:23:17\n",
      "\n",
      "  Average training loss: 1.03\n",
      "  Training epoch took: 1:24:02\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 77 / 100 ========\n",
      "Training...\n",
      "  Epoch 77, Batch 1/173, Loss: 1.0025, Elapsed: 0:00:13\n",
      "  Epoch 77, Batch 11/173, Loss: 0.9786, Elapsed: 0:04:47\n",
      "  Epoch 77, Batch 21/173, Loss: 0.9857, Elapsed: 0:09:39\n",
      "  Epoch 77, Batch 31/173, Loss: 1.0800, Elapsed: 0:14:33\n",
      "  Epoch 77, Batch 41/173, Loss: 1.1066, Elapsed: 0:19:12\n",
      "  Epoch 77, Batch 51/173, Loss: 0.9972, Elapsed: 0:23:50\n",
      "  Epoch 77, Batch 61/173, Loss: 0.9763, Elapsed: 0:28:31\n",
      "  Epoch 77, Batch 71/173, Loss: 1.0468, Elapsed: 0:33:30\n",
      "  Epoch 77, Batch 81/173, Loss: 0.9774, Elapsed: 0:38:31\n",
      "  Epoch 77, Batch 91/173, Loss: 0.9771, Elapsed: 0:43:14\n",
      "  Epoch 77, Batch 101/173, Loss: 0.9289, Elapsed: 0:47:49\n",
      "  Epoch 77, Batch 111/173, Loss: 0.9899, Elapsed: 0:52:46\n",
      "  Epoch 77, Batch 121/173, Loss: 1.0023, Elapsed: 0:57:41\n",
      "  Epoch 77, Batch 131/173, Loss: 1.0455, Elapsed: 1:02:25\n",
      "  Epoch 77, Batch 141/173, Loss: 0.9846, Elapsed: 1:07:01\n",
      "  Epoch 77, Batch 151/173, Loss: 1.0295, Elapsed: 1:11:43\n",
      "  Epoch 77, Batch 161/173, Loss: 0.9412, Elapsed: 1:16:40\n",
      "  Epoch 77, Batch 171/173, Loss: 0.9791, Elapsed: 1:21:38\n",
      "\n",
      "  Average training loss: 1.03\n",
      "  Training epoch took: 1:22:27\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 78 / 100 ========\n",
      "Training...\n",
      "  Epoch 78, Batch 1/173, Loss: 1.0292, Elapsed: 0:00:12\n",
      "  Epoch 78, Batch 11/173, Loss: 1.0357, Elapsed: 0:04:49\n",
      "  Epoch 78, Batch 21/173, Loss: 1.1755, Elapsed: 0:09:26\n",
      "  Epoch 78, Batch 31/173, Loss: 1.0223, Elapsed: 0:14:25\n",
      "  Epoch 78, Batch 41/173, Loss: 1.1829, Elapsed: 0:19:18\n",
      "  Epoch 78, Batch 51/173, Loss: 1.0809, Elapsed: 0:23:59\n",
      "  Epoch 78, Batch 61/173, Loss: 1.0708, Elapsed: 0:28:40\n",
      "  Epoch 78, Batch 71/173, Loss: 1.0102, Elapsed: 0:33:27\n",
      "  Epoch 78, Batch 81/173, Loss: 1.0164, Elapsed: 0:38:30\n",
      "  Epoch 78, Batch 91/173, Loss: 1.0758, Elapsed: 0:43:18\n",
      "  Epoch 78, Batch 101/173, Loss: 1.0111, Elapsed: 0:47:59\n",
      "  Epoch 78, Batch 111/173, Loss: 0.9681, Elapsed: 0:52:39\n",
      "  Epoch 78, Batch 121/173, Loss: 1.1017, Elapsed: 0:57:38\n",
      "  Epoch 78, Batch 131/173, Loss: 1.0072, Elapsed: 1:02:34\n",
      "  Epoch 78, Batch 141/173, Loss: 1.0397, Elapsed: 1:07:17\n",
      "  Epoch 78, Batch 151/173, Loss: 0.9891, Elapsed: 1:11:52\n",
      "  Epoch 78, Batch 161/173, Loss: 1.0268, Elapsed: 1:16:37\n",
      "  Epoch 78, Batch 171/173, Loss: 0.9879, Elapsed: 1:21:35\n",
      "\n",
      "  Average training loss: 1.03\n",
      "  Training epoch took: 1:22:24\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 79 / 100 ========\n",
      "Training...\n",
      "  Epoch 79, Batch 1/173, Loss: 1.1528, Elapsed: 0:00:12\n",
      "  Epoch 79, Batch 11/173, Loss: 0.9741, Elapsed: 0:04:54\n",
      "  Epoch 79, Batch 21/173, Loss: 1.1284, Elapsed: 0:09:32\n",
      "  Epoch 79, Batch 31/173, Loss: 1.0828, Elapsed: 0:14:11\n",
      "  Epoch 79, Batch 41/173, Loss: 1.0694, Elapsed: 0:19:08\n",
      "  Epoch 79, Batch 51/173, Loss: 1.1093, Elapsed: 0:24:10\n",
      "  Epoch 79, Batch 61/173, Loss: 1.0277, Elapsed: 0:28:56\n",
      "  Epoch 79, Batch 71/173, Loss: 1.0797, Elapsed: 0:33:35\n",
      "  Epoch 79, Batch 81/173, Loss: 1.0277, Elapsed: 0:38:29\n",
      "  Epoch 79, Batch 91/173, Loss: 1.0222, Elapsed: 0:43:30\n",
      "  Epoch 79, Batch 101/173, Loss: 1.0118, Elapsed: 0:48:12\n",
      "  Epoch 79, Batch 111/173, Loss: 1.0303, Elapsed: 0:52:45\n",
      "  Epoch 79, Batch 121/173, Loss: 1.0096, Elapsed: 0:57:25\n",
      "  Epoch 79, Batch 131/173, Loss: 0.9565, Elapsed: 1:02:18\n",
      "  Epoch 79, Batch 141/173, Loss: 1.0908, Elapsed: 1:07:11\n",
      "  Epoch 79, Batch 151/173, Loss: 0.9712, Elapsed: 1:11:50\n",
      "  Epoch 79, Batch 161/173, Loss: 1.0268, Elapsed: 1:16:27\n",
      "  Epoch 79, Batch 171/173, Loss: 1.0679, Elapsed: 1:21:17\n",
      "\n",
      "  Average training loss: 1.03\n",
      "  Training epoch took: 1:22:05\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 80 / 100 ========\n",
      "Training...\n",
      "  Epoch 80, Batch 1/173, Loss: 0.9814, Elapsed: 0:00:14\n",
      "  Epoch 80, Batch 11/173, Loss: 1.0099, Elapsed: 0:05:10\n",
      "  Epoch 80, Batch 21/173, Loss: 0.9056, Elapsed: 0:09:59\n",
      "  Epoch 80, Batch 31/173, Loss: 1.0449, Elapsed: 0:14:35\n",
      "  Epoch 80, Batch 41/173, Loss: 1.0817, Elapsed: 0:19:19\n",
      "  Epoch 80, Batch 51/173, Loss: 1.0323, Elapsed: 0:24:11\n",
      "  Epoch 80, Batch 61/173, Loss: 1.2288, Elapsed: 0:29:01\n",
      "  Epoch 80, Batch 71/173, Loss: 1.0413, Elapsed: 0:33:38\n",
      "  Epoch 80, Batch 81/173, Loss: 1.0885, Elapsed: 0:38:19\n",
      "  Epoch 80, Batch 91/173, Loss: 1.0326, Elapsed: 0:43:14\n",
      "  Epoch 80, Batch 101/173, Loss: 0.9687, Elapsed: 0:48:18\n",
      "  Epoch 80, Batch 111/173, Loss: 1.0059, Elapsed: 0:52:56\n",
      "  Epoch 80, Batch 121/173, Loss: 1.0081, Elapsed: 0:57:33\n",
      "  Epoch 80, Batch 131/173, Loss: 1.0742, Elapsed: 1:02:27\n",
      "  Epoch 80, Batch 141/173, Loss: 0.9555, Elapsed: 1:07:28\n",
      "  Epoch 80, Batch 151/173, Loss: 0.9760, Elapsed: 1:12:36\n",
      "  Epoch 80, Batch 161/173, Loss: 1.0021, Elapsed: 1:17:30\n",
      "  Epoch 80, Batch 171/173, Loss: 1.0465, Elapsed: 1:22:20\n",
      "\n",
      "  Average training loss: 1.03\n",
      "  Training epoch took: 1:23:09\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:56\n",
      "\n",
      "======== Epoch 81 / 100 ========\n",
      "Training...\n",
      "  Epoch 81, Batch 1/173, Loss: 1.0970, Elapsed: 0:00:13\n",
      "  Epoch 81, Batch 11/173, Loss: 1.0523, Elapsed: 0:05:26\n",
      "  Epoch 81, Batch 21/173, Loss: 1.0771, Elapsed: 0:10:14\n",
      "  Epoch 81, Batch 31/173, Loss: 1.0197, Elapsed: 0:14:46\n",
      "  Epoch 81, Batch 41/173, Loss: 1.1023, Elapsed: 0:19:17\n",
      "  Epoch 81, Batch 51/173, Loss: 1.0342, Elapsed: 0:23:55\n",
      "  Epoch 81, Batch 61/173, Loss: 1.0472, Elapsed: 0:28:43\n",
      "  Epoch 81, Batch 71/173, Loss: 1.0264, Elapsed: 0:33:32\n",
      "  Epoch 81, Batch 81/173, Loss: 1.0739, Elapsed: 0:38:07\n",
      "  Epoch 81, Batch 91/173, Loss: 1.0615, Elapsed: 0:42:43\n",
      "  Epoch 81, Batch 101/173, Loss: 0.9378, Elapsed: 0:47:32\n",
      "  Epoch 81, Batch 111/173, Loss: 0.9446, Elapsed: 0:52:31\n",
      "  Epoch 81, Batch 121/173, Loss: 1.0297, Elapsed: 0:57:20\n",
      "  Epoch 81, Batch 131/173, Loss: 1.0846, Elapsed: 1:01:57\n",
      "  Epoch 81, Batch 141/173, Loss: 0.9647, Elapsed: 1:06:37\n",
      "  Epoch 81, Batch 151/173, Loss: 0.9929, Elapsed: 1:11:37\n",
      "  Epoch 81, Batch 161/173, Loss: 1.0442, Elapsed: 1:16:35\n",
      "  Epoch 81, Batch 171/173, Loss: 0.9387, Elapsed: 1:21:11\n",
      "\n",
      "  Average training loss: 1.03\n",
      "  Training epoch took: 1:21:56\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 82 / 100 ========\n",
      "Training...\n",
      "  Epoch 82, Batch 1/173, Loss: 0.9560, Elapsed: 0:00:12\n",
      "  Epoch 82, Batch 11/173, Loss: 1.0671, Elapsed: 0:04:50\n",
      "  Epoch 82, Batch 21/173, Loss: 1.1000, Elapsed: 0:09:43\n",
      "  Epoch 82, Batch 31/173, Loss: 1.1063, Elapsed: 0:14:39\n",
      "  Epoch 82, Batch 41/173, Loss: 1.1822, Elapsed: 0:19:36\n",
      "  Epoch 82, Batch 51/173, Loss: 1.1305, Elapsed: 0:24:12\n",
      "  Epoch 82, Batch 61/173, Loss: 0.9901, Elapsed: 0:28:53\n",
      "  Epoch 82, Batch 71/173, Loss: 0.9603, Elapsed: 0:33:47\n",
      "  Epoch 82, Batch 81/173, Loss: 0.9896, Elapsed: 0:38:42\n",
      "  Epoch 82, Batch 91/173, Loss: 1.0199, Elapsed: 0:43:18\n",
      "  Epoch 82, Batch 101/173, Loss: 1.0736, Elapsed: 0:47:57\n",
      "  Epoch 82, Batch 111/173, Loss: 1.0738, Elapsed: 0:52:48\n",
      "  Epoch 82, Batch 121/173, Loss: 0.9173, Elapsed: 0:57:47\n",
      "  Epoch 82, Batch 131/173, Loss: 0.9429, Elapsed: 1:02:31\n",
      "  Epoch 82, Batch 141/173, Loss: 0.9841, Elapsed: 1:07:08\n",
      "  Epoch 82, Batch 151/173, Loss: 1.1230, Elapsed: 1:11:43\n",
      "  Epoch 82, Batch 161/173, Loss: 0.9840, Elapsed: 1:16:35\n",
      "  Epoch 82, Batch 171/173, Loss: 0.9799, Elapsed: 1:21:34\n",
      "\n",
      "  Average training loss: 1.03\n",
      "  Training epoch took: 1:22:21\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 83 / 100 ========\n",
      "Training...\n",
      "  Epoch 83, Batch 1/173, Loss: 0.9997, Elapsed: 0:00:12\n",
      "  Epoch 83, Batch 11/173, Loss: 1.1000, Elapsed: 0:05:02\n",
      "  Epoch 83, Batch 21/173, Loss: 0.9693, Elapsed: 0:09:38\n",
      "  Epoch 83, Batch 31/173, Loss: 1.1131, Elapsed: 0:14:25\n",
      "  Epoch 83, Batch 41/173, Loss: 0.9936, Elapsed: 0:19:21\n",
      "  Epoch 83, Batch 51/173, Loss: 1.0144, Elapsed: 0:24:12\n",
      "  Epoch 83, Batch 61/173, Loss: 1.0729, Elapsed: 0:28:49\n",
      "  Epoch 83, Batch 71/173, Loss: 1.0384, Elapsed: 0:33:25\n",
      "  Epoch 83, Batch 81/173, Loss: 1.0539, Elapsed: 0:38:17\n",
      "  Epoch 83, Batch 91/173, Loss: 1.0696, Elapsed: 0:43:18\n",
      "  Epoch 83, Batch 101/173, Loss: 0.9254, Elapsed: 0:47:59\n",
      "  Epoch 83, Batch 111/173, Loss: 1.0316, Elapsed: 0:52:36\n",
      "  Epoch 83, Batch 121/173, Loss: 0.9529, Elapsed: 0:57:17\n",
      "  Epoch 83, Batch 131/173, Loss: 1.0129, Elapsed: 1:02:08\n",
      "  Epoch 83, Batch 141/173, Loss: 1.0495, Elapsed: 1:07:04\n",
      "  Epoch 83, Batch 151/173, Loss: 1.0480, Elapsed: 1:11:48\n",
      "  Epoch 83, Batch 161/173, Loss: 1.1152, Elapsed: 1:16:22\n",
      "  Epoch 83, Batch 171/173, Loss: 1.0403, Elapsed: 1:21:10\n",
      "\n",
      "  Average training loss: 1.03\n",
      "  Training epoch took: 1:21:57\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 84 / 100 ========\n",
      "Training...\n",
      "  Epoch 84, Batch 1/173, Loss: 0.9944, Elapsed: 0:00:13\n",
      "  Epoch 84, Batch 11/173, Loss: 1.0104, Elapsed: 0:05:08\n",
      "  Epoch 84, Batch 21/173, Loss: 0.9672, Elapsed: 0:09:52\n",
      "  Epoch 84, Batch 31/173, Loss: 0.9781, Elapsed: 0:14:29\n",
      "  Epoch 84, Batch 41/173, Loss: 0.9976, Elapsed: 0:19:10\n",
      "  Epoch 84, Batch 51/173, Loss: 0.9365, Elapsed: 0:24:10\n",
      "  Epoch 84, Batch 61/173, Loss: 1.0609, Elapsed: 0:29:04\n",
      "  Epoch 84, Batch 71/173, Loss: 1.0001, Elapsed: 0:33:49\n",
      "  Epoch 84, Batch 81/173, Loss: 0.8900, Elapsed: 0:38:26\n",
      "  Epoch 84, Batch 91/173, Loss: 0.9410, Elapsed: 0:43:26\n",
      "  Epoch 84, Batch 101/173, Loss: 1.0255, Elapsed: 1:20:34\n",
      "  Epoch 84, Batch 111/173, Loss: 0.9901, Elapsed: 1:29:06\n",
      "  Epoch 84, Batch 121/173, Loss: 1.0364, Elapsed: 1:33:53\n",
      "  Epoch 84, Batch 131/173, Loss: 1.0860, Elapsed: 1:43:11\n",
      "  Epoch 84, Batch 141/173, Loss: 0.9696, Elapsed: 1:48:13\n",
      "  Epoch 84, Batch 151/173, Loss: 1.0678, Elapsed: 1:53:15\n",
      "  Epoch 84, Batch 161/173, Loss: 1.1354, Elapsed: 1:58:18\n",
      "  Epoch 84, Batch 171/173, Loss: 0.9411, Elapsed: 2:04:05\n",
      "\n",
      "  Average training loss: 1.02\n",
      "  Training epoch took: 2:04:59\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:33\n",
      "\n",
      "======== Epoch 85 / 100 ========\n",
      "Training...\n",
      "  Epoch 85, Batch 1/173, Loss: 1.0300, Elapsed: 0:00:13\n",
      "  Epoch 85, Batch 11/173, Loss: 1.0115, Elapsed: 0:05:15\n",
      "  Epoch 85, Batch 21/173, Loss: 1.0633, Elapsed: 0:10:41\n",
      "  Epoch 85, Batch 31/173, Loss: 1.0172, Elapsed: 0:16:29\n",
      "  Epoch 85, Batch 41/173, Loss: 1.0328, Elapsed: 0:21:37\n",
      "  Epoch 85, Batch 51/173, Loss: 0.9691, Elapsed: 0:26:48\n",
      "  Epoch 85, Batch 61/173, Loss: 1.0972, Elapsed: 0:32:37\n",
      "  Epoch 85, Batch 71/173, Loss: 1.0483, Elapsed: 0:37:31\n",
      "  Epoch 85, Batch 81/173, Loss: 1.0919, Elapsed: 0:42:29\n",
      "  Epoch 85, Batch 91/173, Loss: 1.0246, Elapsed: 0:47:52\n",
      "  Epoch 85, Batch 101/173, Loss: 1.0623, Elapsed: 0:53:33\n",
      "  Epoch 85, Batch 111/173, Loss: 1.0067, Elapsed: 0:58:30\n",
      "  Epoch 85, Batch 121/173, Loss: 1.1023, Elapsed: 1:04:29\n",
      "  Epoch 85, Batch 131/173, Loss: 1.1099, Elapsed: 2:12:48\n",
      "  Epoch 85, Batch 141/173, Loss: 1.1315, Elapsed: 2:27:56\n",
      "  Epoch 85, Batch 151/173, Loss: 1.0424, Elapsed: 2:33:12\n",
      "  Epoch 85, Batch 161/173, Loss: 1.0344, Elapsed: 2:38:22\n",
      "  Epoch 85, Batch 171/173, Loss: 0.9625, Elapsed: 2:43:18\n",
      "\n",
      "  Average training loss: 1.03\n",
      "  Training epoch took: 2:44:09\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:35\n",
      "\n",
      "======== Epoch 86 / 100 ========\n",
      "Training...\n",
      "  Epoch 86, Batch 1/173, Loss: 0.9927, Elapsed: 0:00:12\n",
      "  Epoch 86, Batch 11/173, Loss: 1.0046, Elapsed: 0:05:06\n",
      "  Epoch 86, Batch 21/173, Loss: 1.0195, Elapsed: 0:10:46\n",
      "  Epoch 86, Batch 31/173, Loss: 1.0140, Elapsed: 0:15:48\n",
      "  Epoch 86, Batch 41/173, Loss: 0.9724, Elapsed: 0:20:45\n",
      "  Epoch 86, Batch 51/173, Loss: 1.0588, Elapsed: 0:25:58\n",
      "  Epoch 86, Batch 61/173, Loss: 0.9696, Elapsed: 0:31:03\n",
      "  Epoch 86, Batch 71/173, Loss: 1.0276, Elapsed: 0:36:05\n",
      "  Epoch 86, Batch 81/173, Loss: 1.0491, Elapsed: 0:41:19\n",
      "  Epoch 86, Batch 91/173, Loss: 0.9973, Elapsed: 0:46:30\n",
      "  Epoch 86, Batch 101/173, Loss: 0.9901, Elapsed: 0:51:28\n",
      "  Epoch 86, Batch 111/173, Loss: 1.0100, Elapsed: 0:56:39\n",
      "  Epoch 86, Batch 121/173, Loss: 1.0361, Elapsed: 1:02:06\n",
      "  Epoch 86, Batch 131/173, Loss: 1.0383, Elapsed: 1:07:11\n",
      "  Epoch 86, Batch 141/173, Loss: 1.0126, Elapsed: 1:12:11\n",
      "  Epoch 86, Batch 151/173, Loss: 0.9542, Elapsed: 1:17:43\n",
      "  Epoch 86, Batch 161/173, Loss: 1.0762, Elapsed: 1:22:54\n",
      "  Epoch 86, Batch 171/173, Loss: 1.0628, Elapsed: 1:27:50\n",
      "\n",
      "  Average training loss: 1.02\n",
      "  Training epoch took: 1:28:38\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.13\n",
      "  Validation took: 0:01:34\n",
      "\n",
      "======== Epoch 87 / 100 ========\n",
      "Training...\n",
      "  Epoch 87, Batch 1/173, Loss: 1.1040, Elapsed: 0:00:12\n",
      "  Epoch 87, Batch 11/173, Loss: 0.9707, Elapsed: 0:10:19\n",
      "  Epoch 87, Batch 21/173, Loss: 1.0269, Elapsed: 0:20:22\n",
      "  Epoch 87, Batch 31/173, Loss: 1.0094, Elapsed: 0:31:32\n",
      "  Epoch 87, Batch 41/173, Loss: 1.1488, Elapsed: 0:36:31\n",
      "  Epoch 87, Batch 51/173, Loss: 0.9093, Elapsed: 0:41:46\n",
      "  Epoch 87, Batch 61/173, Loss: 1.0537, Elapsed: 0:46:43\n",
      "  Epoch 87, Batch 71/173, Loss: 1.0481, Elapsed: 0:51:29\n",
      "  Epoch 87, Batch 81/173, Loss: 0.9337, Elapsed: 0:56:50\n",
      "  Epoch 87, Batch 91/173, Loss: 0.9375, Elapsed: 1:02:21\n",
      "  Epoch 87, Batch 101/173, Loss: 1.0019, Elapsed: 1:07:22\n",
      "  Epoch 87, Batch 111/173, Loss: 1.1787, Elapsed: 1:12:48\n",
      "  Epoch 87, Batch 121/173, Loss: 1.0059, Elapsed: 1:18:00\n",
      "  Epoch 87, Batch 131/173, Loss: 0.9754, Elapsed: 1:23:00\n",
      "  Epoch 87, Batch 141/173, Loss: 1.1196, Elapsed: 1:28:09\n",
      "  Epoch 87, Batch 151/173, Loss: 1.0488, Elapsed: 1:33:53\n",
      "  Epoch 87, Batch 161/173, Loss: 1.1158, Elapsed: 1:39:26\n",
      "  Epoch 87, Batch 171/173, Loss: 1.1330, Elapsed: 1:44:11\n",
      "\n",
      "  Average training loss: 1.02\n",
      "  Training epoch took: 1:44:59\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 88 / 100 ========\n",
      "Training...\n",
      "  Epoch 88, Batch 1/173, Loss: 1.0780, Elapsed: 0:00:13\n",
      "  Epoch 88, Batch 11/173, Loss: 1.0975, Elapsed: 0:05:29\n",
      "  Epoch 88, Batch 21/173, Loss: 1.1058, Elapsed: 0:10:45\n",
      "  Epoch 88, Batch 31/173, Loss: 1.0787, Elapsed: 0:15:46\n",
      "  Epoch 88, Batch 41/173, Loss: 0.9133, Elapsed: 0:21:11\n",
      "  Epoch 88, Batch 51/173, Loss: 1.0228, Elapsed: 0:26:27\n",
      "  Epoch 88, Batch 61/173, Loss: 1.1026, Elapsed: 0:31:14\n",
      "  Epoch 88, Batch 71/173, Loss: 0.9421, Elapsed: 0:35:58\n",
      "  Epoch 88, Batch 81/173, Loss: 1.0758, Elapsed: 0:41:12\n",
      "  Epoch 88, Batch 91/173, Loss: 0.9584, Elapsed: 0:46:11\n",
      "  Epoch 88, Batch 101/173, Loss: 1.0403, Elapsed: 0:50:52\n",
      "  Epoch 88, Batch 111/173, Loss: 1.0029, Elapsed: 0:55:42\n",
      "  Epoch 88, Batch 121/173, Loss: 1.0685, Elapsed: 1:00:58\n",
      "  Epoch 88, Batch 131/173, Loss: 1.1845, Elapsed: 1:06:11\n",
      "  Epoch 88, Batch 141/173, Loss: 1.0301, Elapsed: 1:11:02\n",
      "  Epoch 88, Batch 151/173, Loss: 1.0730, Elapsed: 1:16:18\n",
      "  Epoch 88, Batch 161/173, Loss: 1.0679, Elapsed: 1:21:53\n",
      "  Epoch 88, Batch 171/173, Loss: 1.0105, Elapsed: 1:26:38\n",
      "\n",
      "  Average training loss: 1.02\n",
      "  Training epoch took: 1:27:24\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:33\n",
      "\n",
      "======== Epoch 89 / 100 ========\n",
      "Training...\n",
      "  Epoch 89, Batch 1/173, Loss: 1.1977, Elapsed: 0:00:13\n",
      "  Epoch 89, Batch 11/173, Loss: 1.0555, Elapsed: 0:05:38\n",
      "  Epoch 89, Batch 21/173, Loss: 1.0804, Elapsed: 0:11:12\n",
      "  Epoch 89, Batch 31/173, Loss: 1.0001, Elapsed: 0:18:45\n",
      "  Epoch 89, Batch 41/173, Loss: 1.1014, Elapsed: 0:50:33\n",
      "  Epoch 89, Batch 51/173, Loss: 0.9640, Elapsed: 0:55:56\n",
      "  Epoch 89, Batch 61/173, Loss: 1.0026, Elapsed: 1:00:44\n",
      "  Epoch 89, Batch 71/173, Loss: 0.9845, Elapsed: 1:05:28\n",
      "  Epoch 89, Batch 81/173, Loss: 1.1147, Elapsed: 1:10:27\n",
      "  Epoch 89, Batch 91/173, Loss: 1.0224, Elapsed: 1:15:33\n",
      "  Epoch 89, Batch 101/173, Loss: 1.1451, Elapsed: 1:20:17\n",
      "  Epoch 89, Batch 111/173, Loss: 1.1118, Elapsed: 1:25:00\n",
      "  Epoch 89, Batch 121/173, Loss: 0.9719, Elapsed: 1:30:16\n",
      "  Epoch 89, Batch 131/173, Loss: 1.0202, Elapsed: 1:35:22\n",
      "  Epoch 89, Batch 141/173, Loss: 1.0428, Elapsed: 1:40:05\n",
      "  Epoch 89, Batch 151/173, Loss: 1.0338, Elapsed: 1:44:45\n",
      "  Epoch 89, Batch 161/173, Loss: 1.0704, Elapsed: 1:49:37\n",
      "  Epoch 89, Batch 171/173, Loss: 0.9441, Elapsed: 1:54:41\n",
      "\n",
      "  Average training loss: 1.02\n",
      "  Training epoch took: 1:55:30\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 90 / 100 ========\n",
      "Training...\n",
      "  Epoch 90, Batch 1/173, Loss: 1.1282, Elapsed: 0:00:12\n",
      "  Epoch 90, Batch 11/173, Loss: 1.0158, Elapsed: 0:05:05\n",
      "  Epoch 90, Batch 21/173, Loss: 0.8942, Elapsed: 0:09:44\n",
      "  Epoch 90, Batch 31/173, Loss: 1.0512, Elapsed: 0:14:29\n",
      "  Epoch 90, Batch 41/173, Loss: 0.8529, Elapsed: 0:19:35\n",
      "  Epoch 90, Batch 51/173, Loss: 1.1055, Elapsed: 0:24:29\n",
      "  Epoch 90, Batch 61/173, Loss: 0.9795, Elapsed: 0:29:08\n",
      "  Epoch 90, Batch 71/173, Loss: 1.0938, Elapsed: 0:33:54\n",
      "  Epoch 90, Batch 81/173, Loss: 1.0059, Elapsed: 0:38:59\n",
      "  Epoch 90, Batch 91/173, Loss: 1.0387, Elapsed: 0:43:49\n",
      "  Epoch 90, Batch 101/173, Loss: 0.9430, Elapsed: 0:48:28\n",
      "  Epoch 90, Batch 111/173, Loss: 0.8404, Elapsed: 0:53:07\n",
      "  Epoch 90, Batch 121/173, Loss: 1.0728, Elapsed: 0:58:11\n",
      "  Epoch 90, Batch 131/173, Loss: 1.0063, Elapsed: 1:03:13\n",
      "  Epoch 90, Batch 141/173, Loss: 1.0825, Elapsed: 1:08:02\n",
      "  Epoch 90, Batch 151/173, Loss: 0.9785, Elapsed: 1:12:41\n",
      "  Epoch 90, Batch 161/173, Loss: 1.1427, Elapsed: 1:17:36\n",
      "  Epoch 90, Batch 171/173, Loss: 1.0075, Elapsed: 1:22:45\n",
      "\n",
      "  Average training loss: 1.02\n",
      "  Training epoch took: 1:23:35\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 91 / 100 ========\n",
      "Training...\n",
      "  Epoch 91, Batch 1/173, Loss: 1.0443, Elapsed: 0:00:12\n",
      "  Epoch 91, Batch 11/173, Loss: 1.0001, Elapsed: 0:04:50\n",
      "  Epoch 91, Batch 21/173, Loss: 0.9971, Elapsed: 0:09:28\n",
      "  Epoch 91, Batch 31/173, Loss: 0.9576, Elapsed: 0:14:16\n",
      "  Epoch 91, Batch 41/173, Loss: 1.0367, Elapsed: 0:19:16\n",
      "  Epoch 91, Batch 51/173, Loss: 1.0163, Elapsed: 0:24:05\n",
      "  Epoch 91, Batch 61/173, Loss: 0.9416, Elapsed: 0:28:41\n",
      "  Epoch 91, Batch 71/173, Loss: 1.0010, Elapsed: 0:33:19\n",
      "  Epoch 91, Batch 81/173, Loss: 1.0053, Elapsed: 0:38:20\n",
      "  Epoch 91, Batch 91/173, Loss: 1.0153, Elapsed: 0:43:21\n",
      "  Epoch 91, Batch 101/173, Loss: 0.9859, Elapsed: 0:48:13\n",
      "  Epoch 91, Batch 111/173, Loss: 1.0024, Elapsed: 0:52:52\n",
      "  Epoch 91, Batch 121/173, Loss: 1.0089, Elapsed: 0:57:46\n",
      "  Epoch 91, Batch 131/173, Loss: 1.1082, Elapsed: 1:02:50\n",
      "  Epoch 91, Batch 141/173, Loss: 0.9923, Elapsed: 1:07:39\n",
      "  Epoch 91, Batch 151/173, Loss: 1.0503, Elapsed: 1:12:19\n",
      "  Epoch 91, Batch 161/173, Loss: 1.0141, Elapsed: 1:17:04\n",
      "  Epoch 91, Batch 171/173, Loss: 1.0855, Elapsed: 1:22:14\n",
      "\n",
      "  Average training loss: 1.03\n",
      "  Training epoch took: 1:23:04\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 92 / 100 ========\n",
      "Training...\n",
      "  Epoch 92, Batch 1/173, Loss: 1.0078, Elapsed: 0:00:14\n",
      "  Epoch 92, Batch 11/173, Loss: 1.0006, Elapsed: 0:05:13\n",
      "  Epoch 92, Batch 21/173, Loss: 1.0492, Elapsed: 0:09:54\n",
      "  Epoch 92, Batch 31/173, Loss: 0.9943, Elapsed: 0:14:39\n",
      "  Epoch 92, Batch 41/173, Loss: 0.9733, Elapsed: 0:19:42\n",
      "  Epoch 92, Batch 51/173, Loss: 1.0389, Elapsed: 0:24:37\n",
      "  Epoch 92, Batch 61/173, Loss: 1.0590, Elapsed: 0:29:16\n",
      "  Epoch 92, Batch 71/173, Loss: 0.8935, Elapsed: 0:33:56\n",
      "  Epoch 92, Batch 81/173, Loss: 1.0636, Elapsed: 0:39:01\n",
      "  Epoch 92, Batch 91/173, Loss: 1.0328, Elapsed: 0:44:03\n",
      "  Epoch 92, Batch 101/173, Loss: 1.1190, Elapsed: 0:48:44\n",
      "  Epoch 92, Batch 111/173, Loss: 1.1457, Elapsed: 0:53:23\n",
      "  Epoch 92, Batch 121/173, Loss: 1.0638, Elapsed: 0:58:23\n",
      "  Epoch 92, Batch 131/173, Loss: 1.0051, Elapsed: 1:03:26\n",
      "  Epoch 92, Batch 141/173, Loss: 1.0388, Elapsed: 1:08:20\n",
      "  Epoch 92, Batch 151/173, Loss: 1.0469, Elapsed: 1:13:03\n",
      "  Epoch 92, Batch 161/173, Loss: 1.0656, Elapsed: 1:17:56\n",
      "  Epoch 92, Batch 171/173, Loss: 1.0278, Elapsed: 1:22:46\n",
      "\n",
      "  Average training loss: 1.02\n",
      "  Training epoch took: 1:23:37\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 93 / 100 ========\n",
      "Training...\n",
      "  Epoch 93, Batch 1/173, Loss: 0.9319, Elapsed: 0:00:13\n",
      "  Epoch 93, Batch 11/173, Loss: 0.9157, Elapsed: 0:04:57\n",
      "  Epoch 93, Batch 21/173, Loss: 1.0563, Elapsed: 0:09:36\n",
      "  Epoch 93, Batch 31/173, Loss: 0.9373, Elapsed: 0:14:19\n",
      "  Epoch 93, Batch 41/173, Loss: 0.9850, Elapsed: 0:19:20\n",
      "  Epoch 93, Batch 51/173, Loss: 0.9581, Elapsed: 0:24:12\n",
      "  Epoch 93, Batch 61/173, Loss: 1.0584, Elapsed: 0:28:51\n",
      "  Epoch 93, Batch 71/173, Loss: 1.0732, Elapsed: 0:33:33\n",
      "  Epoch 93, Batch 81/173, Loss: 1.0376, Elapsed: 0:38:37\n",
      "  Epoch 93, Batch 91/173, Loss: 1.0206, Elapsed: 0:43:43\n",
      "  Epoch 93, Batch 101/173, Loss: 0.9433, Elapsed: 0:48:40\n",
      "  Epoch 93, Batch 111/173, Loss: 1.0336, Elapsed: 0:53:20\n",
      "  Epoch 93, Batch 121/173, Loss: 1.0527, Elapsed: 0:58:07\n",
      "  Epoch 93, Batch 131/173, Loss: 1.0434, Elapsed: 1:03:03\n",
      "  Epoch 93, Batch 141/173, Loss: 0.9796, Elapsed: 1:07:46\n",
      "  Epoch 93, Batch 151/173, Loss: 0.9822, Elapsed: 1:12:23\n",
      "  Epoch 93, Batch 161/173, Loss: 1.0313, Elapsed: 1:17:01\n",
      "  Epoch 93, Batch 171/173, Loss: 0.9291, Elapsed: 1:21:57\n",
      "\n",
      "  Average training loss: 1.02\n",
      "  Training epoch took: 1:22:46\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 94 / 100 ========\n",
      "Training...\n",
      "  Epoch 94, Batch 1/173, Loss: 1.1044, Elapsed: 0:00:15\n",
      "  Epoch 94, Batch 11/173, Loss: 1.0191, Elapsed: 0:05:11\n",
      "  Epoch 94, Batch 21/173, Loss: 0.9995, Elapsed: 0:09:47\n",
      "  Epoch 94, Batch 31/173, Loss: 0.9524, Elapsed: 0:14:24\n",
      "  Epoch 94, Batch 41/173, Loss: 0.9908, Elapsed: 0:19:15\n",
      "  Epoch 94, Batch 51/173, Loss: 1.0138, Elapsed: 0:24:14\n",
      "  Epoch 94, Batch 61/173, Loss: 1.0833, Elapsed: 0:29:01\n",
      "  Epoch 94, Batch 71/173, Loss: 1.1065, Elapsed: 0:33:38\n",
      "  Epoch 94, Batch 81/173, Loss: 1.0127, Elapsed: 0:38:23\n",
      "  Epoch 94, Batch 91/173, Loss: 0.9725, Elapsed: 0:43:28\n",
      "  Epoch 94, Batch 101/173, Loss: 0.9939, Elapsed: 0:48:32\n",
      "  Epoch 94, Batch 111/173, Loss: 0.9574, Elapsed: 0:53:15\n",
      "  Epoch 94, Batch 121/173, Loss: 1.0101, Elapsed: 0:57:55\n",
      "  Epoch 94, Batch 131/173, Loss: 0.9656, Elapsed: 1:02:57\n",
      "  Epoch 94, Batch 141/173, Loss: 1.0743, Elapsed: 1:07:57\n",
      "  Epoch 94, Batch 151/173, Loss: 0.9119, Elapsed: 1:12:37\n",
      "  Epoch 94, Batch 161/173, Loss: 0.9351, Elapsed: 1:17:15\n",
      "  Epoch 94, Batch 171/173, Loss: 1.0118, Elapsed: 1:22:03\n",
      "\n",
      "  Average training loss: 1.03\n",
      "  Training epoch took: 1:22:50\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 95 / 100 ========\n",
      "Training...\n",
      "  Epoch 95, Batch 1/173, Loss: 1.1080, Elapsed: 0:00:12\n",
      "  Epoch 95, Batch 11/173, Loss: 1.0349, Elapsed: 0:05:08\n",
      "  Epoch 95, Batch 21/173, Loss: 1.0649, Elapsed: 0:09:55\n",
      "  Epoch 95, Batch 31/173, Loss: 1.0008, Elapsed: 0:14:34\n",
      "  Epoch 95, Batch 41/173, Loss: 1.1528, Elapsed: 0:19:16\n",
      "  Epoch 95, Batch 51/173, Loss: 0.8931, Elapsed: 0:24:20\n",
      "  Epoch 95, Batch 61/173, Loss: 1.0049, Elapsed: 0:29:23\n",
      "  Epoch 95, Batch 71/173, Loss: 0.9472, Elapsed: 0:34:12\n",
      "  Epoch 95, Batch 81/173, Loss: 1.0284, Elapsed: 0:38:50\n",
      "  Epoch 95, Batch 91/173, Loss: 0.9894, Elapsed: 0:43:52\n",
      "  Epoch 95, Batch 101/173, Loss: 1.0672, Elapsed: 0:48:53\n",
      "  Epoch 95, Batch 111/173, Loss: 0.9654, Elapsed: 0:53:32\n",
      "  Epoch 95, Batch 121/173, Loss: 1.0225, Elapsed: 0:58:15\n",
      "  Epoch 95, Batch 131/173, Loss: 1.0739, Elapsed: 1:03:04\n",
      "  Epoch 95, Batch 141/173, Loss: 0.9668, Elapsed: 1:08:09\n",
      "  Epoch 95, Batch 151/173, Loss: 1.0935, Elapsed: 1:12:57\n",
      "  Epoch 95, Batch 161/173, Loss: 1.0453, Elapsed: 1:17:36\n",
      "  Epoch 95, Batch 171/173, Loss: 1.0374, Elapsed: 1:22:17\n",
      "\n",
      "  Average training loss: 1.03\n",
      "  Training epoch took: 1:23:06\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 96 / 100 ========\n",
      "Training...\n",
      "  Epoch 96, Batch 1/173, Loss: 0.9613, Elapsed: 0:00:15\n",
      "  Epoch 96, Batch 11/173, Loss: 0.9680, Elapsed: 0:05:16\n",
      "  Epoch 96, Batch 21/173, Loss: 0.9866, Elapsed: 0:10:20\n",
      "  Epoch 96, Batch 31/173, Loss: 1.0066, Elapsed: 0:15:05\n",
      "  Epoch 96, Batch 41/173, Loss: 0.9116, Elapsed: 0:19:43\n",
      "  Epoch 96, Batch 51/173, Loss: 1.0455, Elapsed: 0:24:39\n",
      "  Epoch 96, Batch 61/173, Loss: 1.0418, Elapsed: 0:29:39\n",
      "  Epoch 96, Batch 71/173, Loss: 1.0845, Elapsed: 0:34:21\n",
      "  Epoch 96, Batch 81/173, Loss: 0.9663, Elapsed: 0:38:58\n",
      "  Epoch 96, Batch 91/173, Loss: 1.0838, Elapsed: 0:43:44\n",
      "  Epoch 96, Batch 101/173, Loss: 1.0422, Elapsed: 0:48:41\n",
      "  Epoch 96, Batch 111/173, Loss: 1.0736, Elapsed: 0:53:34\n",
      "  Epoch 96, Batch 121/173, Loss: 1.0805, Elapsed: 0:58:11\n",
      "  Epoch 96, Batch 131/173, Loss: 0.9944, Elapsed: 1:02:49\n",
      "  Epoch 96, Batch 141/173, Loss: 1.0123, Elapsed: 1:07:44\n",
      "  Epoch 96, Batch 151/173, Loss: 1.0448, Elapsed: 1:12:40\n",
      "  Epoch 96, Batch 161/173, Loss: 1.0301, Elapsed: 1:17:19\n",
      "  Epoch 96, Batch 171/173, Loss: 0.9869, Elapsed: 1:21:55\n",
      "\n",
      "  Average training loss: 1.02\n",
      "  Training epoch took: 1:22:40\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 97 / 100 ========\n",
      "Training...\n",
      "  Epoch 97, Batch 1/173, Loss: 1.0115, Elapsed: 0:00:12\n",
      "  Epoch 97, Batch 11/173, Loss: 1.0705, Elapsed: 0:04:58\n",
      "  Epoch 97, Batch 21/173, Loss: 0.9974, Elapsed: 0:09:54\n",
      "  Epoch 97, Batch 31/173, Loss: 0.9726, Elapsed: 0:14:42\n",
      "  Epoch 97, Batch 41/173, Loss: 1.1836, Elapsed: 0:19:19\n",
      "  Epoch 97, Batch 51/173, Loss: 0.9304, Elapsed: 0:23:54\n",
      "  Epoch 97, Batch 61/173, Loss: 1.0473, Elapsed: 0:28:50\n",
      "  Epoch 97, Batch 71/173, Loss: 1.0058, Elapsed: 0:33:47\n",
      "  Epoch 97, Batch 81/173, Loss: 1.0487, Elapsed: 0:38:38\n",
      "  Epoch 97, Batch 91/173, Loss: 0.9908, Elapsed: 0:43:15\n",
      "  Epoch 97, Batch 101/173, Loss: 0.9844, Elapsed: 0:48:05\n",
      "  Epoch 97, Batch 111/173, Loss: 1.0153, Elapsed: 0:53:01\n",
      "  Epoch 97, Batch 121/173, Loss: 1.1036, Elapsed: 0:57:51\n",
      "  Epoch 97, Batch 131/173, Loss: 1.0453, Elapsed: 1:02:25\n",
      "  Epoch 97, Batch 141/173, Loss: 1.0228, Elapsed: 1:07:01\n",
      "  Epoch 97, Batch 151/173, Loss: 1.0535, Elapsed: 1:11:54\n",
      "  Epoch 97, Batch 161/173, Loss: 1.0412, Elapsed: 1:16:53\n",
      "  Epoch 97, Batch 171/173, Loss: 1.0578, Elapsed: 1:21:39\n",
      "\n",
      "  Average training loss: 1.02\n",
      "  Training epoch took: 1:22:24\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:30\n",
      "\n",
      "======== Epoch 98 / 100 ========\n",
      "Training...\n",
      "  Epoch 98, Batch 1/173, Loss: 0.9763, Elapsed: 0:00:13\n",
      "  Epoch 98, Batch 11/173, Loss: 0.9332, Elapsed: 0:04:54\n",
      "  Epoch 98, Batch 21/173, Loss: 1.0459, Elapsed: 0:09:46\n",
      "  Epoch 98, Batch 31/173, Loss: 1.0567, Elapsed: 0:14:48\n",
      "  Epoch 98, Batch 41/173, Loss: 0.9952, Elapsed: 0:19:49\n",
      "  Epoch 98, Batch 51/173, Loss: 0.9011, Elapsed: 0:24:27\n",
      "  Epoch 98, Batch 61/173, Loss: 1.0639, Elapsed: 0:29:13\n",
      "  Epoch 98, Batch 71/173, Loss: 1.0252, Elapsed: 0:34:14\n",
      "  Epoch 98, Batch 81/173, Loss: 0.9921, Elapsed: 0:39:09\n",
      "  Epoch 98, Batch 91/173, Loss: 1.0334, Elapsed: 0:43:47\n",
      "  Epoch 98, Batch 101/173, Loss: 0.9898, Elapsed: 0:48:27\n",
      "  Epoch 98, Batch 111/173, Loss: 1.0372, Elapsed: 0:53:21\n",
      "  Epoch 98, Batch 121/173, Loss: 1.0495, Elapsed: 0:58:25\n",
      "  Epoch 98, Batch 131/173, Loss: 0.8776, Elapsed: 1:03:04\n",
      "  Epoch 98, Batch 141/173, Loss: 0.8999, Elapsed: 1:07:43\n",
      "  Epoch 98, Batch 151/173, Loss: 1.0408, Elapsed: 1:12:28\n",
      "  Epoch 98, Batch 161/173, Loss: 1.0341, Elapsed: 1:17:31\n",
      "  Epoch 98, Batch 171/173, Loss: 0.9944, Elapsed: 1:22:30\n",
      "\n",
      "  Average training loss: 1.03\n",
      "  Training epoch took: 1:23:20\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "======== Epoch 99 / 100 ========\n",
      "Training...\n",
      "  Epoch 99, Batch 1/173, Loss: 1.0280, Elapsed: 0:00:12\n",
      "  Epoch 99, Batch 11/173, Loss: 1.0466, Elapsed: 0:04:55\n",
      "  Epoch 99, Batch 21/173, Loss: 1.0640, Elapsed: 0:09:48\n",
      "  Epoch 99, Batch 31/173, Loss: 1.1093, Elapsed: 0:15:00\n",
      "  Epoch 99, Batch 41/173, Loss: 1.0522, Elapsed: 0:19:58\n",
      "  Epoch 99, Batch 51/173, Loss: 0.9959, Elapsed: 0:24:39\n",
      "  Epoch 99, Batch 61/173, Loss: 1.0294, Elapsed: 0:33:54\n",
      "  Epoch 99, Batch 71/173, Loss: 0.9746, Elapsed: 1:04:21\n",
      "  Epoch 99, Batch 81/173, Loss: 1.0161, Elapsed: 1:09:32\n",
      "  Epoch 99, Batch 91/173, Loss: 0.9931, Elapsed: 1:14:43\n",
      "  Epoch 99, Batch 101/173, Loss: 1.0488, Elapsed: 1:20:49\n",
      "  Epoch 99, Batch 111/173, Loss: 1.0932, Elapsed: 1:26:06\n",
      "  Epoch 99, Batch 121/173, Loss: 1.0779, Elapsed: 1:31:43\n",
      "  Epoch 99, Batch 131/173, Loss: 0.9078, Elapsed: 1:37:38\n",
      "  Epoch 99, Batch 141/173, Loss: 1.0792, Elapsed: 1:42:49\n",
      "  Epoch 99, Batch 151/173, Loss: 1.0308, Elapsed: 1:48:23\n",
      "  Epoch 99, Batch 161/173, Loss: 1.1760, Elapsed: 1:54:06\n",
      "  Epoch 99, Batch 171/173, Loss: 0.9612, Elapsed: 1:59:20\n",
      "\n",
      "  Average training loss: 1.02\n",
      "  Training epoch took: 2:00:11\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:37\n",
      "\n",
      "======== Epoch 100 / 100 ========\n",
      "Training...\n",
      "  Epoch 100, Batch 1/173, Loss: 1.0456, Elapsed: 0:00:13\n",
      "  Epoch 100, Batch 11/173, Loss: 1.1075, Elapsed: 0:05:52\n",
      "  Epoch 100, Batch 21/173, Loss: 1.0079, Elapsed: 0:11:43\n",
      "  Epoch 100, Batch 31/173, Loss: 1.0634, Elapsed: 0:17:19\n",
      "  Epoch 100, Batch 41/173, Loss: 0.9859, Elapsed: 0:23:06\n",
      "  Epoch 100, Batch 51/173, Loss: 0.9455, Elapsed: 0:28:24\n",
      "  Epoch 100, Batch 61/173, Loss: 0.9712, Elapsed: 0:33:14\n",
      "  Epoch 100, Batch 71/173, Loss: 0.8493, Elapsed: 0:38:30\n",
      "  Epoch 100, Batch 81/173, Loss: 1.0302, Elapsed: 0:44:30\n",
      "  Epoch 100, Batch 91/173, Loss: 1.1351, Elapsed: 0:49:25\n",
      "  Epoch 100, Batch 101/173, Loss: 1.0655, Elapsed: 0:54:32\n",
      "  Epoch 100, Batch 111/173, Loss: 1.0742, Elapsed: 0:59:43\n",
      "  Epoch 100, Batch 121/173, Loss: 1.0506, Elapsed: 1:04:48\n",
      "  Epoch 100, Batch 131/173, Loss: 0.9383, Elapsed: 1:09:38\n",
      "  Epoch 100, Batch 141/173, Loss: 0.9950, Elapsed: 1:15:00\n",
      "  Epoch 100, Batch 151/173, Loss: 1.0331, Elapsed: 1:20:47\n",
      "  Epoch 100, Batch 161/173, Loss: 0.9986, Elapsed: 1:25:33\n",
      "  Epoch 100, Batch 171/173, Loss: 1.0365, Elapsed: 1:30:20\n",
      "\n",
      "  Average training loss: 1.02\n",
      "  Training epoch took: 1:31:20\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 1.14\n",
      "  Validation took: 0:01:36\n",
      "\n",
      "Training complete!\n",
      "Total training took 6 days, 18:21:54 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "## Fine-tune on properties datasets\n",
    "batch_size = 120\n",
    "\n",
    "train_dataset = GPT2Dataset(prompt_list_train, answer_list_train, tokenizer,\n",
    "                            max_length_propmt=150, max_length_answer=6)\n",
    "val_dataset = GPT2Dataset(prompt_list_test, answer_list_test, tokenizer,\n",
    "                          max_length_propmt=150, max_length_answer=6)\n",
    "\n",
    "print('{:>5,} training samples'.format(len(prompt_list_train)))\n",
    "print('{:>5,} validation samples'.format(len(prompt_list_test)))\n",
    "\n",
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler=RandomSampler(train_dataset),  # Select batches randomly\n",
    "            batch_size=batch_size  # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation, the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,  # The validation samples.\n",
    "            sampler=SequentialSampler(val_dataset),  # Pull out batches sequentially.\n",
    "            batch_size=batch_size  # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "config_path = path\n",
    "model_path = path\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 5e-6\n",
    "warmup_ratio = 0.2\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Updated model_training function with tracking\n",
    "def model_training(config_path, model_path, tokenizer, train_dataloader,\n",
    "                   validation_dataloader, epochs, learning_rate, warmup_ratio,\n",
    "                   epsilon):\n",
    "    configuration = T5Config.from_pretrained(config_path, output_hidden_states=False)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path, config=configuration)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = DataParallel(model)\n",
    "\n",
    "    seed_val = 42\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    warmup_steps = int(len(train_dataloader) * epochs * warmup_ratio)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "    def format_time(elapsed):\n",
    "        return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "\n",
    "    total_t0 = time.time()\n",
    "    training_stats = []\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(\"\")\n",
    "        print(f\"======== Epoch {epoch_i + 1} / {epochs} ========\")\n",
    "        print(\"Training...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            b_masks = batch[1].to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            outputs = model(b_input_ids, labels=b_labels, attention_mask=b_masks, output_attentions=True)\n",
    "            loss = outputs[0]\n",
    "            loss = loss.mean()\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            total_train_loss += batch_loss\n",
    "\n",
    "            # Track progress every 10 batches\n",
    "            if step % 10 == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print(f\"  Epoch {epoch_i + 1}, Batch {step + 1}/{len(train_dataloader)}, Loss: {batch_loss:.4f}, Elapsed: {elapsed}\")\n",
    "\n",
    "            # Sample output every 1000 batches\n",
    "            if step % 1000 == 0 and step != 0:\n",
    "                print(f\"  [Sample] Batch {step + 1} loss: {batch_loss:.4f}\")\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        writer.add_scalar(\"Loss/train\", avg_train_loss, epoch_i + 1)\n",
    "\n",
    "        training_time = format_time(time.time() - t0)\n",
    "        print(\"\")\n",
    "        print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
    "        print(f\"  Training epoch took: {training_time}\")\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "        t0 = time.time()\n",
    "        model.eval()\n",
    "\n",
    "        total_eval_loss = 0\n",
    "        for batch in validation_dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            b_masks = batch[1].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, attention_mask=b_masks, labels=b_labels, output_attentions=True)\n",
    "                loss = outputs[0]\n",
    "                loss = loss.mean()\n",
    "            batch_loss = loss.item()\n",
    "            total_eval_loss += batch_loss\n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        writer.add_scalar(\"Loss/test\", avg_val_loss, epoch_i + 1)\n",
    "\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n",
    "        print(f\"  Validation took: {validation_time}\")\n",
    "\n",
    "        training_stats.append({\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        })\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Total training took {format_time(time.time()-total_t0)} (h:mm:ss)\")\n",
    "\n",
    "    return model, tokenizer, training_stats\n",
    "\n",
    "# Start training with the updated model_training function\n",
    "model, tokenizer, training_stats = model_training(config_path, model_path, tokenizer,\n",
    "                                                  train_dataloader, validation_dataloader,\n",
    "                                                  epochs, learning_rate, warmup_ratio,\n",
    "                                                  epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to /Users/kr24945/Documents/Projects/Materia science/ML4Polymers/ML4Polymers/model_save/cls-5tasks-bs150-bs8-lr5e6-epoch100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/Users/kr24945/Documents/Projects/Materia science/ML4Polymers/ML4Polymers/model_save/cls-5tasks-bs150-bs8-lr5e6-epoch100/tokenizer_config.json',\n",
       " '/Users/kr24945/Documents/Projects/Materia science/ML4Polymers/ML4Polymers/model_save/cls-5tasks-bs150-bs8-lr5e6-epoch100/special_tokens_map.json',\n",
       " '/Users/kr24945/Documents/Projects/Materia science/ML4Polymers/ML4Polymers/model_save/cls-5tasks-bs150-bs8-lr5e6-epoch100/vocab.json',\n",
       " '/Users/kr24945/Documents/Projects/Materia science/ML4Polymers/ML4Polymers/model_save/cls-5tasks-bs150-bs8-lr5e6-epoch100/merges.txt',\n",
       " '/Users/kr24945/Documents/Projects/Materia science/ML4Polymers/ML4Polymers/model_save/cls-5tasks-bs150-bs8-lr5e6-epoch100/added_tokens.json',\n",
       " '/Users/kr24945/Documents/Projects/Materia science/ML4Polymers/ML4Polymers/model_save/cls-5tasks-bs150-bs8-lr5e6-epoch100/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output_dir = '/work/PolyGPT/T5/model_save/cls-5tasks-bs150-bs8-lr5e6-epoch100'\n",
    "output_dir = '/Users/kr24945/Documents/Projects/Materia science/ML4Polymers/ML4Polymers/model_save/cls-5tasks-bs150-bs8-lr5e6-epoch100' # Nika replaced\n",
    "# df_train = pd.read_csv('/work/PolyGPT/T5/0data&code/data/train/prompt-target.csv') # this is only a ref to directory and can be removed later\n",
    "#df_train = pd.read_csv('../data/train/prompt-target.csv')  this is only a ref to directory and can be removed later\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f37fbaf75c241dfa9b0cdea0a68181f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890e814c33ce4c5bb605f49d7eea580e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8ce9590dbf4fb997ff03f2f83f44c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a478b5f9f044f181cfabb073d0cc57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015941cd6f99493bba514e2f919418f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df4f49570f84c618ac1d29f698a178a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nika - adding tokanizer to initiate\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")  # Replace \"t5-small\" with your specific model if necessary\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1266, 12194, 8, 1678, 5673, 853, 13, 8, 826, 180, 7075, 12335, 10, 411, 2423, 75, 536, 29, 599, 254, 61, 75, 599, 2423, 667, 61, 75, 357, 75, 75, 519, 75, 599, 2423, 667, 61, 29, 599, 18, 75, 591, 75, 75, 75, 599, 18, 75, 755, 75, 75, 75, 599, 254, 61, 75, 75, 9120, 75, 75, 7256, 75, 599, 2423, 667, 61, 75, 519, 75, 75, 2658, 1]\n",
      "['▁Pre', 'dict', '▁the', '▁heat', '▁resistance', '▁class', '▁of', '▁the', '▁following', '▁S', 'MI', 'LES', ':', '▁O', '=', 'c', '1', 'n', '(', 'C', ')', 'c', '(', '=', 'O', ')', 'c', '2', 'c', 'c', '3', 'c', '(', '=', 'O', ')', 'n', '(', '-', 'c', '4', 'c', 'c', 'c', '(', '-', 'c', '5', 'c', 'c', 'c', '(', 'C', ')', 'c', 'c', '5)', 'c', 'c', '4)', 'c', '(', '=', 'O', ')', 'c', '3', 'c', 'c', '21', '</s>']\n",
      "1: <pad>Predict the heat resistance class of\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/polyNC1/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.99` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/polyNC1/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `100` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#igenerate a prediction using a fine-tuned T5 model\n",
    "#Nika amended this part as model.module did not work\n",
    "# prompt = '[CLS]' + \"[*]NC(=O)OC(C=C1)=CC=C1[*]'s band gap chain is\"\n",
    "\n",
    "# prompt = \"N1(CCCCCCCCN2C(=O)c3cccc(*)c3C2=O)C(=O)c2c(c(Oc3ccc(Oc4ccc(O*)cc4)cc3)ccc2)C1=O\"\n",
    "prompt = 'Predict the heat resistance class of the following SMILES: O=c1n(C)c(=O)c2cc3c(=O)n(-c4ccc(-c5ccc(C)cc5)cc4)c(=O)c3cc21'\n",
    "\n",
    "generated = tokenizer.encode(prompt) #encodes the prompt into a list of token IDs, which are numerical representations that the model can process.\n",
    "print(generated)\n",
    "tokens = tokenizer.convert_ids_to_tokens(generated) #converts these IDs back to their respective tokens, allowing to see the model's internal vocabulary representation of the prompt.\n",
    "print(tokens)\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0) ##encodes the prompt into a list of token IDs, which are numerical representations that the model can process.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # ensor is then moved to the appropriate device (cuda if available, otherwise cpu)\n",
    "\n",
    "generated = generated.to(device)\n",
    "\n",
    "sample_outputs = model.generate( #calls the generate function to produce text based on the prompt\n",
    "                                generated, \n",
    "                                #bos_token_id=random.randint(1,30000),\n",
    "                                do_sample=False,   #Indicates that deterministic sampling (beam search) is used instead of stochastic sampling.\n",
    "                                top_k=100, #Limits sampling to the top 100 tokens, providing a form of filtering\n",
    "                                max_length = 8, #Sets the maximum length for the generated text.\n",
    "                                top_p=0.99, #Enables nucleus sampling to choose tokens from a cumulative probability distribution of 0.99.\n",
    "                                num_return_sequences=1 #Specifies that only one sequence should be returned.\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\\n\\n\".format(i+1, tokenizer.decode(sample_output, skip_special_tokens=False))) #decodes and prints each generated sequence, with special tokens included (if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polyNC1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
